{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1= pd.read_csv(\"../input/sunpharma-data-redefined/PreviousDayClose.csv\", index_col= \"Date\", parse_dates= True)\ndf1","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"                  Open        High         Low       Close   Adj Close  \\\nDate                                                                     \n2009-12-01  145.354996  155.500000  145.354996  153.990005  144.129593   \n2009-12-02  156.399994  160.000000  148.660004  149.720001  140.133011   \n2009-12-03  150.014999  152.100006  148.750000  149.475006  139.903687   \n2009-12-04  148.500000  150.880005  145.110001  148.270004  138.775833   \n2009-12-07  148.509995  149.789993  144.110001  145.190002  135.893051   \n...                ...         ...         ...         ...         ...   \n2021-03-22  578.599976  593.599976  578.599976  591.000000  591.000000   \n2021-03-23  591.200012  594.700012  583.000000  587.849976  587.849976   \n2021-03-24  585.000000  597.000000  583.200012  585.299988  585.299988   \n2021-03-25  584.400024  587.950012  571.500000  575.750000  575.750000   \n2021-03-26  580.849976  590.299988  574.599976  587.200012  587.200012   \n\n               Volume  Growth% of Close from previous day  \nDate                                                       \n2009-12-01  4338810.0                           -2.772910  \n2009-12-02  4689480.0                           -0.163635  \n2009-12-03  2534030.0                           -0.806156  \n2009-12-04  1216690.0                           -2.077293  \n2009-12-07  1559620.0                            0.888487  \n...               ...                                 ...  \n2021-03-22  7033955.0                           -0.532999  \n2021-03-23  5341729.0                           -0.433782  \n2021-03-24  7299582.0                           -1.631640  \n2021-03-25  4710869.0                            1.988712  \n2021-03-26  5761818.0                            1.788147  \n\n[2786 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>Growth% of Close from previous day</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2009-12-01</th>\n      <td>145.354996</td>\n      <td>155.500000</td>\n      <td>145.354996</td>\n      <td>153.990005</td>\n      <td>144.129593</td>\n      <td>4338810.0</td>\n      <td>-2.772910</td>\n    </tr>\n    <tr>\n      <th>2009-12-02</th>\n      <td>156.399994</td>\n      <td>160.000000</td>\n      <td>148.660004</td>\n      <td>149.720001</td>\n      <td>140.133011</td>\n      <td>4689480.0</td>\n      <td>-0.163635</td>\n    </tr>\n    <tr>\n      <th>2009-12-03</th>\n      <td>150.014999</td>\n      <td>152.100006</td>\n      <td>148.750000</td>\n      <td>149.475006</td>\n      <td>139.903687</td>\n      <td>2534030.0</td>\n      <td>-0.806156</td>\n    </tr>\n    <tr>\n      <th>2009-12-04</th>\n      <td>148.500000</td>\n      <td>150.880005</td>\n      <td>145.110001</td>\n      <td>148.270004</td>\n      <td>138.775833</td>\n      <td>1216690.0</td>\n      <td>-2.077293</td>\n    </tr>\n    <tr>\n      <th>2009-12-07</th>\n      <td>148.509995</td>\n      <td>149.789993</td>\n      <td>144.110001</td>\n      <td>145.190002</td>\n      <td>135.893051</td>\n      <td>1559620.0</td>\n      <td>0.888487</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-03-22</th>\n      <td>578.599976</td>\n      <td>593.599976</td>\n      <td>578.599976</td>\n      <td>591.000000</td>\n      <td>591.000000</td>\n      <td>7033955.0</td>\n      <td>-0.532999</td>\n    </tr>\n    <tr>\n      <th>2021-03-23</th>\n      <td>591.200012</td>\n      <td>594.700012</td>\n      <td>583.000000</td>\n      <td>587.849976</td>\n      <td>587.849976</td>\n      <td>5341729.0</td>\n      <td>-0.433782</td>\n    </tr>\n    <tr>\n      <th>2021-03-24</th>\n      <td>585.000000</td>\n      <td>597.000000</td>\n      <td>583.200012</td>\n      <td>585.299988</td>\n      <td>585.299988</td>\n      <td>7299582.0</td>\n      <td>-1.631640</td>\n    </tr>\n    <tr>\n      <th>2021-03-25</th>\n      <td>584.400024</td>\n      <td>587.950012</td>\n      <td>571.500000</td>\n      <td>575.750000</td>\n      <td>575.750000</td>\n      <td>4710869.0</td>\n      <td>1.988712</td>\n    </tr>\n    <tr>\n      <th>2021-03-26</th>\n      <td>580.849976</td>\n      <td>590.299988</td>\n      <td>574.599976</td>\n      <td>587.200012</td>\n      <td>587.200012</td>\n      <td>5761818.0</td>\n      <td>1.788147</td>\n    </tr>\n  </tbody>\n</table>\n<p>2786 rows Ã— 7 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"ggplot\")\n\ndf1[\"Close\"].plot(label=\"Close\", title=\"Sunpharma Closing price\", figsize=(10,6))","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"<AxesSubplot:title={'center':'Sunpharma Closing price'}, xlabel='Date'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing...  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features and Target segmentation\ndata = df1.iloc[:, 6:7].values # .values to convert it to numpy array","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(data)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"numpy.ndarray"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Normalization\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nmm = MinMaxScaler(feature_range= (0, 1))\nss = StandardScaler()\n\ndata_scaled = mm.fit_transform(data)\ndata_scaled","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"array([[0.47123328],\n       [0.57132007],\n       [0.5466742 ],\n       ...,\n       [0.51501022],\n       [0.65388003],\n       [0.64618673]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keeping last 30 values(about a month) for testing\ntrain_data = data_scaled[:2756, :]\ntest_data= data_scaled[2756:, :]\n\nprint(train_data.shape)\nprint(test_data.shape)","execution_count":7,"outputs":[{"output_type":"stream","text":"(2756, 1)\n(30, 1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a datastructure with 60 time stamps and 1 output(only on training set)\n\nx_train= []\ny_train= []\n\nfor i in range(60, len(train_data)):\n    x_train.append(train_data[i-60:i, :])\n    y_train.append(train_data[i, 0])\n    \n# Converting them to numpy arrays\nx_train, y_train= np.array(x_train), np.array(y_train)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train. shape, y_train.shape","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"((2696, 60, 1), (2696,))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train= y_train.reshape(y_train.shape[0], 1)\ny_train.shape","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(2696, 1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing dependencies\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable ","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tensors = Variable(torch.Tensor(x_train)).cuda()\n# X_test_tensors = Variable(torch.Tensor(X_test)).cuda()\n\ny_train_tensors = Variable(torch.Tensor(y_train)).cuda()\n# y_test_tensors = Variable(torch.Tensor(y_test)) .cuda()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshaping to rows, timestamps, features\n\nX_train_tensors_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n\n# X_test_tensors_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n\n\n# Printing shapes \nprint(\"Training Shape\", X_train_tensors_final.shape, y_train_tensors.shape)\n# print(\"Testing Shape\", X_test_tensors_final.shape, y_test_tensors.shape)","execution_count":13,"outputs":[{"output_type":"stream","text":"Training Shape torch.Size([2696, 1, 60]) torch.Size([2696, 1])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Network Defination...."},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM1(nn.Module):\n    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n        super(LSTM1, self).__init__()\n        self.num_classes = num_classes # number of classes\n        self.num_layers = num_layers # number of layers\n        self.input_size = input_size # input size\n        self.hidden_size = hidden_size # hidden state\n        self.seq_length = seq_length # sequence length\n\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                          num_layers=num_layers, batch_first=True) # lstm\n        self.fc_1 =  nn.Linear(hidden_size, 128) # fully connected 1\n        self.fc = nn.Linear(128, num_classes) # fully connected last layer\n\n        self.relu = nn.ReLU()\n    \n    def forward(self,x):\n        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda() # hidden state\n        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda() # internal state\n        # Propagate input through LSTM\n        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # lstm with input, hidden, and internal state\n        output = output.view(-1, self.hidden_size) # reshaping the output for fc layer\n        out = self.relu(output)\n        out = self.fc_1(out) #first Dense\n        out = self.relu(out) #relu\n        out = self.fc(out) #Final Output\n        return out","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Important parameters.... "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 250000 \nlearning_rate = 0.003 \n\ninput_size = 60 # number of features\nhidden_size = 8 # number of features in hidden state\nnum_layers = 3 # number of stacked lstm layers\n\nnum_classes = 1 # number of output classes(coz regression)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]).cuda() #our lstm class","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate)","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Loop.... "},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_min = np.Inf\n\nfor epoch in range(num_epochs):\n    outputs = lstm1.forward(X_train_tensors_final) #forward pass\n    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n\n    # obtain the loss function\n    loss = criterion(outputs, y_train_tensors)\n\n    loss.backward() #calculates the loss of the loss function\n \n    # save model if loss has decreased\n    if loss.item() <= loss_min:\n        print('Loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        loss_min,\n        loss.item()))\n        checkpoint = {'model': lstm1,\n              'state_dict': lstm1.state_dict(),\n              'optimizer' : optimizer.state_dict(),\n              'train_loss': loss.item()}\n\n        torch.save(checkpoint, 'checkpoint.pth')\n        loss_min = loss.item()\n\n    optimizer.step() #improve from loss, i.e backprop\n    if epoch % 100 == 0:\n        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) ","execution_count":18,"outputs":[{"output_type":"stream","text":"Loss decreased (inf --> 0.406420).  Saving model ...\nEpoch: 0, loss: 0.40642\nLoss decreased (0.406420 --> 0.346326).  Saving model ...\nLoss decreased (0.346326 --> 0.291339).  Saving model ...\nLoss decreased (0.291339 --> 0.241476).  Saving model ...\nLoss decreased (0.241476 --> 0.196709).  Saving model ...\nLoss decreased (0.196709 --> 0.156945).  Saving model ...\nLoss decreased (0.156945 --> 0.122130).  Saving model ...\nLoss decreased (0.122130 --> 0.092174).  Saving model ...\nLoss decreased (0.092174 --> 0.066980).  Saving model ...\nLoss decreased (0.066980 --> 0.046456).  Saving model ...\nLoss decreased (0.046456 --> 0.030419).  Saving model ...\nLoss decreased (0.030419 --> 0.018612).  Saving model ...\nLoss decreased (0.018612 --> 0.010771).  Saving model ...\nLoss decreased (0.010771 --> 0.006531).  Saving model ...\nLoss decreased (0.006531 --> 0.005396).  Saving model ...\nLoss decreased (0.005396 --> 0.005392).  Saving model ...\nLoss decreased (0.005392 --> 0.005386).  Saving model ...\nEpoch: 100, loss: 0.00539\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nEpoch: 200, loss: 0.00539\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nEpoch: 300, loss: 0.00539\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nEpoch: 400, loss: 0.00539\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nEpoch: 500, loss: 0.00539\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nEpoch: 600, loss: 0.00539\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nEpoch: 700, loss: 0.00539\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nEpoch: 800, loss: 0.00539\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nEpoch: 900, loss: 0.00539\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005386).  Saving model ...\nLoss decreased (0.005386 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nEpoch: 1000, loss: 0.00539\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005385).  Saving model ...\nLoss decreased (0.005385 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005384).  Saving model ...\nLoss decreased (0.005384 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005383).  Saving model ...\nLoss decreased (0.005383 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005382).  Saving model ...\nLoss decreased (0.005382 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nEpoch: 1100, loss: 0.00538\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005381).  Saving model ...\nLoss decreased (0.005381 --> 0.005380).  Saving model ...\nLoss decreased (0.005380 --> 0.005380).  Saving model ...\nLoss decreased (0.005380 --> 0.005380).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.005380 --> 0.005380).  Saving model ...\nLoss decreased (0.005380 --> 0.005380).  Saving model ...\nLoss decreased (0.005380 --> 0.005380).  Saving model ...\nLoss decreased (0.005380 --> 0.005380).  Saving model ...\nLoss decreased (0.005380 --> 0.005380).  Saving model ...\nLoss decreased (0.005380 --> 0.005380).  Saving model ...\nLoss decreased (0.005380 --> 0.005379).  Saving model ...\nLoss decreased (0.005379 --> 0.005379).  Saving model ...\nLoss decreased (0.005379 --> 0.005379).  Saving model ...\nLoss decreased (0.005379 --> 0.005379).  Saving model ...\nLoss decreased (0.005379 --> 0.005379).  Saving model ...\nLoss decreased (0.005379 --> 0.005379).  Saving model ...\nLoss decreased (0.005379 --> 0.005379).  Saving model ...\nLoss decreased (0.005379 --> 0.005379).  Saving model ...\nLoss decreased (0.005379 --> 0.005378).  Saving model ...\nLoss decreased (0.005378 --> 0.005378).  Saving model ...\nLoss decreased (0.005378 --> 0.005378).  Saving model ...\nLoss decreased (0.005378 --> 0.005378).  Saving model ...\nLoss decreased (0.005378 --> 0.005378).  Saving model ...\nLoss decreased (0.005378 --> 0.005378).  Saving model ...\nLoss decreased (0.005378 --> 0.005378).  Saving model ...\nLoss decreased (0.005378 --> 0.005377).  Saving model ...\nLoss decreased (0.005377 --> 0.005377).  Saving model ...\nLoss decreased (0.005377 --> 0.005377).  Saving model ...\nLoss decreased (0.005377 --> 0.005377).  Saving model ...\nLoss decreased (0.005377 --> 0.005377).  Saving model ...\nLoss decreased (0.005377 --> 0.005377).  Saving model ...\nLoss decreased (0.005377 --> 0.005376).  Saving model ...\nLoss decreased (0.005376 --> 0.005376).  Saving model ...\nLoss decreased (0.005376 --> 0.005376).  Saving model ...\nLoss decreased (0.005376 --> 0.005376).  Saving model ...\nLoss decreased (0.005376 --> 0.005376).  Saving model ...\nLoss decreased (0.005376 --> 0.005375).  Saving model ...\nLoss decreased (0.005375 --> 0.005375).  Saving model ...\nLoss decreased (0.005375 --> 0.005375).  Saving model ...\nLoss decreased (0.005375 --> 0.005375).  Saving model ...\nLoss decreased (0.005375 --> 0.005374).  Saving model ...\nLoss decreased (0.005374 --> 0.005374).  Saving model ...\nLoss decreased (0.005374 --> 0.005374).  Saving model ...\nLoss decreased (0.005374 --> 0.005374).  Saving model ...\nLoss decreased (0.005374 --> 0.005374).  Saving model ...\nLoss decreased (0.005374 --> 0.005373).  Saving model ...\nLoss decreased (0.005373 --> 0.005373).  Saving model ...\nLoss decreased (0.005373 --> 0.005373).  Saving model ...\nLoss decreased (0.005373 --> 0.005372).  Saving model ...\nLoss decreased (0.005372 --> 0.005372).  Saving model ...\nLoss decreased (0.005372 --> 0.005372).  Saving model ...\nLoss decreased (0.005372 --> 0.005372).  Saving model ...\nLoss decreased (0.005372 --> 0.005371).  Saving model ...\nLoss decreased (0.005371 --> 0.005371).  Saving model ...\nLoss decreased (0.005371 --> 0.005371).  Saving model ...\nLoss decreased (0.005371 --> 0.005370).  Saving model ...\nLoss decreased (0.005370 --> 0.005370).  Saving model ...\nLoss decreased (0.005370 --> 0.005370).  Saving model ...\nLoss decreased (0.005370 --> 0.005369).  Saving model ...\nLoss decreased (0.005369 --> 0.005369).  Saving model ...\nLoss decreased (0.005369 --> 0.005368).  Saving model ...\nLoss decreased (0.005368 --> 0.005368).  Saving model ...\nLoss decreased (0.005368 --> 0.005368).  Saving model ...\nLoss decreased (0.005368 --> 0.005367).  Saving model ...\nLoss decreased (0.005367 --> 0.005367).  Saving model ...\nLoss decreased (0.005367 --> 0.005366).  Saving model ...\nLoss decreased (0.005366 --> 0.005366).  Saving model ...\nLoss decreased (0.005366 --> 0.005365).  Saving model ...\nLoss decreased (0.005365 --> 0.005365).  Saving model ...\nLoss decreased (0.005365 --> 0.005364).  Saving model ...\nLoss decreased (0.005364 --> 0.005364).  Saving model ...\nLoss decreased (0.005364 --> 0.005363).  Saving model ...\nLoss decreased (0.005363 --> 0.005363).  Saving model ...\nLoss decreased (0.005363 --> 0.005362).  Saving model ...\nLoss decreased (0.005362 --> 0.005362).  Saving model ...\nLoss decreased (0.005362 --> 0.005361).  Saving model ...\nLoss decreased (0.005361 --> 0.005361).  Saving model ...\nLoss decreased (0.005361 --> 0.005360).  Saving model ...\nLoss decreased (0.005360 --> 0.005359).  Saving model ...\nLoss decreased (0.005359 --> 0.005359).  Saving model ...\nLoss decreased (0.005359 --> 0.005358).  Saving model ...\nLoss decreased (0.005358 --> 0.005357).  Saving model ...\nLoss decreased (0.005357 --> 0.005356).  Saving model ...\nLoss decreased (0.005356 --> 0.005356).  Saving model ...\nLoss decreased (0.005356 --> 0.005355).  Saving model ...\nLoss decreased (0.005355 --> 0.005354).  Saving model ...\nLoss decreased (0.005354 --> 0.005353).  Saving model ...\nLoss decreased (0.005353 --> 0.005352).  Saving model ...\nLoss decreased (0.005352 --> 0.005352).  Saving model ...\nLoss decreased (0.005352 --> 0.005351).  Saving model ...\nLoss decreased (0.005351 --> 0.005350).  Saving model ...\nLoss decreased (0.005350 --> 0.005349).  Saving model ...\nLoss decreased (0.005349 --> 0.005348).  Saving model ...\nLoss decreased (0.005348 --> 0.005347).  Saving model ...\nEpoch: 1200, loss: 0.00535\nLoss decreased (0.005347 --> 0.005346).  Saving model ...\nLoss decreased (0.005346 --> 0.005345).  Saving model ...\nLoss decreased (0.005345 --> 0.005343).  Saving model ...\nLoss decreased (0.005343 --> 0.005342).  Saving model ...\nLoss decreased (0.005342 --> 0.005341).  Saving model ...\nLoss decreased (0.005341 --> 0.005340).  Saving model ...\nLoss decreased (0.005340 --> 0.005339).  Saving model ...\nLoss decreased (0.005339 --> 0.005337).  Saving model ...\nLoss decreased (0.005337 --> 0.005336).  Saving model ...\nLoss decreased (0.005336 --> 0.005334).  Saving model ...\nLoss decreased (0.005334 --> 0.005333).  Saving model ...\nLoss decreased (0.005333 --> 0.005331).  Saving model ...\nLoss decreased (0.005331 --> 0.005330).  Saving model ...\nLoss decreased (0.005330 --> 0.005328).  Saving model ...\nLoss decreased (0.005328 --> 0.005327).  Saving model ...\nLoss decreased (0.005327 --> 0.005325).  Saving model ...\nLoss decreased (0.005325 --> 0.005323).  Saving model ...\nLoss decreased (0.005323 --> 0.005321).  Saving model ...\nLoss decreased (0.005321 --> 0.005320).  Saving model ...\nLoss decreased (0.005320 --> 0.005318).  Saving model ...\nLoss decreased (0.005318 --> 0.005316).  Saving model ...\nLoss decreased (0.005316 --> 0.005314).  Saving model ...\nLoss decreased (0.005314 --> 0.005312).  Saving model ...\nLoss decreased (0.005312 --> 0.005310).  Saving model ...\nLoss decreased (0.005310 --> 0.005308).  Saving model ...\nLoss decreased (0.005308 --> 0.005305).  Saving model ...\nLoss decreased (0.005305 --> 0.005303).  Saving model ...\nLoss decreased (0.005303 --> 0.005301).  Saving model ...\nLoss decreased (0.005301 --> 0.005299).  Saving model ...\nLoss decreased (0.005299 --> 0.005296).  Saving model ...\nLoss decreased (0.005296 --> 0.005294).  Saving model ...\nLoss decreased (0.005294 --> 0.005292).  Saving model ...\nLoss decreased (0.005292 --> 0.005289).  Saving model ...\nLoss decreased (0.005289 --> 0.005285).  Saving model ...\nLoss decreased (0.005285 --> 0.005281).  Saving model ...\nLoss decreased (0.005281 --> 0.005280).  Saving model ...\nLoss decreased (0.005280 --> 0.005277).  Saving model ...\nLoss decreased (0.005277 --> 0.005275).  Saving model ...\nLoss decreased (0.005275 --> 0.005273).  Saving model ...\nLoss decreased (0.005273 --> 0.005270).  Saving model ...\nLoss decreased (0.005270 --> 0.005269).  Saving model ...\nLoss decreased (0.005269 --> 0.005266).  Saving model ...\nLoss decreased (0.005266 --> 0.005266).  Saving model ...\nLoss decreased (0.005266 --> 0.005263).  Saving model ...\nLoss decreased (0.005263 --> 0.005261).  Saving model ...\nLoss decreased (0.005261 --> 0.005260).  Saving model ...\nLoss decreased (0.005260 --> 0.005258).  Saving model ...\nLoss decreased (0.005258 --> 0.005257).  Saving model ...\nLoss decreased (0.005257 --> 0.005255).  Saving model ...\nLoss decreased (0.005255 --> 0.005253).  Saving model ...\nLoss decreased (0.005253 --> 0.005253).  Saving model ...\nLoss decreased (0.005253 --> 0.005251).  Saving model ...\nLoss decreased (0.005251 --> 0.005250).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.005250 --> 0.005249).  Saving model ...\nLoss decreased (0.005249 --> 0.005248).  Saving model ...\nLoss decreased (0.005248 --> 0.005247).  Saving model ...\nLoss decreased (0.005247 --> 0.005246).  Saving model ...\nLoss decreased (0.005246 --> 0.005245).  Saving model ...\nLoss decreased (0.005245 --> 0.005244).  Saving model ...\nLoss decreased (0.005244 --> 0.005244).  Saving model ...\nLoss decreased (0.005244 --> 0.005242).  Saving model ...\nLoss decreased (0.005242 --> 0.005242).  Saving model ...\nLoss decreased (0.005242 --> 0.005241).  Saving model ...\nLoss decreased (0.005241 --> 0.005240).  Saving model ...\nLoss decreased (0.005240 --> 0.005240).  Saving model ...\nLoss decreased (0.005240 --> 0.005239).  Saving model ...\nLoss decreased (0.005239 --> 0.005238).  Saving model ...\nLoss decreased (0.005238 --> 0.005238).  Saving model ...\nLoss decreased (0.005238 --> 0.005237).  Saving model ...\nLoss decreased (0.005237 --> 0.005236).  Saving model ...\nLoss decreased (0.005236 --> 0.005236).  Saving model ...\nLoss decreased (0.005236 --> 0.005235).  Saving model ...\nLoss decreased (0.005235 --> 0.005235).  Saving model ...\nLoss decreased (0.005235 --> 0.005234).  Saving model ...\nLoss decreased (0.005234 --> 0.005234).  Saving model ...\nLoss decreased (0.005234 --> 0.005233).  Saving model ...\nLoss decreased (0.005233 --> 0.005233).  Saving model ...\nLoss decreased (0.005233 --> 0.005232).  Saving model ...\nLoss decreased (0.005232 --> 0.005231).  Saving model ...\nLoss decreased (0.005231 --> 0.005230).  Saving model ...\nLoss decreased (0.005230 --> 0.005230).  Saving model ...\nLoss decreased (0.005230 --> 0.005229).  Saving model ...\nLoss decreased (0.005229 --> 0.005229).  Saving model ...\nLoss decreased (0.005229 --> 0.005228).  Saving model ...\nLoss decreased (0.005228 --> 0.005228).  Saving model ...\nLoss decreased (0.005228 --> 0.005228).  Saving model ...\nLoss decreased (0.005228 --> 0.005227).  Saving model ...\nLoss decreased (0.005227 --> 0.005227).  Saving model ...\nLoss decreased (0.005227 --> 0.005227).  Saving model ...\nLoss decreased (0.005227 --> 0.005227).  Saving model ...\nEpoch: 1300, loss: 0.00523\nLoss decreased (0.005227 --> 0.005226).  Saving model ...\nLoss decreased (0.005226 --> 0.005226).  Saving model ...\nLoss decreased (0.005226 --> 0.005226).  Saving model ...\nLoss decreased (0.005226 --> 0.005225).  Saving model ...\nLoss decreased (0.005225 --> 0.005225).  Saving model ...\nLoss decreased (0.005225 --> 0.005225).  Saving model ...\nLoss decreased (0.005225 --> 0.005225).  Saving model ...\nLoss decreased (0.005225 --> 0.005224).  Saving model ...\nLoss decreased (0.005224 --> 0.005224).  Saving model ...\nLoss decreased (0.005224 --> 0.005224).  Saving model ...\nLoss decreased (0.005224 --> 0.005224).  Saving model ...\nLoss decreased (0.005224 --> 0.005224).  Saving model ...\nLoss decreased (0.005224 --> 0.005223).  Saving model ...\nLoss decreased (0.005223 --> 0.005223).  Saving model ...\nLoss decreased (0.005223 --> 0.005223).  Saving model ...\nLoss decreased (0.005223 --> 0.005223).  Saving model ...\nLoss decreased (0.005223 --> 0.005222).  Saving model ...\nLoss decreased (0.005222 --> 0.005222).  Saving model ...\nLoss decreased (0.005222 --> 0.005222).  Saving model ...\nLoss decreased (0.005222 --> 0.005222).  Saving model ...\nLoss decreased (0.005222 --> 0.005221).  Saving model ...\nLoss decreased (0.005221 --> 0.005221).  Saving model ...\nLoss decreased (0.005221 --> 0.005221).  Saving model ...\nLoss decreased (0.005221 --> 0.005221).  Saving model ...\nLoss decreased (0.005221 --> 0.005221).  Saving model ...\nLoss decreased (0.005221 --> 0.005220).  Saving model ...\nLoss decreased (0.005220 --> 0.005220).  Saving model ...\nLoss decreased (0.005220 --> 0.005220).  Saving model ...\nLoss decreased (0.005220 --> 0.005220).  Saving model ...\nLoss decreased (0.005220 --> 0.005220).  Saving model ...\nLoss decreased (0.005220 --> 0.005219).  Saving model ...\nLoss decreased (0.005219 --> 0.005219).  Saving model ...\nLoss decreased (0.005219 --> 0.005219).  Saving model ...\nLoss decreased (0.005219 --> 0.005219).  Saving model ...\nLoss decreased (0.005219 --> 0.005219).  Saving model ...\nLoss decreased (0.005219 --> 0.005218).  Saving model ...\nLoss decreased (0.005218 --> 0.005218).  Saving model ...\nLoss decreased (0.005218 --> 0.005218).  Saving model ...\nLoss decreased (0.005218 --> 0.005218).  Saving model ...\nLoss decreased (0.005218 --> 0.005218).  Saving model ...\nLoss decreased (0.005218 --> 0.005218).  Saving model ...\nLoss decreased (0.005218 --> 0.005217).  Saving model ...\nLoss decreased (0.005217 --> 0.005217).  Saving model ...\nLoss decreased (0.005217 --> 0.005217).  Saving model ...\nLoss decreased (0.005217 --> 0.005217).  Saving model ...\nLoss decreased (0.005217 --> 0.005217).  Saving model ...\nLoss decreased (0.005217 --> 0.005217).  Saving model ...\nLoss decreased (0.005217 --> 0.005216).  Saving model ...\nLoss decreased (0.005216 --> 0.005216).  Saving model ...\nLoss decreased (0.005216 --> 0.005216).  Saving model ...\nLoss decreased (0.005216 --> 0.005216).  Saving model ...\nLoss decreased (0.005216 --> 0.005215).  Saving model ...\nLoss decreased (0.005215 --> 0.005215).  Saving model ...\nLoss decreased (0.005215 --> 0.005215).  Saving model ...\nLoss decreased (0.005215 --> 0.005214).  Saving model ...\nLoss decreased (0.005214 --> 0.005214).  Saving model ...\nLoss decreased (0.005214 --> 0.005214).  Saving model ...\nLoss decreased (0.005214 --> 0.005214).  Saving model ...\nLoss decreased (0.005214 --> 0.005214).  Saving model ...\nLoss decreased (0.005214 --> 0.005213).  Saving model ...\nLoss decreased (0.005213 --> 0.005213).  Saving model ...\nLoss decreased (0.005213 --> 0.005213).  Saving model ...\nLoss decreased (0.005213 --> 0.005212).  Saving model ...\nLoss decreased (0.005212 --> 0.005212).  Saving model ...\nLoss decreased (0.005212 --> 0.005212).  Saving model ...\nLoss decreased (0.005212 --> 0.005211).  Saving model ...\nLoss decreased (0.005211 --> 0.005211).  Saving model ...\nLoss decreased (0.005211 --> 0.005211).  Saving model ...\nLoss decreased (0.005211 --> 0.005211).  Saving model ...\nEpoch: 1400, loss: 0.00521\nLoss decreased (0.005211 --> 0.005210).  Saving model ...\nLoss decreased (0.005210 --> 0.005210).  Saving model ...\nLoss decreased (0.005210 --> 0.005210).  Saving model ...\nLoss decreased (0.005210 --> 0.005210).  Saving model ...\nLoss decreased (0.005210 --> 0.005210).  Saving model ...\nLoss decreased (0.005210 --> 0.005210).  Saving model ...\nLoss decreased (0.005210 --> 0.005210).  Saving model ...\nLoss decreased (0.005210 --> 0.005209).  Saving model ...\nLoss decreased (0.005209 --> 0.005209).  Saving model ...\nLoss decreased (0.005209 --> 0.005209).  Saving model ...\nLoss decreased (0.005209 --> 0.005209).  Saving model ...\nLoss decreased (0.005209 --> 0.005209).  Saving model ...\nLoss decreased (0.005209 --> 0.005209).  Saving model ...\nLoss decreased (0.005209 --> 0.005209).  Saving model ...\nLoss decreased (0.005209 --> 0.005209).  Saving model ...\nLoss decreased (0.005209 --> 0.005208).  Saving model ...\nLoss decreased (0.005208 --> 0.005208).  Saving model ...\nLoss decreased (0.005208 --> 0.005208).  Saving model ...\nLoss decreased (0.005208 --> 0.005208).  Saving model ...\nLoss decreased (0.005208 --> 0.005208).  Saving model ...\nLoss decreased (0.005208 --> 0.005208).  Saving model ...\nLoss decreased (0.005208 --> 0.005207).  Saving model ...\nLoss decreased (0.005207 --> 0.005207).  Saving model ...\nLoss decreased (0.005207 --> 0.005207).  Saving model ...\nLoss decreased (0.005207 --> 0.005207).  Saving model ...\nLoss decreased (0.005207 --> 0.005207).  Saving model ...\nLoss decreased (0.005207 --> 0.005207).  Saving model ...\nLoss decreased (0.005207 --> 0.005207).  Saving model ...\nLoss decreased (0.005207 --> 0.005207).  Saving model ...\nLoss decreased (0.005207 --> 0.005207).  Saving model ...\nLoss decreased (0.005207 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005206).  Saving model ...\nLoss decreased (0.005206 --> 0.005205).  Saving model ...\nLoss decreased (0.005205 --> 0.005205).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.005205 --> 0.005205).  Saving model ...\nLoss decreased (0.005205 --> 0.005205).  Saving model ...\nLoss decreased (0.005205 --> 0.005205).  Saving model ...\nLoss decreased (0.005205 --> 0.005204).  Saving model ...\nLoss decreased (0.005204 --> 0.005204).  Saving model ...\nLoss decreased (0.005204 --> 0.005204).  Saving model ...\nEpoch: 1500, loss: 0.00520\nLoss decreased (0.005204 --> 0.005203).  Saving model ...\nLoss decreased (0.005203 --> 0.005203).  Saving model ...\nLoss decreased (0.005203 --> 0.005203).  Saving model ...\nLoss decreased (0.005203 --> 0.005203).  Saving model ...\nLoss decreased (0.005203 --> 0.005203).  Saving model ...\nLoss decreased (0.005203 --> 0.005203).  Saving model ...\nLoss decreased (0.005203 --> 0.005203).  Saving model ...\nLoss decreased (0.005203 --> 0.005203).  Saving model ...\nLoss decreased (0.005203 --> 0.005202).  Saving model ...\nLoss decreased (0.005202 --> 0.005202).  Saving model ...\nLoss decreased (0.005202 --> 0.005202).  Saving model ...\nLoss decreased (0.005202 --> 0.005202).  Saving model ...\nLoss decreased (0.005202 --> 0.005202).  Saving model ...\nLoss decreased (0.005202 --> 0.005202).  Saving model ...\nLoss decreased (0.005202 --> 0.005202).  Saving model ...\nLoss decreased (0.005202 --> 0.005202).  Saving model ...\nLoss decreased (0.005202 --> 0.005202).  Saving model ...\nLoss decreased (0.005202 --> 0.005202).  Saving model ...\nLoss decreased (0.005202 --> 0.005201).  Saving model ...\nLoss decreased (0.005201 --> 0.005201).  Saving model ...\nLoss decreased (0.005201 --> 0.005201).  Saving model ...\nLoss decreased (0.005201 --> 0.005201).  Saving model ...\nLoss decreased (0.005201 --> 0.005201).  Saving model ...\nLoss decreased (0.005201 --> 0.005201).  Saving model ...\nLoss decreased (0.005201 --> 0.005201).  Saving model ...\nLoss decreased (0.005201 --> 0.005201).  Saving model ...\nLoss decreased (0.005201 --> 0.005201).  Saving model ...\nLoss decreased (0.005201 --> 0.005200).  Saving model ...\nLoss decreased (0.005200 --> 0.005200).  Saving model ...\nLoss decreased (0.005200 --> 0.005200).  Saving model ...\nLoss decreased (0.005200 --> 0.005200).  Saving model ...\nLoss decreased (0.005200 --> 0.005200).  Saving model ...\nLoss decreased (0.005200 --> 0.005200).  Saving model ...\nLoss decreased (0.005200 --> 0.005200).  Saving model ...\nLoss decreased (0.005200 --> 0.005199).  Saving model ...\nLoss decreased (0.005199 --> 0.005199).  Saving model ...\nLoss decreased (0.005199 --> 0.005199).  Saving model ...\nLoss decreased (0.005199 --> 0.005199).  Saving model ...\nLoss decreased (0.005199 --> 0.005199).  Saving model ...\nLoss decreased (0.005199 --> 0.005199).  Saving model ...\nLoss decreased (0.005199 --> 0.005199).  Saving model ...\nLoss decreased (0.005199 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nEpoch: 1600, loss: 0.00520\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005198).  Saving model ...\nLoss decreased (0.005198 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005197).  Saving model ...\nLoss decreased (0.005197 --> 0.005196).  Saving model ...\nLoss decreased (0.005196 --> 0.005196).  Saving model ...\nLoss decreased (0.005196 --> 0.005196).  Saving model ...\nLoss decreased (0.005196 --> 0.005196).  Saving model ...\nLoss decreased (0.005196 --> 0.005196).  Saving model ...\nLoss decreased (0.005196 --> 0.005196).  Saving model ...\nLoss decreased (0.005196 --> 0.005196).  Saving model ...\nLoss decreased (0.005196 --> 0.005196).  Saving model ...\nLoss decreased (0.005196 --> 0.005195).  Saving model ...\nLoss decreased (0.005195 --> 0.005195).  Saving model ...\nLoss decreased (0.005195 --> 0.005195).  Saving model ...\nLoss decreased (0.005195 --> 0.005195).  Saving model ...\nLoss decreased (0.005195 --> 0.005195).  Saving model ...\nLoss decreased (0.005195 --> 0.005195).  Saving model ...\nLoss decreased (0.005195 --> 0.005195).  Saving model ...\nLoss decreased (0.005195 --> 0.005194).  Saving model ...\nLoss decreased (0.005194 --> 0.005194).  Saving model ...\nLoss decreased (0.005194 --> 0.005194).  Saving model ...\nLoss decreased (0.005194 --> 0.005194).  Saving model ...\nLoss decreased (0.005194 --> 0.005194).  Saving model ...\nLoss decreased (0.005194 --> 0.005193).  Saving model ...\nLoss decreased (0.005193 --> 0.005193).  Saving model ...\nLoss decreased (0.005193 --> 0.005193).  Saving model ...\nLoss decreased (0.005193 --> 0.005193).  Saving model ...\nLoss decreased (0.005193 --> 0.005192).  Saving model ...\nLoss decreased (0.005192 --> 0.005192).  Saving model ...\nLoss decreased (0.005192 --> 0.005192).  Saving model ...\nLoss decreased (0.005192 --> 0.005192).  Saving model ...\nLoss decreased (0.005192 --> 0.005192).  Saving model ...\nLoss decreased (0.005192 --> 0.005192).  Saving model ...\nLoss decreased (0.005192 --> 0.005191).  Saving model ...\nLoss decreased (0.005191 --> 0.005191).  Saving model ...\nLoss decreased (0.005191 --> 0.005191).  Saving model ...\nLoss decreased (0.005191 --> 0.005191).  Saving model ...\nLoss decreased (0.005191 --> 0.005191).  Saving model ...\nLoss decreased (0.005191 --> 0.005190).  Saving model ...\nLoss decreased (0.005190 --> 0.005190).  Saving model ...\nEpoch: 1700, loss: 0.00519\nLoss decreased (0.005190 --> 0.005190).  Saving model ...\nLoss decreased (0.005190 --> 0.005190).  Saving model ...\nLoss decreased (0.005190 --> 0.005189).  Saving model ...\nLoss decreased (0.005189 --> 0.005189).  Saving model ...\nLoss decreased (0.005189 --> 0.005189).  Saving model ...\nLoss decreased (0.005189 --> 0.005189).  Saving model ...\nLoss decreased (0.005189 --> 0.005189).  Saving model ...\nLoss decreased (0.005189 --> 0.005189).  Saving model ...\nLoss decreased (0.005189 --> 0.005189).  Saving model ...\nLoss decreased (0.005189 --> 0.005188).  Saving model ...\nLoss decreased (0.005188 --> 0.005188).  Saving model ...\nLoss decreased (0.005188 --> 0.005188).  Saving model ...\nLoss decreased (0.005188 --> 0.005187).  Saving model ...\nLoss decreased (0.005187 --> 0.005187).  Saving model ...\nLoss decreased (0.005187 --> 0.005187).  Saving model ...\nLoss decreased (0.005187 --> 0.005186).  Saving model ...\nLoss decreased (0.005186 --> 0.005186).  Saving model ...\nLoss decreased (0.005186 --> 0.005186).  Saving model ...\nLoss decreased (0.005186 --> 0.005186).  Saving model ...\nLoss decreased (0.005186 --> 0.005186).  Saving model ...\nLoss decreased (0.005186 --> 0.005186).  Saving model ...\nLoss decreased (0.005186 --> 0.005186).  Saving model ...\nLoss decreased (0.005186 --> 0.005185).  Saving model ...\nLoss decreased (0.005185 --> 0.005185).  Saving model ...\nLoss decreased (0.005185 --> 0.005185).  Saving model ...\nLoss decreased (0.005185 --> 0.005185).  Saving model ...\nLoss decreased (0.005185 --> 0.005185).  Saving model ...\nLoss decreased (0.005185 --> 0.005184).  Saving model ...\nLoss decreased (0.005184 --> 0.005184).  Saving model ...\nLoss decreased (0.005184 --> 0.005184).  Saving model ...\nLoss decreased (0.005184 --> 0.005184).  Saving model ...\nLoss decreased (0.005184 --> 0.005184).  Saving model ...\nLoss decreased (0.005184 --> 0.005184).  Saving model ...\nLoss decreased (0.005184 --> 0.005183).  Saving model ...\nLoss decreased (0.005183 --> 0.005183).  Saving model ...\nLoss decreased (0.005183 --> 0.005183).  Saving model ...\nLoss decreased (0.005183 --> 0.005183).  Saving model ...\nLoss decreased (0.005183 --> 0.005183).  Saving model ...\nLoss decreased (0.005183 --> 0.005182).  Saving model ...\nLoss decreased (0.005182 --> 0.005182).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.005182 --> 0.005182).  Saving model ...\nLoss decreased (0.005182 --> 0.005181).  Saving model ...\nLoss decreased (0.005181 --> 0.005181).  Saving model ...\nLoss decreased (0.005181 --> 0.005181).  Saving model ...\nLoss decreased (0.005181 --> 0.005180).  Saving model ...\nLoss decreased (0.005180 --> 0.005180).  Saving model ...\nLoss decreased (0.005180 --> 0.005180).  Saving model ...\nLoss decreased (0.005180 --> 0.005180).  Saving model ...\nLoss decreased (0.005180 --> 0.005179).  Saving model ...\nLoss decreased (0.005179 --> 0.005179).  Saving model ...\nEpoch: 1800, loss: 0.00518\nLoss decreased (0.005179 --> 0.005179).  Saving model ...\nLoss decreased (0.005179 --> 0.005178).  Saving model ...\nLoss decreased (0.005178 --> 0.005177).  Saving model ...\nLoss decreased (0.005177 --> 0.005177).  Saving model ...\nLoss decreased (0.005177 --> 0.005177).  Saving model ...\nLoss decreased (0.005177 --> 0.005177).  Saving model ...\nLoss decreased (0.005177 --> 0.005176).  Saving model ...\nLoss decreased (0.005176 --> 0.005176).  Saving model ...\nLoss decreased (0.005176 --> 0.005176).  Saving model ...\nLoss decreased (0.005176 --> 0.005176).  Saving model ...\nLoss decreased (0.005176 --> 0.005175).  Saving model ...\nLoss decreased (0.005175 --> 0.005175).  Saving model ...\nLoss decreased (0.005175 --> 0.005175).  Saving model ...\nLoss decreased (0.005175 --> 0.005175).  Saving model ...\nLoss decreased (0.005175 --> 0.005174).  Saving model ...\nLoss decreased (0.005174 --> 0.005174).  Saving model ...\nLoss decreased (0.005174 --> 0.005174).  Saving model ...\nLoss decreased (0.005174 --> 0.005173).  Saving model ...\nLoss decreased (0.005173 --> 0.005173).  Saving model ...\nLoss decreased (0.005173 --> 0.005173).  Saving model ...\nLoss decreased (0.005173 --> 0.005173).  Saving model ...\nLoss decreased (0.005173 --> 0.005172).  Saving model ...\nLoss decreased (0.005172 --> 0.005172).  Saving model ...\nLoss decreased (0.005172 --> 0.005172).  Saving model ...\nLoss decreased (0.005172 --> 0.005171).  Saving model ...\nLoss decreased (0.005171 --> 0.005171).  Saving model ...\nLoss decreased (0.005171 --> 0.005171).  Saving model ...\nLoss decreased (0.005171 --> 0.005171).  Saving model ...\nLoss decreased (0.005171 --> 0.005171).  Saving model ...\nLoss decreased (0.005171 --> 0.005170).  Saving model ...\nLoss decreased (0.005170 --> 0.005170).  Saving model ...\nLoss decreased (0.005170 --> 0.005169).  Saving model ...\nLoss decreased (0.005169 --> 0.005169).  Saving model ...\nLoss decreased (0.005169 --> 0.005168).  Saving model ...\nLoss decreased (0.005168 --> 0.005168).  Saving model ...\nLoss decreased (0.005168 --> 0.005167).  Saving model ...\nLoss decreased (0.005167 --> 0.005167).  Saving model ...\nLoss decreased (0.005167 --> 0.005167).  Saving model ...\nLoss decreased (0.005167 --> 0.005166).  Saving model ...\nLoss decreased (0.005166 --> 0.005166).  Saving model ...\nLoss decreased (0.005166 --> 0.005166).  Saving model ...\nLoss decreased (0.005166 --> 0.005165).  Saving model ...\nLoss decreased (0.005165 --> 0.005165).  Saving model ...\nLoss decreased (0.005165 --> 0.005165).  Saving model ...\nLoss decreased (0.005165 --> 0.005164).  Saving model ...\nLoss decreased (0.005164 --> 0.005163).  Saving model ...\nLoss decreased (0.005163 --> 0.005163).  Saving model ...\nLoss decreased (0.005163 --> 0.005163).  Saving model ...\nLoss decreased (0.005163 --> 0.005163).  Saving model ...\nLoss decreased (0.005163 --> 0.005162).  Saving model ...\nLoss decreased (0.005162 --> 0.005162).  Saving model ...\nLoss decreased (0.005162 --> 0.005161).  Saving model ...\nLoss decreased (0.005161 --> 0.005161).  Saving model ...\nLoss decreased (0.005161 --> 0.005160).  Saving model ...\nLoss decreased (0.005160 --> 0.005160).  Saving model ...\nLoss decreased (0.005160 --> 0.005160).  Saving model ...\nLoss decreased (0.005160 --> 0.005159).  Saving model ...\nLoss decreased (0.005159 --> 0.005158).  Saving model ...\nLoss decreased (0.005158 --> 0.005158).  Saving model ...\nLoss decreased (0.005158 --> 0.005158).  Saving model ...\nLoss decreased (0.005158 --> 0.005156).  Saving model ...\nLoss decreased (0.005156 --> 0.005155).  Saving model ...\nLoss decreased (0.005155 --> 0.005154).  Saving model ...\nLoss decreased (0.005154 --> 0.005152).  Saving model ...\nLoss decreased (0.005152 --> 0.005151).  Saving model ...\nLoss decreased (0.005151 --> 0.005151).  Saving model ...\nLoss decreased (0.005151 --> 0.005149).  Saving model ...\nLoss decreased (0.005149 --> 0.005149).  Saving model ...\nLoss decreased (0.005149 --> 0.005148).  Saving model ...\nLoss decreased (0.005148 --> 0.005147).  Saving model ...\nEpoch: 1900, loss: 0.00515\nLoss decreased (0.005147 --> 0.005146).  Saving model ...\nLoss decreased (0.005146 --> 0.005145).  Saving model ...\nLoss decreased (0.005145 --> 0.005144).  Saving model ...\nLoss decreased (0.005144 --> 0.005143).  Saving model ...\nLoss decreased (0.005143 --> 0.005142).  Saving model ...\nLoss decreased (0.005142 --> 0.005142).  Saving model ...\nLoss decreased (0.005142 --> 0.005141).  Saving model ...\nLoss decreased (0.005141 --> 0.005140).  Saving model ...\nLoss decreased (0.005140 --> 0.005140).  Saving model ...\nLoss decreased (0.005140 --> 0.005139).  Saving model ...\nLoss decreased (0.005139 --> 0.005138).  Saving model ...\nLoss decreased (0.005138 --> 0.005137).  Saving model ...\nLoss decreased (0.005137 --> 0.005137).  Saving model ...\nLoss decreased (0.005137 --> 0.005136).  Saving model ...\nLoss decreased (0.005136 --> 0.005135).  Saving model ...\nLoss decreased (0.005135 --> 0.005135).  Saving model ...\nLoss decreased (0.005135 --> 0.005134).  Saving model ...\nLoss decreased (0.005134 --> 0.005133).  Saving model ...\nLoss decreased (0.005133 --> 0.005132).  Saving model ...\nLoss decreased (0.005132 --> 0.005131).  Saving model ...\nLoss decreased (0.005131 --> 0.005130).  Saving model ...\nLoss decreased (0.005130 --> 0.005129).  Saving model ...\nLoss decreased (0.005129 --> 0.005129).  Saving model ...\nLoss decreased (0.005129 --> 0.005129).  Saving model ...\nLoss decreased (0.005129 --> 0.005128).  Saving model ...\nLoss decreased (0.005128 --> 0.005127).  Saving model ...\nLoss decreased (0.005127 --> 0.005126).  Saving model ...\nLoss decreased (0.005126 --> 0.005125).  Saving model ...\nLoss decreased (0.005125 --> 0.005124).  Saving model ...\nLoss decreased (0.005124 --> 0.005123).  Saving model ...\nLoss decreased (0.005123 --> 0.005122).  Saving model ...\nLoss decreased (0.005122 --> 0.005122).  Saving model ...\nLoss decreased (0.005122 --> 0.005121).  Saving model ...\nLoss decreased (0.005121 --> 0.005120).  Saving model ...\nLoss decreased (0.005120 --> 0.005119).  Saving model ...\nLoss decreased (0.005119 --> 0.005118).  Saving model ...\nLoss decreased (0.005118 --> 0.005118).  Saving model ...\nLoss decreased (0.005118 --> 0.005117).  Saving model ...\nLoss decreased (0.005117 --> 0.005117).  Saving model ...\nLoss decreased (0.005117 --> 0.005117).  Saving model ...\nLoss decreased (0.005117 --> 0.005114).  Saving model ...\nLoss decreased (0.005114 --> 0.005114).  Saving model ...\nLoss decreased (0.005114 --> 0.005113).  Saving model ...\nLoss decreased (0.005113 --> 0.005109).  Saving model ...\nLoss decreased (0.005109 --> 0.005109).  Saving model ...\nLoss decreased (0.005109 --> 0.005106).  Saving model ...\nLoss decreased (0.005106 --> 0.005105).  Saving model ...\nLoss decreased (0.005105 --> 0.005102).  Saving model ...\nLoss decreased (0.005102 --> 0.005098).  Saving model ...\nLoss decreased (0.005098 --> 0.005097).  Saving model ...\nLoss decreased (0.005097 --> 0.005095).  Saving model ...\nLoss decreased (0.005095 --> 0.005095).  Saving model ...\nLoss decreased (0.005095 --> 0.005094).  Saving model ...\nLoss decreased (0.005094 --> 0.005092).  Saving model ...\nLoss decreased (0.005092 --> 0.005091).  Saving model ...\nLoss decreased (0.005091 --> 0.005090).  Saving model ...\nLoss decreased (0.005090 --> 0.005088).  Saving model ...\nEpoch: 2000, loss: 0.00509\nLoss decreased (0.005088 --> 0.005087).  Saving model ...\nLoss decreased (0.005087 --> 0.005086).  Saving model ...\nLoss decreased (0.005086 --> 0.005085).  Saving model ...\nLoss decreased (0.005085 --> 0.005084).  Saving model ...\nLoss decreased (0.005084 --> 0.005083).  Saving model ...\nLoss decreased (0.005083 --> 0.005082).  Saving model ...\nLoss decreased (0.005082 --> 0.005082).  Saving model ...\nLoss decreased (0.005082 --> 0.005081).  Saving model ...\nLoss decreased (0.005081 --> 0.005081).  Saving model ...\nLoss decreased (0.005081 --> 0.005080).  Saving model ...\nLoss decreased (0.005080 --> 0.005079).  Saving model ...\nLoss decreased (0.005079 --> 0.005078).  Saving model ...\nLoss decreased (0.005078 --> 0.005078).  Saving model ...\nLoss decreased (0.005078 --> 0.005078).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.005078 --> 0.005077).  Saving model ...\nLoss decreased (0.005077 --> 0.005077).  Saving model ...\nLoss decreased (0.005077 --> 0.005076).  Saving model ...\nLoss decreased (0.005076 --> 0.005075).  Saving model ...\nLoss decreased (0.005075 --> 0.005075).  Saving model ...\nLoss decreased (0.005075 --> 0.005074).  Saving model ...\nLoss decreased (0.005074 --> 0.005074).  Saving model ...\nLoss decreased (0.005074 --> 0.005073).  Saving model ...\nLoss decreased (0.005073 --> 0.005073).  Saving model ...\nLoss decreased (0.005073 --> 0.005072).  Saving model ...\nLoss decreased (0.005072 --> 0.005072).  Saving model ...\nLoss decreased (0.005072 --> 0.005071).  Saving model ...\nLoss decreased (0.005071 --> 0.005071).  Saving model ...\nLoss decreased (0.005071 --> 0.005070).  Saving model ...\nLoss decreased (0.005070 --> 0.005069).  Saving model ...\nLoss decreased (0.005069 --> 0.005069).  Saving model ...\nLoss decreased (0.005069 --> 0.005068).  Saving model ...\nLoss decreased (0.005068 --> 0.005067).  Saving model ...\nLoss decreased (0.005067 --> 0.005067).  Saving model ...\nLoss decreased (0.005067 --> 0.005066).  Saving model ...\nLoss decreased (0.005066 --> 0.005066).  Saving model ...\nLoss decreased (0.005066 --> 0.005066).  Saving model ...\nLoss decreased (0.005066 --> 0.005065).  Saving model ...\nLoss decreased (0.005065 --> 0.005064).  Saving model ...\nLoss decreased (0.005064 --> 0.005064).  Saving model ...\nLoss decreased (0.005064 --> 0.005063).  Saving model ...\nLoss decreased (0.005063 --> 0.005062).  Saving model ...\nLoss decreased (0.005062 --> 0.005062).  Saving model ...\nLoss decreased (0.005062 --> 0.005061).  Saving model ...\nLoss decreased (0.005061 --> 0.005060).  Saving model ...\nLoss decreased (0.005060 --> 0.005060).  Saving model ...\nLoss decreased (0.005060 --> 0.005059).  Saving model ...\nLoss decreased (0.005059 --> 0.005058).  Saving model ...\nLoss decreased (0.005058 --> 0.005058).  Saving model ...\nLoss decreased (0.005058 --> 0.005057).  Saving model ...\nLoss decreased (0.005057 --> 0.005056).  Saving model ...\nLoss decreased (0.005056 --> 0.005056).  Saving model ...\nLoss decreased (0.005056 --> 0.005054).  Saving model ...\nLoss decreased (0.005054 --> 0.005053).  Saving model ...\nLoss decreased (0.005053 --> 0.005053).  Saving model ...\nLoss decreased (0.005053 --> 0.005051).  Saving model ...\nLoss decreased (0.005051 --> 0.005051).  Saving model ...\nLoss decreased (0.005051 --> 0.005050).  Saving model ...\nLoss decreased (0.005050 --> 0.005049).  Saving model ...\nLoss decreased (0.005049 --> 0.005047).  Saving model ...\nLoss decreased (0.005047 --> 0.005047).  Saving model ...\nLoss decreased (0.005047 --> 0.005045).  Saving model ...\nLoss decreased (0.005045 --> 0.005045).  Saving model ...\nLoss decreased (0.005045 --> 0.005043).  Saving model ...\nLoss decreased (0.005043 --> 0.005041).  Saving model ...\nLoss decreased (0.005041 --> 0.005039).  Saving model ...\nLoss decreased (0.005039 --> 0.005037).  Saving model ...\nLoss decreased (0.005037 --> 0.005037).  Saving model ...\nLoss decreased (0.005037 --> 0.005035).  Saving model ...\nEpoch: 2100, loss: 0.00503\nLoss decreased (0.005035 --> 0.005032).  Saving model ...\nLoss decreased (0.005032 --> 0.005030).  Saving model ...\nLoss decreased (0.005030 --> 0.005029).  Saving model ...\nLoss decreased (0.005029 --> 0.005026).  Saving model ...\nLoss decreased (0.005026 --> 0.005024).  Saving model ...\nLoss decreased (0.005024 --> 0.005023).  Saving model ...\nLoss decreased (0.005023 --> 0.005023).  Saving model ...\nLoss decreased (0.005023 --> 0.005021).  Saving model ...\nLoss decreased (0.005021 --> 0.005019).  Saving model ...\nLoss decreased (0.005019 --> 0.005017).  Saving model ...\nLoss decreased (0.005017 --> 0.005017).  Saving model ...\nLoss decreased (0.005017 --> 0.005017).  Saving model ...\nLoss decreased (0.005017 --> 0.005014).  Saving model ...\nLoss decreased (0.005014 --> 0.005013).  Saving model ...\nLoss decreased (0.005013 --> 0.005013).  Saving model ...\nLoss decreased (0.005013 --> 0.005012).  Saving model ...\nLoss decreased (0.005012 --> 0.005010).  Saving model ...\nLoss decreased (0.005010 --> 0.005008).  Saving model ...\nLoss decreased (0.005008 --> 0.005007).  Saving model ...\nLoss decreased (0.005007 --> 0.005005).  Saving model ...\nLoss decreased (0.005005 --> 0.005002).  Saving model ...\nLoss decreased (0.005002 --> 0.005000).  Saving model ...\nLoss decreased (0.005000 --> 0.004998).  Saving model ...\nLoss decreased (0.004998 --> 0.004995).  Saving model ...\nLoss decreased (0.004995 --> 0.004995).  Saving model ...\nLoss decreased (0.004995 --> 0.004992).  Saving model ...\nLoss decreased (0.004992 --> 0.004991).  Saving model ...\nLoss decreased (0.004991 --> 0.004991).  Saving model ...\nLoss decreased (0.004991 --> 0.004989).  Saving model ...\nLoss decreased (0.004989 --> 0.004988).  Saving model ...\nLoss decreased (0.004988 --> 0.004987).  Saving model ...\nLoss decreased (0.004987 --> 0.004986).  Saving model ...\nLoss decreased (0.004986 --> 0.004985).  Saving model ...\nLoss decreased (0.004985 --> 0.004984).  Saving model ...\nLoss decreased (0.004984 --> 0.004983).  Saving model ...\nLoss decreased (0.004983 --> 0.004983).  Saving model ...\nLoss decreased (0.004983 --> 0.004982).  Saving model ...\nLoss decreased (0.004982 --> 0.004979).  Saving model ...\nLoss decreased (0.004979 --> 0.004975).  Saving model ...\nLoss decreased (0.004975 --> 0.004975).  Saving model ...\nLoss decreased (0.004975 --> 0.004971).  Saving model ...\nLoss decreased (0.004971 --> 0.004971).  Saving model ...\nLoss decreased (0.004971 --> 0.004967).  Saving model ...\nEpoch: 2200, loss: 0.00497\nLoss decreased (0.004967 --> 0.004964).  Saving model ...\nLoss decreased (0.004964 --> 0.004963).  Saving model ...\nLoss decreased (0.004963 --> 0.004962).  Saving model ...\nLoss decreased (0.004962 --> 0.004959).  Saving model ...\nLoss decreased (0.004959 --> 0.004958).  Saving model ...\nLoss decreased (0.004958 --> 0.004956).  Saving model ...\nLoss decreased (0.004956 --> 0.004955).  Saving model ...\nLoss decreased (0.004955 --> 0.004954).  Saving model ...\nLoss decreased (0.004954 --> 0.004950).  Saving model ...\nLoss decreased (0.004950 --> 0.004947).  Saving model ...\nLoss decreased (0.004947 --> 0.004944).  Saving model ...\nLoss decreased (0.004944 --> 0.004943).  Saving model ...\nLoss decreased (0.004943 --> 0.004941).  Saving model ...\nLoss decreased (0.004941 --> 0.004941).  Saving model ...\nLoss decreased (0.004941 --> 0.004939).  Saving model ...\nLoss decreased (0.004939 --> 0.004937).  Saving model ...\nLoss decreased (0.004937 --> 0.004936).  Saving model ...\nLoss decreased (0.004936 --> 0.004935).  Saving model ...\nLoss decreased (0.004935 --> 0.004934).  Saving model ...\nLoss decreased (0.004934 --> 0.004934).  Saving model ...\nLoss decreased (0.004934 --> 0.004931).  Saving model ...\nLoss decreased (0.004931 --> 0.004931).  Saving model ...\nLoss decreased (0.004931 --> 0.004929).  Saving model ...\nLoss decreased (0.004929 --> 0.004927).  Saving model ...\nLoss decreased (0.004927 --> 0.004926).  Saving model ...\nLoss decreased (0.004926 --> 0.004925).  Saving model ...\nLoss decreased (0.004925 --> 0.004923).  Saving model ...\nLoss decreased (0.004923 --> 0.004923).  Saving model ...\nLoss decreased (0.004923 --> 0.004921).  Saving model ...\nLoss decreased (0.004921 --> 0.004920).  Saving model ...\nLoss decreased (0.004920 --> 0.004918).  Saving model ...\nLoss decreased (0.004918 --> 0.004917).  Saving model ...\nLoss decreased (0.004917 --> 0.004916).  Saving model ...\nLoss decreased (0.004916 --> 0.004904).  Saving model ...\nLoss decreased (0.004904 --> 0.004899).  Saving model ...\nLoss decreased (0.004899 --> 0.004896).  Saving model ...\nLoss decreased (0.004896 --> 0.004891).  Saving model ...\nLoss decreased (0.004891 --> 0.004891).  Saving model ...\nLoss decreased (0.004891 --> 0.004889).  Saving model ...\nLoss decreased (0.004889 --> 0.004887).  Saving model ...\nLoss decreased (0.004887 --> 0.004886).  Saving model ...\nLoss decreased (0.004886 --> 0.004878).  Saving model ...\nLoss decreased (0.004878 --> 0.004878).  Saving model ...\nLoss decreased (0.004878 --> 0.004877).  Saving model ...\nLoss decreased (0.004877 --> 0.004872).  Saving model ...\nLoss decreased (0.004872 --> 0.004865).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.004865 --> 0.004862).  Saving model ...\nEpoch: 2300, loss: 0.00486\nLoss decreased (0.004862 --> 0.004860).  Saving model ...\nLoss decreased (0.004860 --> 0.004855).  Saving model ...\nLoss decreased (0.004855 --> 0.004854).  Saving model ...\nLoss decreased (0.004854 --> 0.004848).  Saving model ...\nLoss decreased (0.004848 --> 0.004846).  Saving model ...\nLoss decreased (0.004846 --> 0.004845).  Saving model ...\nLoss decreased (0.004845 --> 0.004842).  Saving model ...\nLoss decreased (0.004842 --> 0.004837).  Saving model ...\nLoss decreased (0.004837 --> 0.004831).  Saving model ...\nLoss decreased (0.004831 --> 0.004831).  Saving model ...\nLoss decreased (0.004831 --> 0.004828).  Saving model ...\nLoss decreased (0.004828 --> 0.004824).  Saving model ...\nLoss decreased (0.004824 --> 0.004822).  Saving model ...\nLoss decreased (0.004822 --> 0.004818).  Saving model ...\nLoss decreased (0.004818 --> 0.004816).  Saving model ...\nLoss decreased (0.004816 --> 0.004813).  Saving model ...\nLoss decreased (0.004813 --> 0.004811).  Saving model ...\nLoss decreased (0.004811 --> 0.004807).  Saving model ...\nLoss decreased (0.004807 --> 0.004805).  Saving model ...\nLoss decreased (0.004805 --> 0.004805).  Saving model ...\nLoss decreased (0.004805 --> 0.004803).  Saving model ...\nLoss decreased (0.004803 --> 0.004800).  Saving model ...\nLoss decreased (0.004800 --> 0.004799).  Saving model ...\nLoss decreased (0.004799 --> 0.004797).  Saving model ...\nLoss decreased (0.004797 --> 0.004794).  Saving model ...\nLoss decreased (0.004794 --> 0.004789).  Saving model ...\nLoss decreased (0.004789 --> 0.004788).  Saving model ...\nEpoch: 2400, loss: 0.00481\nLoss decreased (0.004788 --> 0.004783).  Saving model ...\nLoss decreased (0.004783 --> 0.004782).  Saving model ...\nLoss decreased (0.004782 --> 0.004774).  Saving model ...\nLoss decreased (0.004774 --> 0.004771).  Saving model ...\nLoss decreased (0.004771 --> 0.004765).  Saving model ...\nLoss decreased (0.004765 --> 0.004764).  Saving model ...\nLoss decreased (0.004764 --> 0.004760).  Saving model ...\nLoss decreased (0.004760 --> 0.004757).  Saving model ...\nLoss decreased (0.004757 --> 0.004755).  Saving model ...\nLoss decreased (0.004755 --> 0.004753).  Saving model ...\nLoss decreased (0.004753 --> 0.004752).  Saving model ...\nLoss decreased (0.004752 --> 0.004746).  Saving model ...\nLoss decreased (0.004746 --> 0.004744).  Saving model ...\nLoss decreased (0.004744 --> 0.004739).  Saving model ...\nLoss decreased (0.004739 --> 0.004734).  Saving model ...\nLoss decreased (0.004734 --> 0.004726).  Saving model ...\nEpoch: 2500, loss: 0.00474\nLoss decreased (0.004726 --> 0.004722).  Saving model ...\nLoss decreased (0.004722 --> 0.004715).  Saving model ...\nLoss decreased (0.004715 --> 0.004712).  Saving model ...\nLoss decreased (0.004712 --> 0.004710).  Saving model ...\nLoss decreased (0.004710 --> 0.004708).  Saving model ...\nLoss decreased (0.004708 --> 0.004705).  Saving model ...\nLoss decreased (0.004705 --> 0.004705).  Saving model ...\nLoss decreased (0.004705 --> 0.004704).  Saving model ...\nLoss decreased (0.004704 --> 0.004700).  Saving model ...\nLoss decreased (0.004700 --> 0.004698).  Saving model ...\nLoss decreased (0.004698 --> 0.004696).  Saving model ...\nLoss decreased (0.004696 --> 0.004695).  Saving model ...\nLoss decreased (0.004695 --> 0.004694).  Saving model ...\nLoss decreased (0.004694 --> 0.004693).  Saving model ...\nLoss decreased (0.004693 --> 0.004688).  Saving model ...\nLoss decreased (0.004688 --> 0.004687).  Saving model ...\nLoss decreased (0.004687 --> 0.004685).  Saving model ...\nLoss decreased (0.004685 --> 0.004682).  Saving model ...\nLoss decreased (0.004682 --> 0.004682).  Saving model ...\nLoss decreased (0.004682 --> 0.004681).  Saving model ...\nLoss decreased (0.004681 --> 0.004680).  Saving model ...\nLoss decreased (0.004680 --> 0.004677).  Saving model ...\nLoss decreased (0.004677 --> 0.004677).  Saving model ...\nLoss decreased (0.004677 --> 0.004676).  Saving model ...\nLoss decreased (0.004676 --> 0.004674).  Saving model ...\nLoss decreased (0.004674 --> 0.004673).  Saving model ...\nEpoch: 2600, loss: 0.00478\nLoss decreased (0.004673 --> 0.004667).  Saving model ...\nLoss decreased (0.004667 --> 0.004663).  Saving model ...\nLoss decreased (0.004663 --> 0.004658).  Saving model ...\nLoss decreased (0.004658 --> 0.004653).  Saving model ...\nLoss decreased (0.004653 --> 0.004648).  Saving model ...\nLoss decreased (0.004648 --> 0.004647).  Saving model ...\nLoss decreased (0.004647 --> 0.004647).  Saving model ...\nLoss decreased (0.004647 --> 0.004644).  Saving model ...\nLoss decreased (0.004644 --> 0.004643).  Saving model ...\nLoss decreased (0.004643 --> 0.004641).  Saving model ...\nLoss decreased (0.004641 --> 0.004628).  Saving model ...\nLoss decreased (0.004628 --> 0.004627).  Saving model ...\nEpoch: 2700, loss: 0.00463\nLoss decreased (0.004627 --> 0.004619).  Saving model ...\nLoss decreased (0.004619 --> 0.004618).  Saving model ...\nLoss decreased (0.004618 --> 0.004617).  Saving model ...\nLoss decreased (0.004617 --> 0.004613).  Saving model ...\nLoss decreased (0.004613 --> 0.004609).  Saving model ...\nLoss decreased (0.004609 --> 0.004606).  Saving model ...\nLoss decreased (0.004606 --> 0.004594).  Saving model ...\nLoss decreased (0.004594 --> 0.004587).  Saving model ...\nLoss decreased (0.004587 --> 0.004586).  Saving model ...\nLoss decreased (0.004586 --> 0.004578).  Saving model ...\nLoss decreased (0.004578 --> 0.004575).  Saving model ...\nLoss decreased (0.004575 --> 0.004571).  Saving model ...\nEpoch: 2800, loss: 0.00458\nLoss decreased (0.004571 --> 0.004565).  Saving model ...\nLoss decreased (0.004565 --> 0.004564).  Saving model ...\nLoss decreased (0.004564 --> 0.004556).  Saving model ...\nLoss decreased (0.004556 --> 0.004554).  Saving model ...\nLoss decreased (0.004554 --> 0.004551).  Saving model ...\nLoss decreased (0.004551 --> 0.004549).  Saving model ...\nLoss decreased (0.004549 --> 0.004543).  Saving model ...\nLoss decreased (0.004543 --> 0.004540).  Saving model ...\nLoss decreased (0.004540 --> 0.004538).  Saving model ...\nLoss decreased (0.004538 --> 0.004534).  Saving model ...\nLoss decreased (0.004534 --> 0.004532).  Saving model ...\nLoss decreased (0.004532 --> 0.004528).  Saving model ...\nEpoch: 2900, loss: 0.00453\nLoss decreased (0.004528 --> 0.004526).  Saving model ...\nLoss decreased (0.004526 --> 0.004523).  Saving model ...\nLoss decreased (0.004523 --> 0.004518).  Saving model ...\nLoss decreased (0.004518 --> 0.004515).  Saving model ...\nLoss decreased (0.004515 --> 0.004514).  Saving model ...\nLoss decreased (0.004514 --> 0.004511).  Saving model ...\nLoss decreased (0.004511 --> 0.004509).  Saving model ...\nLoss decreased (0.004509 --> 0.004508).  Saving model ...\nLoss decreased (0.004508 --> 0.004506).  Saving model ...\nLoss decreased (0.004506 --> 0.004504).  Saving model ...\nLoss decreased (0.004504 --> 0.004504).  Saving model ...\nLoss decreased (0.004504 --> 0.004502).  Saving model ...\nLoss decreased (0.004502 --> 0.004500).  Saving model ...\nLoss decreased (0.004500 --> 0.004499).  Saving model ...\nLoss decreased (0.004499 --> 0.004498).  Saving model ...\nLoss decreased (0.004498 --> 0.004498).  Saving model ...\nLoss decreased (0.004498 --> 0.004497).  Saving model ...\nLoss decreased (0.004497 --> 0.004497).  Saving model ...\nLoss decreased (0.004497 --> 0.004497).  Saving model ...\nLoss decreased (0.004497 --> 0.004494).  Saving model ...\nLoss decreased (0.004494 --> 0.004494).  Saving model ...\nLoss decreased (0.004494 --> 0.004493).  Saving model ...\nLoss decreased (0.004493 --> 0.004492).  Saving model ...\nLoss decreased (0.004492 --> 0.004491).  Saving model ...\nLoss decreased (0.004491 --> 0.004487).  Saving model ...\nLoss decreased (0.004487 --> 0.004485).  Saving model ...\nLoss decreased (0.004485 --> 0.004483).  Saving model ...\nEpoch: 3000, loss: 0.00450\nLoss decreased (0.004483 --> 0.004483).  Saving model ...\nLoss decreased (0.004483 --> 0.004477).  Saving model ...\nLoss decreased (0.004477 --> 0.004477).  Saving model ...\nLoss decreased (0.004477 --> 0.004473).  Saving model ...\nLoss decreased (0.004473 --> 0.004471).  Saving model ...\nLoss decreased (0.004471 --> 0.004464).  Saving model ...\nLoss decreased (0.004464 --> 0.004463).  Saving model ...\nLoss decreased (0.004463 --> 0.004460).  Saving model ...\nLoss decreased (0.004460 --> 0.004458).  Saving model ...\nLoss decreased (0.004458 --> 0.004455).  Saving model ...\nLoss decreased (0.004455 --> 0.004452).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.004452 --> 0.004447).  Saving model ...\nLoss decreased (0.004447 --> 0.004445).  Saving model ...\nLoss decreased (0.004445 --> 0.004444).  Saving model ...\nLoss decreased (0.004444 --> 0.004439).  Saving model ...\nLoss decreased (0.004439 --> 0.004437).  Saving model ...\nLoss decreased (0.004437 --> 0.004431).  Saving model ...\nLoss decreased (0.004431 --> 0.004430).  Saving model ...\nEpoch: 3100, loss: 0.00443\nLoss decreased (0.004430 --> 0.004429).  Saving model ...\nLoss decreased (0.004429 --> 0.004425).  Saving model ...\nLoss decreased (0.004425 --> 0.004424).  Saving model ...\nLoss decreased (0.004424 --> 0.004414).  Saving model ...\nLoss decreased (0.004414 --> 0.004405).  Saving model ...\nLoss decreased (0.004405 --> 0.004401).  Saving model ...\nEpoch: 3200, loss: 0.00454\nLoss decreased (0.004401 --> 0.004392).  Saving model ...\nLoss decreased (0.004392 --> 0.004383).  Saving model ...\nLoss decreased (0.004383 --> 0.004382).  Saving model ...\nLoss decreased (0.004382 --> 0.004381).  Saving model ...\nLoss decreased (0.004381 --> 0.004374).  Saving model ...\nLoss decreased (0.004374 --> 0.004371).  Saving model ...\nLoss decreased (0.004371 --> 0.004369).  Saving model ...\nLoss decreased (0.004369 --> 0.004363).  Saving model ...\nLoss decreased (0.004363 --> 0.004361).  Saving model ...\nLoss decreased (0.004361 --> 0.004360).  Saving model ...\nLoss decreased (0.004360 --> 0.004359).  Saving model ...\nLoss decreased (0.004359 --> 0.004358).  Saving model ...\nLoss decreased (0.004358 --> 0.004356).  Saving model ...\nLoss decreased (0.004356 --> 0.004354).  Saving model ...\nEpoch: 3300, loss: 0.00439\nLoss decreased (0.004354 --> 0.004350).  Saving model ...\nLoss decreased (0.004350 --> 0.004340).  Saving model ...\nLoss decreased (0.004340 --> 0.004335).  Saving model ...\nLoss decreased (0.004335 --> 0.004329).  Saving model ...\nLoss decreased (0.004329 --> 0.004329).  Saving model ...\nLoss decreased (0.004329 --> 0.004324).  Saving model ...\nLoss decreased (0.004324 --> 0.004317).  Saving model ...\nLoss decreased (0.004317 --> 0.004317).  Saving model ...\nEpoch: 3400, loss: 0.00434\nLoss decreased (0.004317 --> 0.004315).  Saving model ...\nLoss decreased (0.004315 --> 0.004314).  Saving model ...\nLoss decreased (0.004314 --> 0.004307).  Saving model ...\nLoss decreased (0.004307 --> 0.004300).  Saving model ...\nLoss decreased (0.004300 --> 0.004293).  Saving model ...\nLoss decreased (0.004293 --> 0.004288).  Saving model ...\nEpoch: 3500, loss: 0.00434\nLoss decreased (0.004288 --> 0.004282).  Saving model ...\nLoss decreased (0.004282 --> 0.004282).  Saving model ...\nLoss decreased (0.004282 --> 0.004280).  Saving model ...\nLoss decreased (0.004280 --> 0.004279).  Saving model ...\nLoss decreased (0.004279 --> 0.004275).  Saving model ...\nLoss decreased (0.004275 --> 0.004267).  Saving model ...\nLoss decreased (0.004267 --> 0.004263).  Saving model ...\nLoss decreased (0.004263 --> 0.004262).  Saving model ...\nLoss decreased (0.004262 --> 0.004259).  Saving model ...\nLoss decreased (0.004259 --> 0.004253).  Saving model ...\nLoss decreased (0.004253 --> 0.004248).  Saving model ...\nLoss decreased (0.004248 --> 0.004246).  Saving model ...\nLoss decreased (0.004246 --> 0.004243).  Saving model ...\nLoss decreased (0.004243 --> 0.004243).  Saving model ...\nEpoch: 3600, loss: 0.00430\nLoss decreased (0.004243 --> 0.004243).  Saving model ...\nLoss decreased (0.004243 --> 0.004237).  Saving model ...\nLoss decreased (0.004237 --> 0.004235).  Saving model ...\nLoss decreased (0.004235 --> 0.004228).  Saving model ...\nLoss decreased (0.004228 --> 0.004228).  Saving model ...\nLoss decreased (0.004228 --> 0.004227).  Saving model ...\nLoss decreased (0.004227 --> 0.004225).  Saving model ...\nLoss decreased (0.004225 --> 0.004219).  Saving model ...\nLoss decreased (0.004219 --> 0.004218).  Saving model ...\nEpoch: 3700, loss: 0.00422\nLoss decreased (0.004218 --> 0.004211).  Saving model ...\nLoss decreased (0.004211 --> 0.004205).  Saving model ...\nLoss decreased (0.004205 --> 0.004203).  Saving model ...\nLoss decreased (0.004203 --> 0.004202).  Saving model ...\nLoss decreased (0.004202 --> 0.004201).  Saving model ...\nLoss decreased (0.004201 --> 0.004199).  Saving model ...\nLoss decreased (0.004199 --> 0.004198).  Saving model ...\nLoss decreased (0.004198 --> 0.004196).  Saving model ...\nLoss decreased (0.004196 --> 0.004196).  Saving model ...\nLoss decreased (0.004196 --> 0.004195).  Saving model ...\nLoss decreased (0.004195 --> 0.004193).  Saving model ...\nLoss decreased (0.004193 --> 0.004191).  Saving model ...\nLoss decreased (0.004191 --> 0.004190).  Saving model ...\nLoss decreased (0.004190 --> 0.004188).  Saving model ...\nLoss decreased (0.004188 --> 0.004184).  Saving model ...\nLoss decreased (0.004184 --> 0.004183).  Saving model ...\nLoss decreased (0.004183 --> 0.004181).  Saving model ...\nLoss decreased (0.004181 --> 0.004176).  Saving model ...\nEpoch: 3800, loss: 0.00431\nLoss decreased (0.004176 --> 0.004176).  Saving model ...\nLoss decreased (0.004176 --> 0.004173).  Saving model ...\nLoss decreased (0.004173 --> 0.004167).  Saving model ...\nLoss decreased (0.004167 --> 0.004165).  Saving model ...\nLoss decreased (0.004165 --> 0.004157).  Saving model ...\nLoss decreased (0.004157 --> 0.004156).  Saving model ...\nLoss decreased (0.004156 --> 0.004155).  Saving model ...\nLoss decreased (0.004155 --> 0.004154).  Saving model ...\nLoss decreased (0.004154 --> 0.004153).  Saving model ...\nEpoch: 3900, loss: 0.00418\nLoss decreased (0.004153 --> 0.004147).  Saving model ...\nLoss decreased (0.004147 --> 0.004142).  Saving model ...\nLoss decreased (0.004142 --> 0.004141).  Saving model ...\nLoss decreased (0.004141 --> 0.004136).  Saving model ...\nLoss decreased (0.004136 --> 0.004135).  Saving model ...\nLoss decreased (0.004135 --> 0.004133).  Saving model ...\nLoss decreased (0.004133 --> 0.004131).  Saving model ...\nLoss decreased (0.004131 --> 0.004129).  Saving model ...\nLoss decreased (0.004129 --> 0.004128).  Saving model ...\nLoss decreased (0.004128 --> 0.004125).  Saving model ...\nEpoch: 4000, loss: 0.00413\nLoss decreased (0.004125 --> 0.004124).  Saving model ...\nLoss decreased (0.004124 --> 0.004124).  Saving model ...\nLoss decreased (0.004124 --> 0.004124).  Saving model ...\nLoss decreased (0.004124 --> 0.004122).  Saving model ...\nLoss decreased (0.004122 --> 0.004118).  Saving model ...\nLoss decreased (0.004118 --> 0.004117).  Saving model ...\nLoss decreased (0.004117 --> 0.004116).  Saving model ...\nLoss decreased (0.004116 --> 0.004115).  Saving model ...\nLoss decreased (0.004115 --> 0.004112).  Saving model ...\nLoss decreased (0.004112 --> 0.004111).  Saving model ...\nLoss decreased (0.004111 --> 0.004107).  Saving model ...\nEpoch: 4100, loss: 0.00411\nLoss decreased (0.004107 --> 0.004102).  Saving model ...\nLoss decreased (0.004102 --> 0.004101).  Saving model ...\nLoss decreased (0.004101 --> 0.004099).  Saving model ...\nLoss decreased (0.004099 --> 0.004098).  Saving model ...\nLoss decreased (0.004098 --> 0.004095).  Saving model ...\nLoss decreased (0.004095 --> 0.004094).  Saving model ...\nLoss decreased (0.004094 --> 0.004092).  Saving model ...\nLoss decreased (0.004092 --> 0.004089).  Saving model ...\nLoss decreased (0.004089 --> 0.004088).  Saving model ...\nEpoch: 4200, loss: 0.00409\nLoss decreased (0.004088 --> 0.004081).  Saving model ...\nLoss decreased (0.004081 --> 0.004080).  Saving model ...\nLoss decreased (0.004080 --> 0.004079).  Saving model ...\nLoss decreased (0.004079 --> 0.004077).  Saving model ...\nLoss decreased (0.004077 --> 0.004073).  Saving model ...\nLoss decreased (0.004073 --> 0.004069).  Saving model ...\nLoss decreased (0.004069 --> 0.004066).  Saving model ...\nLoss decreased (0.004066 --> 0.004066).  Saving model ...\nLoss decreased (0.004066 --> 0.004066).  Saving model ...\nLoss decreased (0.004066 --> 0.004063).  Saving model ...\nLoss decreased (0.004063 --> 0.004063).  Saving model ...\nLoss decreased (0.004063 --> 0.004060).  Saving model ...\nLoss decreased (0.004060 --> 0.004060).  Saving model ...\nLoss decreased (0.004060 --> 0.004059).  Saving model ...\nLoss decreased (0.004059 --> 0.004056).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.004056 --> 0.004055).  Saving model ...\nLoss decreased (0.004055 --> 0.004054).  Saving model ...\nLoss decreased (0.004054 --> 0.004053).  Saving model ...\nLoss decreased (0.004053 --> 0.004053).  Saving model ...\nLoss decreased (0.004053 --> 0.004052).  Saving model ...\nLoss decreased (0.004052 --> 0.004052).  Saving model ...\nLoss decreased (0.004052 --> 0.004048).  Saving model ...\nLoss decreased (0.004048 --> 0.004047).  Saving model ...\nLoss decreased (0.004047 --> 0.004041).  Saving model ...\nLoss decreased (0.004041 --> 0.004040).  Saving model ...\nEpoch: 4300, loss: 0.00406\nLoss decreased (0.004040 --> 0.004037).  Saving model ...\nLoss decreased (0.004037 --> 0.004033).  Saving model ...\nLoss decreased (0.004033 --> 0.004029).  Saving model ...\nLoss decreased (0.004029 --> 0.004027).  Saving model ...\nEpoch: 4400, loss: 0.00413\nLoss decreased (0.004027 --> 0.004023).  Saving model ...\nLoss decreased (0.004023 --> 0.004014).  Saving model ...\nLoss decreased (0.004014 --> 0.004013).  Saving model ...\nLoss decreased (0.004013 --> 0.004011).  Saving model ...\nLoss decreased (0.004011 --> 0.004002).  Saving model ...\nLoss decreased (0.004002 --> 0.003998).  Saving model ...\nLoss decreased (0.003998 --> 0.003995).  Saving model ...\nLoss decreased (0.003995 --> 0.003995).  Saving model ...\nLoss decreased (0.003995 --> 0.003989).  Saving model ...\nEpoch: 4500, loss: 0.00400\nLoss decreased (0.003989 --> 0.003986).  Saving model ...\nLoss decreased (0.003986 --> 0.003984).  Saving model ...\nLoss decreased (0.003984 --> 0.003982).  Saving model ...\nLoss decreased (0.003982 --> 0.003976).  Saving model ...\nLoss decreased (0.003976 --> 0.003975).  Saving model ...\nLoss decreased (0.003975 --> 0.003973).  Saving model ...\nLoss decreased (0.003973 --> 0.003972).  Saving model ...\nLoss decreased (0.003972 --> 0.003971).  Saving model ...\nLoss decreased (0.003971 --> 0.003970).  Saving model ...\nLoss decreased (0.003970 --> 0.003968).  Saving model ...\nLoss decreased (0.003968 --> 0.003966).  Saving model ...\nLoss decreased (0.003966 --> 0.003963).  Saving model ...\nLoss decreased (0.003963 --> 0.003963).  Saving model ...\nLoss decreased (0.003963 --> 0.003961).  Saving model ...\nEpoch: 4600, loss: 0.00396\nLoss decreased (0.003961 --> 0.003960).  Saving model ...\nLoss decreased (0.003960 --> 0.003960).  Saving model ...\nLoss decreased (0.003960 --> 0.003959).  Saving model ...\nLoss decreased (0.003959 --> 0.003957).  Saving model ...\nEpoch: 4700, loss: 0.00405\nLoss decreased (0.003957 --> 0.003955).  Saving model ...\nLoss decreased (0.003955 --> 0.003951).  Saving model ...\nLoss decreased (0.003951 --> 0.003950).  Saving model ...\nLoss decreased (0.003950 --> 0.003950).  Saving model ...\nLoss decreased (0.003950 --> 0.003948).  Saving model ...\nLoss decreased (0.003948 --> 0.003946).  Saving model ...\nLoss decreased (0.003946 --> 0.003946).  Saving model ...\nEpoch: 4800, loss: 0.00395\nLoss decreased (0.003946 --> 0.003940).  Saving model ...\nLoss decreased (0.003940 --> 0.003940).  Saving model ...\nLoss decreased (0.003940 --> 0.003935).  Saving model ...\nLoss decreased (0.003935 --> 0.003934).  Saving model ...\nLoss decreased (0.003934 --> 0.003933).  Saving model ...\nLoss decreased (0.003933 --> 0.003932).  Saving model ...\nLoss decreased (0.003932 --> 0.003931).  Saving model ...\nLoss decreased (0.003931 --> 0.003930).  Saving model ...\nLoss decreased (0.003930 --> 0.003929).  Saving model ...\nLoss decreased (0.003929 --> 0.003929).  Saving model ...\nLoss decreased (0.003929 --> 0.003928).  Saving model ...\nLoss decreased (0.003928 --> 0.003926).  Saving model ...\nLoss decreased (0.003926 --> 0.003926).  Saving model ...\nLoss decreased (0.003926 --> 0.003924).  Saving model ...\nLoss decreased (0.003924 --> 0.003924).  Saving model ...\nLoss decreased (0.003924 --> 0.003924).  Saving model ...\nEpoch: 4900, loss: 0.00394\nLoss decreased (0.003924 --> 0.003922).  Saving model ...\nLoss decreased (0.003922 --> 0.003920).  Saving model ...\nLoss decreased (0.003920 --> 0.003918).  Saving model ...\nLoss decreased (0.003918 --> 0.003915).  Saving model ...\nLoss decreased (0.003915 --> 0.003914).  Saving model ...\nLoss decreased (0.003914 --> 0.003913).  Saving model ...\nLoss decreased (0.003913 --> 0.003913).  Saving model ...\nLoss decreased (0.003913 --> 0.003907).  Saving model ...\nLoss decreased (0.003907 --> 0.003907).  Saving model ...\nLoss decreased (0.003907 --> 0.003905).  Saving model ...\nLoss decreased (0.003905 --> 0.003905).  Saving model ...\nLoss decreased (0.003905 --> 0.003902).  Saving model ...\nEpoch: 5000, loss: 0.00391\nLoss decreased (0.003902 --> 0.003902).  Saving model ...\nLoss decreased (0.003902 --> 0.003900).  Saving model ...\nLoss decreased (0.003900 --> 0.003899).  Saving model ...\nLoss decreased (0.003899 --> 0.003898).  Saving model ...\nLoss decreased (0.003898 --> 0.003896).  Saving model ...\nLoss decreased (0.003896 --> 0.003894).  Saving model ...\nLoss decreased (0.003894 --> 0.003888).  Saving model ...\nEpoch: 5100, loss: 0.00422\nLoss decreased (0.003888 --> 0.003883).  Saving model ...\nLoss decreased (0.003883 --> 0.003882).  Saving model ...\nLoss decreased (0.003882 --> 0.003881).  Saving model ...\nLoss decreased (0.003881 --> 0.003878).  Saving model ...\nEpoch: 5200, loss: 0.00388\nLoss decreased (0.003878 --> 0.003878).  Saving model ...\nLoss decreased (0.003878 --> 0.003876).  Saving model ...\nLoss decreased (0.003876 --> 0.003875).  Saving model ...\nLoss decreased (0.003875 --> 0.003871).  Saving model ...\nLoss decreased (0.003871 --> 0.003867).  Saving model ...\nLoss decreased (0.003867 --> 0.003867).  Saving model ...\nLoss decreased (0.003867 --> 0.003861).  Saving model ...\nLoss decreased (0.003861 --> 0.003858).  Saving model ...\nEpoch: 5300, loss: 0.00387\nLoss decreased (0.003858 --> 0.003856).  Saving model ...\nLoss decreased (0.003856 --> 0.003854).  Saving model ...\nLoss decreased (0.003854 --> 0.003854).  Saving model ...\nLoss decreased (0.003854 --> 0.003853).  Saving model ...\nLoss decreased (0.003853 --> 0.003851).  Saving model ...\nLoss decreased (0.003851 --> 0.003851).  Saving model ...\nLoss decreased (0.003851 --> 0.003851).  Saving model ...\nLoss decreased (0.003851 --> 0.003849).  Saving model ...\nLoss decreased (0.003849 --> 0.003849).  Saving model ...\nLoss decreased (0.003849 --> 0.003847).  Saving model ...\nLoss decreased (0.003847 --> 0.003847).  Saving model ...\nLoss decreased (0.003847 --> 0.003845).  Saving model ...\nLoss decreased (0.003845 --> 0.003842).  Saving model ...\nLoss decreased (0.003842 --> 0.003840).  Saving model ...\nLoss decreased (0.003840 --> 0.003840).  Saving model ...\nLoss decreased (0.003840 --> 0.003840).  Saving model ...\nLoss decreased (0.003840 --> 0.003839).  Saving model ...\nLoss decreased (0.003839 --> 0.003838).  Saving model ...\nLoss decreased (0.003838 --> 0.003837).  Saving model ...\nLoss decreased (0.003837 --> 0.003836).  Saving model ...\nLoss decreased (0.003836 --> 0.003834).  Saving model ...\nLoss decreased (0.003834 --> 0.003834).  Saving model ...\nEpoch: 5400, loss: 0.00383\nLoss decreased (0.003834 --> 0.003833).  Saving model ...\nLoss decreased (0.003833 --> 0.003827).  Saving model ...\nLoss decreased (0.003827 --> 0.003826).  Saving model ...\nLoss decreased (0.003826 --> 0.003826).  Saving model ...\nLoss decreased (0.003826 --> 0.003824).  Saving model ...\nLoss decreased (0.003824 --> 0.003823).  Saving model ...\nEpoch: 5500, loss: 0.00391\nLoss decreased (0.003823 --> 0.003818).  Saving model ...\nLoss decreased (0.003818 --> 0.003817).  Saving model ...\nLoss decreased (0.003817 --> 0.003816).  Saving model ...\nLoss decreased (0.003816 --> 0.003816).  Saving model ...\nLoss decreased (0.003816 --> 0.003814).  Saving model ...\nLoss decreased (0.003814 --> 0.003814).  Saving model ...\nLoss decreased (0.003814 --> 0.003814).  Saving model ...\nLoss decreased (0.003814 --> 0.003810).  Saving model ...\nEpoch: 5600, loss: 0.00383\nLoss decreased (0.003810 --> 0.003808).  Saving model ...\nEpoch: 5700, loss: 0.00420\nLoss decreased (0.003808 --> 0.003803).  Saving model ...\nLoss decreased (0.003803 --> 0.003802).  Saving model ...\nLoss decreased (0.003802 --> 0.003798).  Saving model ...\nLoss decreased (0.003798 --> 0.003790).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.003790 --> 0.003790).  Saving model ...\nLoss decreased (0.003790 --> 0.003788).  Saving model ...\nLoss decreased (0.003788 --> 0.003788).  Saving model ...\nLoss decreased (0.003788 --> 0.003783).  Saving model ...\nLoss decreased (0.003783 --> 0.003783).  Saving model ...\nLoss decreased (0.003783 --> 0.003781).  Saving model ...\nLoss decreased (0.003781 --> 0.003779).  Saving model ...\nLoss decreased (0.003779 --> 0.003779).  Saving model ...\nEpoch: 5800, loss: 0.00383\nLoss decreased (0.003779 --> 0.003778).  Saving model ...\nLoss decreased (0.003778 --> 0.003777).  Saving model ...\nLoss decreased (0.003777 --> 0.003777).  Saving model ...\nLoss decreased (0.003777 --> 0.003775).  Saving model ...\nLoss decreased (0.003775 --> 0.003773).  Saving model ...\nLoss decreased (0.003773 --> 0.003772).  Saving model ...\nLoss decreased (0.003772 --> 0.003770).  Saving model ...\nLoss decreased (0.003770 --> 0.003768).  Saving model ...\nLoss decreased (0.003768 --> 0.003767).  Saving model ...\nLoss decreased (0.003767 --> 0.003765).  Saving model ...\nEpoch: 5900, loss: 0.00380\nLoss decreased (0.003765 --> 0.003763).  Saving model ...\nLoss decreased (0.003763 --> 0.003760).  Saving model ...\nLoss decreased (0.003760 --> 0.003758).  Saving model ...\nLoss decreased (0.003758 --> 0.003756).  Saving model ...\nEpoch: 6000, loss: 0.00388\nLoss decreased (0.003756 --> 0.003755).  Saving model ...\nLoss decreased (0.003755 --> 0.003747).  Saving model ...\nLoss decreased (0.003747 --> 0.003746).  Saving model ...\nLoss decreased (0.003746 --> 0.003744).  Saving model ...\nLoss decreased (0.003744 --> 0.003743).  Saving model ...\nLoss decreased (0.003743 --> 0.003742).  Saving model ...\nLoss decreased (0.003742 --> 0.003739).  Saving model ...\nLoss decreased (0.003739 --> 0.003738).  Saving model ...\nLoss decreased (0.003738 --> 0.003738).  Saving model ...\nEpoch: 6100, loss: 0.00377\nLoss decreased (0.003738 --> 0.003735).  Saving model ...\nLoss decreased (0.003735 --> 0.003735).  Saving model ...\nLoss decreased (0.003735 --> 0.003731).  Saving model ...\nLoss decreased (0.003731 --> 0.003731).  Saving model ...\nLoss decreased (0.003731 --> 0.003727).  Saving model ...\nLoss decreased (0.003727 --> 0.003725).  Saving model ...\nLoss decreased (0.003725 --> 0.003724).  Saving model ...\nLoss decreased (0.003724 --> 0.003722).  Saving model ...\nEpoch: 6200, loss: 0.00374\nLoss decreased (0.003722 --> 0.003720).  Saving model ...\nLoss decreased (0.003720 --> 0.003719).  Saving model ...\nLoss decreased (0.003719 --> 0.003716).  Saving model ...\nLoss decreased (0.003716 --> 0.003715).  Saving model ...\nLoss decreased (0.003715 --> 0.003713).  Saving model ...\nEpoch: 6300, loss: 0.00371\nLoss decreased (0.003713 --> 0.003709).  Saving model ...\nLoss decreased (0.003709 --> 0.003708).  Saving model ...\nLoss decreased (0.003708 --> 0.003707).  Saving model ...\nLoss decreased (0.003707 --> 0.003701).  Saving model ...\nLoss decreased (0.003701 --> 0.003700).  Saving model ...\nLoss decreased (0.003700 --> 0.003699).  Saving model ...\nLoss decreased (0.003699 --> 0.003698).  Saving model ...\nLoss decreased (0.003698 --> 0.003698).  Saving model ...\nLoss decreased (0.003698 --> 0.003697).  Saving model ...\nEpoch: 6400, loss: 0.00370\nLoss decreased (0.003697 --> 0.003694).  Saving model ...\nLoss decreased (0.003694 --> 0.003693).  Saving model ...\nEpoch: 6500, loss: 0.00371\nLoss decreased (0.003693 --> 0.003689).  Saving model ...\nLoss decreased (0.003689 --> 0.003688).  Saving model ...\nLoss decreased (0.003688 --> 0.003686).  Saving model ...\nLoss decreased (0.003686 --> 0.003680).  Saving model ...\nEpoch: 6600, loss: 0.00374\nLoss decreased (0.003680 --> 0.003675).  Saving model ...\nEpoch: 6700, loss: 0.00368\nLoss decreased (0.003675 --> 0.003674).  Saving model ...\nLoss decreased (0.003674 --> 0.003667).  Saving model ...\nLoss decreased (0.003667 --> 0.003664).  Saving model ...\nEpoch: 6800, loss: 0.00368\nLoss decreased (0.003664 --> 0.003661).  Saving model ...\nLoss decreased (0.003661 --> 0.003654).  Saving model ...\nLoss decreased (0.003654 --> 0.003651).  Saving model ...\nEpoch: 6900, loss: 0.00367\nLoss decreased (0.003651 --> 0.003649).  Saving model ...\nEpoch: 7000, loss: 0.00374\nLoss decreased (0.003649 --> 0.003641).  Saving model ...\nLoss decreased (0.003641 --> 0.003638).  Saving model ...\nLoss decreased (0.003638 --> 0.003637).  Saving model ...\nLoss decreased (0.003637 --> 0.003636).  Saving model ...\nLoss decreased (0.003636 --> 0.003635).  Saving model ...\nLoss decreased (0.003635 --> 0.003635).  Saving model ...\nLoss decreased (0.003635 --> 0.003634).  Saving model ...\nEpoch: 7100, loss: 0.00371\nLoss decreased (0.003634 --> 0.003631).  Saving model ...\nLoss decreased (0.003631 --> 0.003630).  Saving model ...\nLoss decreased (0.003630 --> 0.003628).  Saving model ...\nLoss decreased (0.003628 --> 0.003624).  Saving model ...\nLoss decreased (0.003624 --> 0.003622).  Saving model ...\nEpoch: 7200, loss: 0.00364\nLoss decreased (0.003622 --> 0.003619).  Saving model ...\nLoss decreased (0.003619 --> 0.003617).  Saving model ...\nEpoch: 7300, loss: 0.00362\nLoss decreased (0.003617 --> 0.003617).  Saving model ...\nLoss decreased (0.003617 --> 0.003615).  Saving model ...\nLoss decreased (0.003615 --> 0.003614).  Saving model ...\nLoss decreased (0.003614 --> 0.003611).  Saving model ...\nLoss decreased (0.003611 --> 0.003608).  Saving model ...\nLoss decreased (0.003608 --> 0.003606).  Saving model ...\nLoss decreased (0.003606 --> 0.003605).  Saving model ...\nEpoch: 7400, loss: 0.00363\nLoss decreased (0.003605 --> 0.003604).  Saving model ...\nEpoch: 7500, loss: 0.00365\nLoss decreased (0.003604 --> 0.003595).  Saving model ...\nLoss decreased (0.003595 --> 0.003593).  Saving model ...\nLoss decreased (0.003593 --> 0.003591).  Saving model ...\nLoss decreased (0.003591 --> 0.003589).  Saving model ...\nEpoch: 7600, loss: 0.00366\nLoss decreased (0.003589 --> 0.003583).  Saving model ...\nEpoch: 7700, loss: 0.00361\nLoss decreased (0.003583 --> 0.003582).  Saving model ...\nLoss decreased (0.003582 --> 0.003582).  Saving model ...\nLoss decreased (0.003582 --> 0.003580).  Saving model ...\nLoss decreased (0.003580 --> 0.003578).  Saving model ...\nLoss decreased (0.003578 --> 0.003578).  Saving model ...\nEpoch: 7800, loss: 0.00373\nLoss decreased (0.003578 --> 0.003576).  Saving model ...\nLoss decreased (0.003576 --> 0.003572).  Saving model ...\nLoss decreased (0.003572 --> 0.003572).  Saving model ...\nLoss decreased (0.003572 --> 0.003569).  Saving model ...\nLoss decreased (0.003569 --> 0.003568).  Saving model ...\nEpoch: 7900, loss: 0.00364\nLoss decreased (0.003568 --> 0.003565).  Saving model ...\nLoss decreased (0.003565 --> 0.003564).  Saving model ...\nLoss decreased (0.003564 --> 0.003562).  Saving model ...\nEpoch: 8000, loss: 0.00360\nLoss decreased (0.003562 --> 0.003559).  Saving model ...\nLoss decreased (0.003559 --> 0.003558).  Saving model ...\nLoss decreased (0.003558 --> 0.003555).  Saving model ...\nEpoch: 8100, loss: 0.00373\nLoss decreased (0.003555 --> 0.003554).  Saving model ...\nLoss decreased (0.003554 --> 0.003550).  Saving model ...\nEpoch: 8200, loss: 0.00362\nLoss decreased (0.003550 --> 0.003549).  Saving model ...\nLoss decreased (0.003549 --> 0.003545).  Saving model ...\nLoss decreased (0.003545 --> 0.003541).  Saving model ...\nEpoch: 8300, loss: 0.00355\nLoss decreased (0.003541 --> 0.003539).  Saving model ...\nEpoch: 8400, loss: 0.00357\nLoss decreased (0.003539 --> 0.003539).  Saving model ...\nLoss decreased (0.003539 --> 0.003536).  Saving model ...\nLoss decreased (0.003536 --> 0.003534).  Saving model ...\nLoss decreased (0.003534 --> 0.003533).  Saving model ...\nLoss decreased (0.003533 --> 0.003532).  Saving model ...\nLoss decreased (0.003532 --> 0.003529).  Saving model ...\nEpoch: 8500, loss: 0.00357\nLoss decreased (0.003529 --> 0.003528).  Saving model ...\nLoss decreased (0.003528 --> 0.003526).  Saving model ...\nLoss decreased (0.003526 --> 0.003522).  Saving model ...\nLoss decreased (0.003522 --> 0.003521).  Saving model ...\nLoss decreased (0.003521 --> 0.003520).  Saving model ...\nLoss decreased (0.003520 --> 0.003518).  Saving model ...\nEpoch: 8600, loss: 0.00354\nLoss decreased (0.003518 --> 0.003517).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.003517 --> 0.003516).  Saving model ...\nLoss decreased (0.003516 --> 0.003513).  Saving model ...\nLoss decreased (0.003513 --> 0.003512).  Saving model ...\nLoss decreased (0.003512 --> 0.003510).  Saving model ...\nLoss decreased (0.003510 --> 0.003508).  Saving model ...\nEpoch: 8700, loss: 0.00360\nEpoch: 8800, loss: 0.00354\nLoss decreased (0.003508 --> 0.003503).  Saving model ...\nLoss decreased (0.003503 --> 0.003501).  Saving model ...\nEpoch: 8900, loss: 0.00352\nLoss decreased (0.003501 --> 0.003501).  Saving model ...\nLoss decreased (0.003501 --> 0.003498).  Saving model ...\nLoss decreased (0.003498 --> 0.003496).  Saving model ...\nEpoch: 9000, loss: 0.00355\nLoss decreased (0.003496 --> 0.003494).  Saving model ...\nLoss decreased (0.003494 --> 0.003490).  Saving model ...\nLoss decreased (0.003490 --> 0.003488).  Saving model ...\nLoss decreased (0.003488 --> 0.003487).  Saving model ...\nLoss decreased (0.003487 --> 0.003487).  Saving model ...\nLoss decreased (0.003487 --> 0.003485).  Saving model ...\nLoss decreased (0.003485 --> 0.003485).  Saving model ...\nLoss decreased (0.003485 --> 0.003484).  Saving model ...\nLoss decreased (0.003484 --> 0.003484).  Saving model ...\nLoss decreased (0.003484 --> 0.003484).  Saving model ...\nLoss decreased (0.003484 --> 0.003483).  Saving model ...\nEpoch: 9100, loss: 0.00362\nLoss decreased (0.003483 --> 0.003483).  Saving model ...\nLoss decreased (0.003483 --> 0.003481).  Saving model ...\nLoss decreased (0.003481 --> 0.003480).  Saving model ...\nLoss decreased (0.003480 --> 0.003479).  Saving model ...\nLoss decreased (0.003479 --> 0.003479).  Saving model ...\nEpoch: 9200, loss: 0.00349\nLoss decreased (0.003479 --> 0.003477).  Saving model ...\nLoss decreased (0.003477 --> 0.003476).  Saving model ...\nEpoch: 9300, loss: 0.00350\nLoss decreased (0.003476 --> 0.003474).  Saving model ...\nLoss decreased (0.003474 --> 0.003474).  Saving model ...\nLoss decreased (0.003474 --> 0.003473).  Saving model ...\nLoss decreased (0.003473 --> 0.003469).  Saving model ...\nEpoch: 9400, loss: 0.00350\nLoss decreased (0.003469 --> 0.003467).  Saving model ...\nLoss decreased (0.003467 --> 0.003463).  Saving model ...\nEpoch: 9500, loss: 0.00348\nLoss decreased (0.003463 --> 0.003462).  Saving model ...\nLoss decreased (0.003462 --> 0.003460).  Saving model ...\nEpoch: 9600, loss: 0.00349\nLoss decreased (0.003460 --> 0.003459).  Saving model ...\nLoss decreased (0.003459 --> 0.003454).  Saving model ...\nLoss decreased (0.003454 --> 0.003452).  Saving model ...\nEpoch: 9700, loss: 0.00357\nLoss decreased (0.003452 --> 0.003451).  Saving model ...\nLoss decreased (0.003451 --> 0.003442).  Saving model ...\nEpoch: 9800, loss: 0.00346\nLoss decreased (0.003442 --> 0.003441).  Saving model ...\nLoss decreased (0.003441 --> 0.003437).  Saving model ...\nLoss decreased (0.003437 --> 0.003435).  Saving model ...\nEpoch: 9900, loss: 0.00345\nLoss decreased (0.003435 --> 0.003433).  Saving model ...\nLoss decreased (0.003433 --> 0.003432).  Saving model ...\nLoss decreased (0.003432 --> 0.003431).  Saving model ...\nLoss decreased (0.003431 --> 0.003429).  Saving model ...\nEpoch: 10000, loss: 0.00348\nLoss decreased (0.003429 --> 0.003427).  Saving model ...\nLoss decreased (0.003427 --> 0.003424).  Saving model ...\nLoss decreased (0.003424 --> 0.003424).  Saving model ...\nLoss decreased (0.003424 --> 0.003422).  Saving model ...\nLoss decreased (0.003422 --> 0.003420).  Saving model ...\nEpoch: 10100, loss: 0.00351\nEpoch: 10200, loss: 0.00348\nLoss decreased (0.003420 --> 0.003418).  Saving model ...\nLoss decreased (0.003418 --> 0.003415).  Saving model ...\nLoss decreased (0.003415 --> 0.003413).  Saving model ...\nEpoch: 10300, loss: 0.00372\nLoss decreased (0.003413 --> 0.003410).  Saving model ...\nLoss decreased (0.003410 --> 0.003407).  Saving model ...\nLoss decreased (0.003407 --> 0.003406).  Saving model ...\nEpoch: 10400, loss: 0.00347\nLoss decreased (0.003406 --> 0.003404).  Saving model ...\nLoss decreased (0.003404 --> 0.003403).  Saving model ...\nEpoch: 10500, loss: 0.00360\nLoss decreased (0.003403 --> 0.003400).  Saving model ...\nLoss decreased (0.003400 --> 0.003398).  Saving model ...\nLoss decreased (0.003398 --> 0.003398).  Saving model ...\nEpoch: 10600, loss: 0.00344\nLoss decreased (0.003398 --> 0.003396).  Saving model ...\nLoss decreased (0.003396 --> 0.003392).  Saving model ...\nLoss decreased (0.003392 --> 0.003390).  Saving model ...\nEpoch: 10700, loss: 0.00411\nEpoch: 10800, loss: 0.00350\nLoss decreased (0.003390 --> 0.003389).  Saving model ...\nLoss decreased (0.003389 --> 0.003386).  Saving model ...\nLoss decreased (0.003386 --> 0.003385).  Saving model ...\nLoss decreased (0.003385 --> 0.003385).  Saving model ...\nLoss decreased (0.003385 --> 0.003384).  Saving model ...\nLoss decreased (0.003384 --> 0.003384).  Saving model ...\nLoss decreased (0.003384 --> 0.003383).  Saving model ...\nLoss decreased (0.003383 --> 0.003382).  Saving model ...\nLoss decreased (0.003382 --> 0.003382).  Saving model ...\nLoss decreased (0.003382 --> 0.003380).  Saving model ...\nEpoch: 10900, loss: 0.00341\nLoss decreased (0.003380 --> 0.003378).  Saving model ...\nLoss decreased (0.003378 --> 0.003377).  Saving model ...\nLoss decreased (0.003377 --> 0.003377).  Saving model ...\nEpoch: 11000, loss: 0.00340\nLoss decreased (0.003377 --> 0.003374).  Saving model ...\nEpoch: 11100, loss: 0.00347\nEpoch: 11200, loss: 0.00363\nLoss decreased (0.003374 --> 0.003373).  Saving model ...\nLoss decreased (0.003373 --> 0.003373).  Saving model ...\nLoss decreased (0.003373 --> 0.003372).  Saving model ...\nLoss decreased (0.003372 --> 0.003371).  Saving model ...\nLoss decreased (0.003371 --> 0.003369).  Saving model ...\nLoss decreased (0.003369 --> 0.003368).  Saving model ...\nLoss decreased (0.003368 --> 0.003368).  Saving model ...\nLoss decreased (0.003368 --> 0.003365).  Saving model ...\nLoss decreased (0.003365 --> 0.003365).  Saving model ...\nLoss decreased (0.003365 --> 0.003365).  Saving model ...\nLoss decreased (0.003365 --> 0.003363).  Saving model ...\nLoss decreased (0.003363 --> 0.003362).  Saving model ...\nLoss decreased (0.003362 --> 0.003361).  Saving model ...\nLoss decreased (0.003361 --> 0.003361).  Saving model ...\nLoss decreased (0.003361 --> 0.003360).  Saving model ...\nEpoch: 11300, loss: 0.00337\nLoss decreased (0.003360 --> 0.003359).  Saving model ...\nLoss decreased (0.003359 --> 0.003359).  Saving model ...\nEpoch: 11400, loss: 0.00345\nLoss decreased (0.003359 --> 0.003358).  Saving model ...\nLoss decreased (0.003358 --> 0.003356).  Saving model ...\nLoss decreased (0.003356 --> 0.003354).  Saving model ...\nLoss decreased (0.003354 --> 0.003353).  Saving model ...\nLoss decreased (0.003353 --> 0.003353).  Saving model ...\nLoss decreased (0.003353 --> 0.003353).  Saving model ...\nLoss decreased (0.003353 --> 0.003351).  Saving model ...\nEpoch: 11500, loss: 0.00342\nEpoch: 11600, loss: 0.00337\nLoss decreased (0.003351 --> 0.003349).  Saving model ...\nLoss decreased (0.003349 --> 0.003349).  Saving model ...\nLoss decreased (0.003349 --> 0.003348).  Saving model ...\nLoss decreased (0.003348 --> 0.003344).  Saving model ...\nLoss decreased (0.003344 --> 0.003341).  Saving model ...\nEpoch: 11700, loss: 0.00334\nLoss decreased (0.003341 --> 0.003340).  Saving model ...\nLoss decreased (0.003340 --> 0.003339).  Saving model ...\nLoss decreased (0.003339 --> 0.003338).  Saving model ...\nLoss decreased (0.003338 --> 0.003338).  Saving model ...\nLoss decreased (0.003338 --> 0.003337).  Saving model ...\nLoss decreased (0.003337 --> 0.003337).  Saving model ...\nLoss decreased (0.003337 --> 0.003337).  Saving model ...\nLoss decreased (0.003337 --> 0.003336).  Saving model ...\nLoss decreased (0.003336 --> 0.003336).  Saving model ...\nLoss decreased (0.003336 --> 0.003335).  Saving model ...\nEpoch: 11800, loss: 0.00336\nLoss decreased (0.003335 --> 0.003333).  Saving model ...\nEpoch: 11900, loss: 0.00337\nLoss decreased (0.003333 --> 0.003333).  Saving model ...\nLoss decreased (0.003333 --> 0.003330).  Saving model ...\nLoss decreased (0.003330 --> 0.003329).  Saving model ...\nLoss decreased (0.003329 --> 0.003328).  Saving model ...\nLoss decreased (0.003328 --> 0.003328).  Saving model ...\nLoss decreased (0.003328 --> 0.003327).  Saving model ...\nLoss decreased (0.003327 --> 0.003325).  Saving model ...\nLoss decreased (0.003325 --> 0.003325).  Saving model ...\nLoss decreased (0.003325 --> 0.003325).  Saving model ...\nLoss decreased (0.003325 --> 0.003325).  Saving model ...\nLoss decreased (0.003325 --> 0.003324).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.003324 --> 0.003323).  Saving model ...\nLoss decreased (0.003323 --> 0.003323).  Saving model ...\nLoss decreased (0.003323 --> 0.003322).  Saving model ...\nLoss decreased (0.003322 --> 0.003322).  Saving model ...\nLoss decreased (0.003322 --> 0.003321).  Saving model ...\nEpoch: 12000, loss: 0.00342\nLoss decreased (0.003321 --> 0.003320).  Saving model ...\nEpoch: 12100, loss: 0.00333\nLoss decreased (0.003320 --> 0.003316).  Saving model ...\nLoss decreased (0.003316 --> 0.003311).  Saving model ...\nLoss decreased (0.003311 --> 0.003308).  Saving model ...\nLoss decreased (0.003308 --> 0.003308).  Saving model ...\nLoss decreased (0.003308 --> 0.003306).  Saving model ...\nEpoch: 12200, loss: 0.00343\nLoss decreased (0.003306 --> 0.003304).  Saving model ...\nLoss decreased (0.003304 --> 0.003304).  Saving model ...\nLoss decreased (0.003304 --> 0.003303).  Saving model ...\nLoss decreased (0.003303 --> 0.003302).  Saving model ...\nEpoch: 12300, loss: 0.00330\nLoss decreased (0.003302 --> 0.003301).  Saving model ...\nLoss decreased (0.003301 --> 0.003301).  Saving model ...\nLoss decreased (0.003301 --> 0.003299).  Saving model ...\nLoss decreased (0.003299 --> 0.003298).  Saving model ...\nLoss decreased (0.003298 --> 0.003297).  Saving model ...\nLoss decreased (0.003297 --> 0.003296).  Saving model ...\nLoss decreased (0.003296 --> 0.003295).  Saving model ...\nLoss decreased (0.003295 --> 0.003294).  Saving model ...\nLoss decreased (0.003294 --> 0.003293).  Saving model ...\nLoss decreased (0.003293 --> 0.003293).  Saving model ...\nLoss decreased (0.003293 --> 0.003292).  Saving model ...\nEpoch: 12400, loss: 0.00330\nLoss decreased (0.003292 --> 0.003288).  Saving model ...\nLoss decreased (0.003288 --> 0.003288).  Saving model ...\nLoss decreased (0.003288 --> 0.003287).  Saving model ...\nLoss decreased (0.003287 --> 0.003286).  Saving model ...\nLoss decreased (0.003286 --> 0.003285).  Saving model ...\nEpoch: 12500, loss: 0.00329\nLoss decreased (0.003285 --> 0.003284).  Saving model ...\nLoss decreased (0.003284 --> 0.003283).  Saving model ...\nLoss decreased (0.003283 --> 0.003282).  Saving model ...\nLoss decreased (0.003282 --> 0.003281).  Saving model ...\nLoss decreased (0.003281 --> 0.003281).  Saving model ...\nLoss decreased (0.003281 --> 0.003280).  Saving model ...\nEpoch: 12600, loss: 0.00347\nLoss decreased (0.003280 --> 0.003276).  Saving model ...\nLoss decreased (0.003276 --> 0.003275).  Saving model ...\nLoss decreased (0.003275 --> 0.003273).  Saving model ...\nLoss decreased (0.003273 --> 0.003271).  Saving model ...\nEpoch: 12700, loss: 0.00327\nLoss decreased (0.003271 --> 0.003271).  Saving model ...\nLoss decreased (0.003271 --> 0.003271).  Saving model ...\nLoss decreased (0.003271 --> 0.003270).  Saving model ...\nEpoch: 12800, loss: 0.00328\nLoss decreased (0.003270 --> 0.003270).  Saving model ...\nLoss decreased (0.003270 --> 0.003265).  Saving model ...\nLoss decreased (0.003265 --> 0.003264).  Saving model ...\nEpoch: 12900, loss: 0.00376\nEpoch: 13000, loss: 0.00328\nLoss decreased (0.003264 --> 0.003261).  Saving model ...\nLoss decreased (0.003261 --> 0.003260).  Saving model ...\nLoss decreased (0.003260 --> 0.003258).  Saving model ...\nLoss decreased (0.003258 --> 0.003257).  Saving model ...\nLoss decreased (0.003257 --> 0.003254).  Saving model ...\nEpoch: 13100, loss: 0.00332\nLoss decreased (0.003254 --> 0.003251).  Saving model ...\nLoss decreased (0.003251 --> 0.003248).  Saving model ...\nLoss decreased (0.003248 --> 0.003247).  Saving model ...\nLoss decreased (0.003247 --> 0.003244).  Saving model ...\nLoss decreased (0.003244 --> 0.003244).  Saving model ...\nLoss decreased (0.003244 --> 0.003242).  Saving model ...\nLoss decreased (0.003242 --> 0.003241).  Saving model ...\nLoss decreased (0.003241 --> 0.003241).  Saving model ...\nLoss decreased (0.003241 --> 0.003238).  Saving model ...\nEpoch: 13200, loss: 0.00328\nEpoch: 13300, loss: 0.00345\nLoss decreased (0.003238 --> 0.003234).  Saving model ...\nLoss decreased (0.003234 --> 0.003233).  Saving model ...\nLoss decreased (0.003233 --> 0.003231).  Saving model ...\nLoss decreased (0.003231 --> 0.003231).  Saving model ...\nLoss decreased (0.003231 --> 0.003229).  Saving model ...\nLoss decreased (0.003229 --> 0.003228).  Saving model ...\nEpoch: 13400, loss: 0.00323\nLoss decreased (0.003228 --> 0.003227).  Saving model ...\nLoss decreased (0.003227 --> 0.003226).  Saving model ...\nLoss decreased (0.003226 --> 0.003224).  Saving model ...\nEpoch: 13500, loss: 0.00326\nLoss decreased (0.003224 --> 0.003220).  Saving model ...\nLoss decreased (0.003220 --> 0.003220).  Saving model ...\nEpoch: 13600, loss: 0.00330\nLoss decreased (0.003220 --> 0.003215).  Saving model ...\nLoss decreased (0.003215 --> 0.003214).  Saving model ...\nLoss decreased (0.003214 --> 0.003211).  Saving model ...\nEpoch: 13700, loss: 0.00330\nLoss decreased (0.003211 --> 0.003208).  Saving model ...\nEpoch: 13800, loss: 0.00364\nLoss decreased (0.003208 --> 0.003206).  Saving model ...\nLoss decreased (0.003206 --> 0.003204).  Saving model ...\nLoss decreased (0.003204 --> 0.003203).  Saving model ...\nLoss decreased (0.003203 --> 0.003201).  Saving model ...\nLoss decreased (0.003201 --> 0.003198).  Saving model ...\nLoss decreased (0.003198 --> 0.003196).  Saving model ...\nEpoch: 13900, loss: 0.00324\nEpoch: 14000, loss: 0.00320\nLoss decreased (0.003196 --> 0.003192).  Saving model ...\nLoss decreased (0.003192 --> 0.003191).  Saving model ...\nLoss decreased (0.003191 --> 0.003190).  Saving model ...\nLoss decreased (0.003190 --> 0.003189).  Saving model ...\nEpoch: 14100, loss: 0.00320\nLoss decreased (0.003189 --> 0.003187).  Saving model ...\nLoss decreased (0.003187 --> 0.003186).  Saving model ...\nEpoch: 14200, loss: 0.00321\nLoss decreased (0.003186 --> 0.003179).  Saving model ...\nLoss decreased (0.003179 --> 0.003178).  Saving model ...\nEpoch: 14300, loss: 0.00320\nLoss decreased (0.003178 --> 0.003173).  Saving model ...\nLoss decreased (0.003173 --> 0.003170).  Saving model ...\nLoss decreased (0.003170 --> 0.003170).  Saving model ...\nLoss decreased (0.003170 --> 0.003167).  Saving model ...\nLoss decreased (0.003167 --> 0.003165).  Saving model ...\nEpoch: 14400, loss: 0.00320\nLoss decreased (0.003165 --> 0.003161).  Saving model ...\nLoss decreased (0.003161 --> 0.003160).  Saving model ...\nLoss decreased (0.003160 --> 0.003159).  Saving model ...\nLoss decreased (0.003159 --> 0.003159).  Saving model ...\nLoss decreased (0.003159 --> 0.003153).  Saving model ...\nEpoch: 14500, loss: 0.00320\nLoss decreased (0.003153 --> 0.003151).  Saving model ...\nLoss decreased (0.003151 --> 0.003150).  Saving model ...\nLoss decreased (0.003150 --> 0.003149).  Saving model ...\nEpoch: 14600, loss: 0.00319\nLoss decreased (0.003149 --> 0.003149).  Saving model ...\nLoss decreased (0.003149 --> 0.003146).  Saving model ...\nEpoch: 14700, loss: 0.00321\nLoss decreased (0.003146 --> 0.003143).  Saving model ...\nLoss decreased (0.003143 --> 0.003140).  Saving model ...\nLoss decreased (0.003140 --> 0.003139).  Saving model ...\nLoss decreased (0.003139 --> 0.003135).  Saving model ...\nEpoch: 14800, loss: 0.00316\nLoss decreased (0.003135 --> 0.003131).  Saving model ...\nLoss decreased (0.003131 --> 0.003131).  Saving model ...\nLoss decreased (0.003131 --> 0.003130).  Saving model ...\nLoss decreased (0.003130 --> 0.003129).  Saving model ...\nEpoch: 14900, loss: 0.00322\nLoss decreased (0.003129 --> 0.003125).  Saving model ...\nLoss decreased (0.003125 --> 0.003123).  Saving model ...\nLoss decreased (0.003123 --> 0.003121).  Saving model ...\nLoss decreased (0.003121 --> 0.003117).  Saving model ...\nEpoch: 15000, loss: 0.00315\nLoss decreased (0.003117 --> 0.003117).  Saving model ...\nLoss decreased (0.003117 --> 0.003115).  Saving model ...\nEpoch: 15100, loss: 0.00345\nLoss decreased (0.003115 --> 0.003111).  Saving model ...\nLoss decreased (0.003111 --> 0.003110).  Saving model ...\nEpoch: 15200, loss: 0.00311\nLoss decreased (0.003110 --> 0.003108).  Saving model ...\nLoss decreased (0.003108 --> 0.003104).  Saving model ...\nLoss decreased (0.003104 --> 0.003102).  Saving model ...\nLoss decreased (0.003102 --> 0.003098).  Saving model ...\nLoss decreased (0.003098 --> 0.003096).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 15300, loss: 0.00318\nLoss decreased (0.003096 --> 0.003093).  Saving model ...\nEpoch: 15400, loss: 0.00311\nLoss decreased (0.003093 --> 0.003092).  Saving model ...\nLoss decreased (0.003092 --> 0.003091).  Saving model ...\nLoss decreased (0.003091 --> 0.003087).  Saving model ...\nLoss decreased (0.003087 --> 0.003086).  Saving model ...\nLoss decreased (0.003086 --> 0.003083).  Saving model ...\nLoss decreased (0.003083 --> 0.003079).  Saving model ...\nEpoch: 15500, loss: 0.00309\nLoss decreased (0.003079 --> 0.003077).  Saving model ...\nEpoch: 15600, loss: 0.00314\nLoss decreased (0.003077 --> 0.003076).  Saving model ...\nLoss decreased (0.003076 --> 0.003070).  Saving model ...\nLoss decreased (0.003070 --> 0.003069).  Saving model ...\nLoss decreased (0.003069 --> 0.003066).  Saving model ...\nEpoch: 15700, loss: 0.00308\nLoss decreased (0.003066 --> 0.003062).  Saving model ...\nLoss decreased (0.003062 --> 0.003059).  Saving model ...\nEpoch: 15800, loss: 0.00311\nLoss decreased (0.003059 --> 0.003059).  Saving model ...\nLoss decreased (0.003059 --> 0.003055).  Saving model ...\nEpoch: 15900, loss: 0.00313\nLoss decreased (0.003055 --> 0.003049).  Saving model ...\nLoss decreased (0.003049 --> 0.003049).  Saving model ...\nLoss decreased (0.003049 --> 0.003043).  Saving model ...\nLoss decreased (0.003043 --> 0.003041).  Saving model ...\nEpoch: 16000, loss: 0.00306\nLoss decreased (0.003041 --> 0.003040).  Saving model ...\nLoss decreased (0.003040 --> 0.003040).  Saving model ...\nLoss decreased (0.003040 --> 0.003037).  Saving model ...\nEpoch: 16100, loss: 0.00304\nLoss decreased (0.003037 --> 0.003032).  Saving model ...\nEpoch: 16200, loss: 0.00308\nLoss decreased (0.003032 --> 0.003032).  Saving model ...\nLoss decreased (0.003032 --> 0.003032).  Saving model ...\nLoss decreased (0.003032 --> 0.003024).  Saving model ...\nEpoch: 16300, loss: 0.00303\nLoss decreased (0.003024 --> 0.003023).  Saving model ...\nLoss decreased (0.003023 --> 0.003018).  Saving model ...\nEpoch: 16400, loss: 0.00307\nLoss decreased (0.003018 --> 0.003016).  Saving model ...\nLoss decreased (0.003016 --> 0.003011).  Saving model ...\nLoss decreased (0.003011 --> 0.003010).  Saving model ...\nEpoch: 16500, loss: 0.00318\nLoss decreased (0.003010 --> 0.003009).  Saving model ...\nLoss decreased (0.003009 --> 0.003007).  Saving model ...\nLoss decreased (0.003007 --> 0.003006).  Saving model ...\nLoss decreased (0.003006 --> 0.003005).  Saving model ...\nLoss decreased (0.003005 --> 0.003000).  Saving model ...\nLoss decreased (0.003000 --> 0.002999).  Saving model ...\nLoss decreased (0.002999 --> 0.002998).  Saving model ...\nEpoch: 16600, loss: 0.00311\nLoss decreased (0.002998 --> 0.002996).  Saving model ...\nEpoch: 16700, loss: 0.00304\nLoss decreased (0.002996 --> 0.002993).  Saving model ...\nLoss decreased (0.002993 --> 0.002993).  Saving model ...\nLoss decreased (0.002993 --> 0.002989).  Saving model ...\nEpoch: 16800, loss: 0.00299\nLoss decreased (0.002989 --> 0.002982).  Saving model ...\nLoss decreased (0.002982 --> 0.002981).  Saving model ...\nLoss decreased (0.002981 --> 0.002980).  Saving model ...\nEpoch: 16900, loss: 0.00314\nLoss decreased (0.002980 --> 0.002976).  Saving model ...\nLoss decreased (0.002976 --> 0.002973).  Saving model ...\nEpoch: 17000, loss: 0.00298\nLoss decreased (0.002973 --> 0.002973).  Saving model ...\nLoss decreased (0.002973 --> 0.002972).  Saving model ...\nEpoch: 17100, loss: 0.00304\nLoss decreased (0.002972 --> 0.002971).  Saving model ...\nLoss decreased (0.002971 --> 0.002964).  Saving model ...\nLoss decreased (0.002964 --> 0.002958).  Saving model ...\nEpoch: 17200, loss: 0.00299\nEpoch: 17300, loss: 0.00309\nLoss decreased (0.002958 --> 0.002954).  Saving model ...\nLoss decreased (0.002954 --> 0.002953).  Saving model ...\nLoss decreased (0.002953 --> 0.002953).  Saving model ...\nLoss decreased (0.002953 --> 0.002949).  Saving model ...\nLoss decreased (0.002949 --> 0.002947).  Saving model ...\nLoss decreased (0.002947 --> 0.002945).  Saving model ...\nLoss decreased (0.002945 --> 0.002942).  Saving model ...\nEpoch: 17400, loss: 0.00314\nLoss decreased (0.002942 --> 0.002938).  Saving model ...\nEpoch: 17500, loss: 0.00294\nLoss decreased (0.002938 --> 0.002937).  Saving model ...\nLoss decreased (0.002937 --> 0.002932).  Saving model ...\nLoss decreased (0.002932 --> 0.002929).  Saving model ...\nEpoch: 17600, loss: 0.00296\nLoss decreased (0.002929 --> 0.002925).  Saving model ...\nEpoch: 17700, loss: 0.00298\nLoss decreased (0.002925 --> 0.002925).  Saving model ...\nLoss decreased (0.002925 --> 0.002924).  Saving model ...\nLoss decreased (0.002924 --> 0.002920).  Saving model ...\nLoss decreased (0.002920 --> 0.002920).  Saving model ...\nEpoch: 17800, loss: 0.00294\nLoss decreased (0.002920 --> 0.002917).  Saving model ...\nLoss decreased (0.002917 --> 0.002910).  Saving model ...\nLoss decreased (0.002910 --> 0.002909).  Saving model ...\nEpoch: 17900, loss: 0.00303\nLoss decreased (0.002909 --> 0.002908).  Saving model ...\nLoss decreased (0.002908 --> 0.002906).  Saving model ...\nEpoch: 18000, loss: 0.00299\nLoss decreased (0.002906 --> 0.002905).  Saving model ...\nLoss decreased (0.002905 --> 0.002899).  Saving model ...\nEpoch: 18100, loss: 0.00361\nEpoch: 18200, loss: 0.00295\nLoss decreased (0.002899 --> 0.002898).  Saving model ...\nLoss decreased (0.002898 --> 0.002894).  Saving model ...\nLoss decreased (0.002894 --> 0.002890).  Saving model ...\nLoss decreased (0.002890 --> 0.002889).  Saving model ...\nLoss decreased (0.002889 --> 0.002887).  Saving model ...\nEpoch: 18300, loss: 0.00309\nLoss decreased (0.002887 --> 0.002882).  Saving model ...\nEpoch: 18400, loss: 0.00293\nLoss decreased (0.002882 --> 0.002880).  Saving model ...\nLoss decreased (0.002880 --> 0.002876).  Saving model ...\nEpoch: 18500, loss: 0.00291\nLoss decreased (0.002876 --> 0.002875).  Saving model ...\nLoss decreased (0.002875 --> 0.002875).  Saving model ...\nLoss decreased (0.002875 --> 0.002871).  Saving model ...\nEpoch: 18600, loss: 0.00292\nLoss decreased (0.002871 --> 0.002866).  Saving model ...\nLoss decreased (0.002866 --> 0.002862).  Saving model ...\nEpoch: 18700, loss: 0.00291\nLoss decreased (0.002862 --> 0.002861).  Saving model ...\nLoss decreased (0.002861 --> 0.002860).  Saving model ...\nEpoch: 18800, loss: 0.00305\nLoss decreased (0.002860 --> 0.002859).  Saving model ...\nLoss decreased (0.002859 --> 0.002859).  Saving model ...\nLoss decreased (0.002859 --> 0.002857).  Saving model ...\nLoss decreased (0.002857 --> 0.002856).  Saving model ...\nEpoch: 18900, loss: 0.00298\nLoss decreased (0.002856 --> 0.002854).  Saving model ...\nLoss decreased (0.002854 --> 0.002846).  Saving model ...\nLoss decreased (0.002846 --> 0.002844).  Saving model ...\nEpoch: 19000, loss: 0.00293\nLoss decreased (0.002844 --> 0.002844).  Saving model ...\nLoss decreased (0.002844 --> 0.002839).  Saving model ...\nEpoch: 19100, loss: 0.00287\nLoss decreased (0.002839 --> 0.002839).  Saving model ...\nLoss decreased (0.002839 --> 0.002831).  Saving model ...\nEpoch: 19200, loss: 0.00304\nLoss decreased (0.002831 --> 0.002829).  Saving model ...\nLoss decreased (0.002829 --> 0.002824).  Saving model ...\nLoss decreased (0.002824 --> 0.002820).  Saving model ...\nLoss decreased (0.002820 --> 0.002819).  Saving model ...\nLoss decreased (0.002819 --> 0.002818).  Saving model ...\nEpoch: 19300, loss: 0.00296\nLoss decreased (0.002818 --> 0.002818).  Saving model ...\nEpoch: 19400, loss: 0.00286\nLoss decreased (0.002818 --> 0.002816).  Saving model ...\nLoss decreased (0.002816 --> 0.002811).  Saving model ...\nLoss decreased (0.002811 --> 0.002810).  Saving model ...\nEpoch: 19500, loss: 0.00284\nLoss decreased (0.002810 --> 0.002808).  Saving model ...\nLoss decreased (0.002808 --> 0.002805).  Saving model ...\nLoss decreased (0.002805 --> 0.002802).  Saving model ...\nLoss decreased (0.002802 --> 0.002802).  Saving model ...\nLoss decreased (0.002802 --> 0.002800).  Saving model ...\nLoss decreased (0.002800 --> 0.002800).  Saving model ...\nEpoch: 19600, loss: 0.00287\nEpoch: 19700, loss: 0.00311\nLoss decreased (0.002800 --> 0.002799).  Saving model ...\nEpoch: 19800, loss: 0.00283\nLoss decreased (0.002799 --> 0.002794).  Saving model ...\nLoss decreased (0.002794 --> 0.002789).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.002789 --> 0.002787).  Saving model ...\nEpoch: 19900, loss: 0.00292\nLoss decreased (0.002787 --> 0.002783).  Saving model ...\nLoss decreased (0.002783 --> 0.002776).  Saving model ...\nLoss decreased (0.002776 --> 0.002776).  Saving model ...\nEpoch: 20000, loss: 0.00279\nLoss decreased (0.002776 --> 0.002775).  Saving model ...\nLoss decreased (0.002775 --> 0.002769).  Saving model ...\nLoss decreased (0.002769 --> 0.002766).  Saving model ...\nEpoch: 20100, loss: 0.00287\nLoss decreased (0.002766 --> 0.002761).  Saving model ...\nLoss decreased (0.002761 --> 0.002761).  Saving model ...\nLoss decreased (0.002761 --> 0.002761).  Saving model ...\nEpoch: 20200, loss: 0.00296\nLoss decreased (0.002761 --> 0.002759).  Saving model ...\nLoss decreased (0.002759 --> 0.002754).  Saving model ...\nEpoch: 20300, loss: 0.00289\nLoss decreased (0.002754 --> 0.002750).  Saving model ...\nLoss decreased (0.002750 --> 0.002749).  Saving model ...\nEpoch: 20400, loss: 0.00277\nLoss decreased (0.002749 --> 0.002748).  Saving model ...\nLoss decreased (0.002748 --> 0.002747).  Saving model ...\nLoss decreased (0.002747 --> 0.002745).  Saving model ...\nLoss decreased (0.002745 --> 0.002743).  Saving model ...\nEpoch: 20500, loss: 0.00284\nLoss decreased (0.002743 --> 0.002742).  Saving model ...\nLoss decreased (0.002742 --> 0.002741).  Saving model ...\nLoss decreased (0.002741 --> 0.002739).  Saving model ...\nLoss decreased (0.002739 --> 0.002739).  Saving model ...\nLoss decreased (0.002739 --> 0.002734).  Saving model ...\nLoss decreased (0.002734 --> 0.002731).  Saving model ...\nEpoch: 20600, loss: 0.00279\nLoss decreased (0.002731 --> 0.002730).  Saving model ...\nLoss decreased (0.002730 --> 0.002728).  Saving model ...\nEpoch: 20700, loss: 0.00276\nLoss decreased (0.002728 --> 0.002726).  Saving model ...\nLoss decreased (0.002726 --> 0.002722).  Saving model ...\nEpoch: 20800, loss: 0.00283\nLoss decreased (0.002722 --> 0.002718).  Saving model ...\nEpoch: 20900, loss: 0.00280\nLoss decreased (0.002718 --> 0.002711).  Saving model ...\nLoss decreased (0.002711 --> 0.002709).  Saving model ...\nLoss decreased (0.002709 --> 0.002706).  Saving model ...\nEpoch: 21000, loss: 0.00274\nLoss decreased (0.002706 --> 0.002702).  Saving model ...\nEpoch: 21100, loss: 0.00286\nLoss decreased (0.002702 --> 0.002702).  Saving model ...\nLoss decreased (0.002702 --> 0.002697).  Saving model ...\nLoss decreased (0.002697 --> 0.002696).  Saving model ...\nEpoch: 21200, loss: 0.00272\nLoss decreased (0.002696 --> 0.002695).  Saving model ...\nLoss decreased (0.002695 --> 0.002694).  Saving model ...\nLoss decreased (0.002694 --> 0.002692).  Saving model ...\nEpoch: 21300, loss: 0.00275\nLoss decreased (0.002692 --> 0.002690).  Saving model ...\nLoss decreased (0.002690 --> 0.002686).  Saving model ...\nLoss decreased (0.002686 --> 0.002685).  Saving model ...\nLoss decreased (0.002685 --> 0.002684).  Saving model ...\nEpoch: 21400, loss: 0.00272\nLoss decreased (0.002684 --> 0.002682).  Saving model ...\nLoss decreased (0.002682 --> 0.002681).  Saving model ...\nLoss decreased (0.002681 --> 0.002680).  Saving model ...\nEpoch: 21500, loss: 0.00272\nLoss decreased (0.002680 --> 0.002677).  Saving model ...\nLoss decreased (0.002677 --> 0.002675).  Saving model ...\nEpoch: 21600, loss: 0.00269\nLoss decreased (0.002675 --> 0.002670).  Saving model ...\nLoss decreased (0.002670 --> 0.002669).  Saving model ...\nLoss decreased (0.002669 --> 0.002667).  Saving model ...\nEpoch: 21700, loss: 0.00269\nLoss decreased (0.002667 --> 0.002663).  Saving model ...\nEpoch: 21800, loss: 0.00270\nLoss decreased (0.002663 --> 0.002661).  Saving model ...\nLoss decreased (0.002661 --> 0.002660).  Saving model ...\nLoss decreased (0.002660 --> 0.002658).  Saving model ...\nLoss decreased (0.002658 --> 0.002653).  Saving model ...\nLoss decreased (0.002653 --> 0.002653).  Saving model ...\nEpoch: 21900, loss: 0.00275\nEpoch: 22000, loss: 0.00267\nLoss decreased (0.002653 --> 0.002645).  Saving model ...\nLoss decreased (0.002645 --> 0.002643).  Saving model ...\nLoss decreased (0.002643 --> 0.002642).  Saving model ...\nEpoch: 22100, loss: 0.00268\nLoss decreased (0.002642 --> 0.002638).  Saving model ...\nLoss decreased (0.002638 --> 0.002636).  Saving model ...\nEpoch: 22200, loss: 0.00278\nEpoch: 22300, loss: 0.00265\nLoss decreased (0.002636 --> 0.002635).  Saving model ...\nLoss decreased (0.002635 --> 0.002634).  Saving model ...\nLoss decreased (0.002634 --> 0.002631).  Saving model ...\nLoss decreased (0.002631 --> 0.002628).  Saving model ...\nLoss decreased (0.002628 --> 0.002625).  Saving model ...\nLoss decreased (0.002625 --> 0.002624).  Saving model ...\nEpoch: 22400, loss: 0.00271\nLoss decreased (0.002624 --> 0.002619).  Saving model ...\nEpoch: 22500, loss: 0.00303\nLoss decreased (0.002619 --> 0.002616).  Saving model ...\nEpoch: 22600, loss: 0.00265\nLoss decreased (0.002616 --> 0.002610).  Saving model ...\nEpoch: 22700, loss: 0.00268\nEpoch: 22800, loss: 0.00266\nLoss decreased (0.002610 --> 0.002606).  Saving model ...\nLoss decreased (0.002606 --> 0.002601).  Saving model ...\nLoss decreased (0.002601 --> 0.002599).  Saving model ...\nLoss decreased (0.002599 --> 0.002595).  Saving model ...\nEpoch: 22900, loss: 0.00261\nLoss decreased (0.002595 --> 0.002594).  Saving model ...\nEpoch: 23000, loss: 0.00276\nLoss decreased (0.002594 --> 0.002592).  Saving model ...\nLoss decreased (0.002592 --> 0.002589).  Saving model ...\nLoss decreased (0.002589 --> 0.002588).  Saving model ...\nEpoch: 23100, loss: 0.00267\nLoss decreased (0.002588 --> 0.002588).  Saving model ...\nEpoch: 23200, loss: 0.00269\nLoss decreased (0.002588 --> 0.002586).  Saving model ...\nLoss decreased (0.002586 --> 0.002585).  Saving model ...\nEpoch: 23300, loss: 0.00268\nLoss decreased (0.002585 --> 0.002584).  Saving model ...\nLoss decreased (0.002584 --> 0.002574).  Saving model ...\nEpoch: 23400, loss: 0.00268\nLoss decreased (0.002574 --> 0.002573).  Saving model ...\nEpoch: 23500, loss: 0.00272\nEpoch: 23600, loss: 0.00261\nLoss decreased (0.002573 --> 0.002568).  Saving model ...\nLoss decreased (0.002568 --> 0.002564).  Saving model ...\nEpoch: 23700, loss: 0.00258\nLoss decreased (0.002564 --> 0.002560).  Saving model ...\nLoss decreased (0.002560 --> 0.002556).  Saving model ...\nEpoch: 23800, loss: 0.00262\nEpoch: 23900, loss: 0.00258\nLoss decreased (0.002556 --> 0.002549).  Saving model ...\nLoss decreased (0.002549 --> 0.002548).  Saving model ...\nLoss decreased (0.002548 --> 0.002544).  Saving model ...\nEpoch: 24000, loss: 0.00259\nLoss decreased (0.002544 --> 0.002542).  Saving model ...\nEpoch: 24100, loss: 0.00259\nLoss decreased (0.002542 --> 0.002538).  Saving model ...\nLoss decreased (0.002538 --> 0.002535).  Saving model ...\nEpoch: 24200, loss: 0.00289\nLoss decreased (0.002535 --> 0.002535).  Saving model ...\nLoss decreased (0.002535 --> 0.002534).  Saving model ...\nLoss decreased (0.002534 --> 0.002534).  Saving model ...\nLoss decreased (0.002534 --> 0.002531).  Saving model ...\nLoss decreased (0.002531 --> 0.002530).  Saving model ...\nLoss decreased (0.002530 --> 0.002530).  Saving model ...\nEpoch: 24300, loss: 0.00253\nLoss decreased (0.002530 --> 0.002529).  Saving model ...\nLoss decreased (0.002529 --> 0.002529).  Saving model ...\nLoss decreased (0.002529 --> 0.002525).  Saving model ...\nEpoch: 24400, loss: 0.00255\nLoss decreased (0.002525 --> 0.002522).  Saving model ...\nEpoch: 24500, loss: 0.00268\nLoss decreased (0.002522 --> 0.002522).  Saving model ...\nLoss decreased (0.002522 --> 0.002520).  Saving model ...\nEpoch: 24600, loss: 0.00265\nLoss decreased (0.002520 --> 0.002518).  Saving model ...\nLoss decreased (0.002518 --> 0.002515).  Saving model ...\nEpoch: 24700, loss: 0.00260\nLoss decreased (0.002515 --> 0.002512).  Saving model ...\nLoss decreased (0.002512 --> 0.002506).  Saving model ...\nEpoch: 24800, loss: 0.00268\nEpoch: 24900, loss: 0.00257\nLoss decreased (0.002506 --> 0.002506).  Saving model ...\nEpoch: 25000, loss: 0.00300\nEpoch: 25100, loss: 0.00252\nLoss decreased (0.002506 --> 0.002502).  Saving model ...\nEpoch: 25200, loss: 0.00251\nLoss decreased (0.002502 --> 0.002502).  Saving model ...\nLoss decreased (0.002502 --> 0.002498).  Saving model ...\nLoss decreased (0.002498 --> 0.002496).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.002496 --> 0.002489).  Saving model ...\nEpoch: 25300, loss: 0.00255\nEpoch: 25400, loss: 0.00274\nLoss decreased (0.002489 --> 0.002487).  Saving model ...\nEpoch: 25500, loss: 0.00255\nLoss decreased (0.002487 --> 0.002486).  Saving model ...\nLoss decreased (0.002486 --> 0.002480).  Saving model ...\nLoss decreased (0.002480 --> 0.002475).  Saving model ...\nEpoch: 25600, loss: 0.00251\nLoss decreased (0.002475 --> 0.002474).  Saving model ...\nEpoch: 25700, loss: 0.00260\nLoss decreased (0.002474 --> 0.002474).  Saving model ...\nLoss decreased (0.002474 --> 0.002473).  Saving model ...\nLoss decreased (0.002473 --> 0.002466).  Saving model ...\nLoss decreased (0.002466 --> 0.002464).  Saving model ...\nEpoch: 25800, loss: 0.00249\nEpoch: 25900, loss: 0.00259\nLoss decreased (0.002464 --> 0.002459).  Saving model ...\nEpoch: 26000, loss: 0.00248\nEpoch: 26100, loss: 0.00249\nLoss decreased (0.002459 --> 0.002456).  Saving model ...\nLoss decreased (0.002456 --> 0.002456).  Saving model ...\nEpoch: 26200, loss: 0.00252\nLoss decreased (0.002456 --> 0.002455).  Saving model ...\nEpoch: 26300, loss: 0.00248\nLoss decreased (0.002455 --> 0.002451).  Saving model ...\nLoss decreased (0.002451 --> 0.002446).  Saving model ...\nLoss decreased (0.002446 --> 0.002446).  Saving model ...\nEpoch: 26400, loss: 0.00255\nLoss decreased (0.002446 --> 0.002440).  Saving model ...\nEpoch: 26500, loss: 0.00255\nLoss decreased (0.002440 --> 0.002437).  Saving model ...\nEpoch: 26600, loss: 0.00259\nLoss decreased (0.002437 --> 0.002431).  Saving model ...\nEpoch: 26700, loss: 0.00259\nEpoch: 26800, loss: 0.00244\nLoss decreased (0.002431 --> 0.002431).  Saving model ...\nLoss decreased (0.002431 --> 0.002427).  Saving model ...\nLoss decreased (0.002427 --> 0.002423).  Saving model ...\nLoss decreased (0.002423 --> 0.002422).  Saving model ...\nLoss decreased (0.002422 --> 0.002420).  Saving model ...\nLoss decreased (0.002420 --> 0.002419).  Saving model ...\nEpoch: 26900, loss: 0.00254\nEpoch: 27000, loss: 0.00252\nEpoch: 27100, loss: 0.00245\nLoss decreased (0.002419 --> 0.002418).  Saving model ...\nEpoch: 27200, loss: 0.00249\nLoss decreased (0.002418 --> 0.002413).  Saving model ...\nEpoch: 27300, loss: 0.00257\nLoss decreased (0.002413 --> 0.002411).  Saving model ...\nLoss decreased (0.002411 --> 0.002409).  Saving model ...\nLoss decreased (0.002409 --> 0.002408).  Saving model ...\nEpoch: 27400, loss: 0.00243\nLoss decreased (0.002408 --> 0.002400).  Saving model ...\nEpoch: 27500, loss: 0.00247\nEpoch: 27600, loss: 0.00245\nLoss decreased (0.002400 --> 0.002393).  Saving model ...\nEpoch: 27700, loss: 0.00257\nLoss decreased (0.002393 --> 0.002391).  Saving model ...\nLoss decreased (0.002391 --> 0.002390).  Saving model ...\nEpoch: 27800, loss: 0.00248\nLoss decreased (0.002390 --> 0.002389).  Saving model ...\nEpoch: 27900, loss: 0.00243\nLoss decreased (0.002389 --> 0.002387).  Saving model ...\nLoss decreased (0.002387 --> 0.002387).  Saving model ...\nLoss decreased (0.002387 --> 0.002383).  Saving model ...\nEpoch: 28000, loss: 0.00241\nEpoch: 28100, loss: 0.00243\nLoss decreased (0.002383 --> 0.002378).  Saving model ...\nEpoch: 28200, loss: 0.00243\nLoss decreased (0.002378 --> 0.002375).  Saving model ...\nEpoch: 28300, loss: 0.00241\nLoss decreased (0.002375 --> 0.002375).  Saving model ...\nEpoch: 28400, loss: 0.00240\nLoss decreased (0.002375 --> 0.002371).  Saving model ...\nLoss decreased (0.002371 --> 0.002363).  Saving model ...\nEpoch: 28500, loss: 0.00250\nEpoch: 28600, loss: 0.00258\nEpoch: 28700, loss: 0.00241\nLoss decreased (0.002363 --> 0.002361).  Saving model ...\nEpoch: 28800, loss: 0.00252\nLoss decreased (0.002361 --> 0.002359).  Saving model ...\nEpoch: 28900, loss: 0.00251\nLoss decreased (0.002359 --> 0.002354).  Saving model ...\nEpoch: 29000, loss: 0.00243\nLoss decreased (0.002354 --> 0.002350).  Saving model ...\nLoss decreased (0.002350 --> 0.002346).  Saving model ...\nEpoch: 29100, loss: 0.00236\nEpoch: 29200, loss: 0.00251\nEpoch: 29300, loss: 0.00236\nEpoch: 29400, loss: 0.00247\nLoss decreased (0.002346 --> 0.002345).  Saving model ...\nLoss decreased (0.002345 --> 0.002340).  Saving model ...\nEpoch: 29500, loss: 0.00244\nEpoch: 29600, loss: 0.00245\nLoss decreased (0.002340 --> 0.002334).  Saving model ...\nLoss decreased (0.002334 --> 0.002334).  Saving model ...\nEpoch: 29700, loss: 0.00235\nLoss decreased (0.002334 --> 0.002330).  Saving model ...\nEpoch: 29800, loss: 0.00252\nEpoch: 29900, loss: 0.00235\nLoss decreased (0.002330 --> 0.002329).  Saving model ...\nLoss decreased (0.002329 --> 0.002326).  Saving model ...\nLoss decreased (0.002326 --> 0.002322).  Saving model ...\nLoss decreased (0.002322 --> 0.002319).  Saving model ...\nEpoch: 30000, loss: 0.00237\nEpoch: 30100, loss: 0.00258\nEpoch: 30200, loss: 0.00246\nLoss decreased (0.002319 --> 0.002315).  Saving model ...\nEpoch: 30300, loss: 0.00233\nEpoch: 30400, loss: 0.00241\nLoss decreased (0.002315 --> 0.002305).  Saving model ...\nLoss decreased (0.002305 --> 0.002303).  Saving model ...\nLoss decreased (0.002303 --> 0.002303).  Saving model ...\nEpoch: 30500, loss: 0.00259\nEpoch: 30600, loss: 0.00242\nEpoch: 30700, loss: 0.00235\nEpoch: 30800, loss: 0.00233\nEpoch: 30900, loss: 0.00239\nEpoch: 31000, loss: 0.00238\nLoss decreased (0.002303 --> 0.002295).  Saving model ...\nEpoch: 31100, loss: 0.00235\nEpoch: 31200, loss: 0.00241\nLoss decreased (0.002295 --> 0.002293).  Saving model ...\nLoss decreased (0.002293 --> 0.002292).  Saving model ...\nLoss decreased (0.002292 --> 0.002285).  Saving model ...\nEpoch: 31300, loss: 0.00229\nEpoch: 31400, loss: 0.00232\nEpoch: 31500, loss: 0.00242\nLoss decreased (0.002285 --> 0.002282).  Saving model ...\nLoss decreased (0.002282 --> 0.002280).  Saving model ...\nLoss decreased (0.002280 --> 0.002275).  Saving model ...\nEpoch: 31600, loss: 0.00236\nEpoch: 31700, loss: 0.00236\nLoss decreased (0.002275 --> 0.002269).  Saving model ...\nEpoch: 31800, loss: 0.00231\nEpoch: 31900, loss: 0.00227\nLoss decreased (0.002269 --> 0.002265).  Saving model ...\nEpoch: 32000, loss: 0.00235\nEpoch: 32100, loss: 0.00247\nLoss decreased (0.002265 --> 0.002259).  Saving model ...\nEpoch: 32200, loss: 0.00230\nLoss decreased (0.002259 --> 0.002257).  Saving model ...\nEpoch: 32300, loss: 0.00229\nLoss decreased (0.002257 --> 0.002253).  Saving model ...\nEpoch: 32400, loss: 0.00240\nEpoch: 32500, loss: 0.00233\nEpoch: 32600, loss: 0.00231\nLoss decreased (0.002253 --> 0.002252).  Saving model ...\nLoss decreased (0.002252 --> 0.002251).  Saving model ...\nLoss decreased (0.002251 --> 0.002249).  Saving model ...\nEpoch: 32700, loss: 0.00237\nEpoch: 32800, loss: 0.00240\nLoss decreased (0.002249 --> 0.002248).  Saving model ...\nLoss decreased (0.002248 --> 0.002247).  Saving model ...\nLoss decreased (0.002247 --> 0.002237).  Saving model ...\nEpoch: 32900, loss: 0.00233\nEpoch: 33000, loss: 0.00234\nEpoch: 33100, loss: 0.00233\nLoss decreased (0.002237 --> 0.002236).  Saving model ...\nLoss decreased (0.002236 --> 0.002234).  Saving model ...\nEpoch: 33200, loss: 0.00228\nLoss decreased (0.002234 --> 0.002232).  Saving model ...\nLoss decreased (0.002232 --> 0.002227).  Saving model ...\nLoss decreased (0.002227 --> 0.002226).  Saving model ...\nLoss decreased (0.002226 --> 0.002225).  Saving model ...\nEpoch: 33300, loss: 0.00224\nEpoch: 33400, loss: 0.00223\nLoss decreased (0.002225 --> 0.002223).  Saving model ...\nEpoch: 33500, loss: 0.00246\nLoss decreased (0.002223 --> 0.002220).  Saving model ...\nEpoch: 33600, loss: 0.00223\nEpoch: 33700, loss: 0.00224\nLoss decreased (0.002220 --> 0.002215).  Saving model ...\nEpoch: 33800, loss: 0.00244\nEpoch: 33900, loss: 0.00230\nLoss decreased (0.002215 --> 0.002214).  Saving model ...\nLoss decreased (0.002214 --> 0.002213).  Saving model ...\nEpoch: 34000, loss: 0.00229\nLoss decreased (0.002213 --> 0.002209).  Saving model ...\nEpoch: 34100, loss: 0.00223\nLoss decreased (0.002209 --> 0.002208).  Saving model ...\nEpoch: 34200, loss: 0.00228\nLoss decreased (0.002208 --> 0.002208).  Saving model ...\nLoss decreased (0.002208 --> 0.002206).  Saving model ...\nEpoch: 34300, loss: 0.00223\nLoss decreased (0.002206 --> 0.002203).  Saving model ...\nLoss decreased (0.002203 --> 0.002203).  Saving model ...\nLoss decreased (0.002203 --> 0.002203).  Saving model ...\nEpoch: 34400, loss: 0.00234\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 34500, loss: 0.00222\nLoss decreased (0.002203 --> 0.002201).  Saving model ...\nEpoch: 34600, loss: 0.00226\nLoss decreased (0.002201 --> 0.002193).  Saving model ...\nLoss decreased (0.002193 --> 0.002193).  Saving model ...\nLoss decreased (0.002193 --> 0.002193).  Saving model ...\nLoss decreased (0.002193 --> 0.002190).  Saving model ...\nEpoch: 34700, loss: 0.00224\nEpoch: 34800, loss: 0.00230\nEpoch: 34900, loss: 0.00250\nEpoch: 35000, loss: 0.00241\nEpoch: 35100, loss: 0.00226\nLoss decreased (0.002190 --> 0.002188).  Saving model ...\nLoss decreased (0.002188 --> 0.002185).  Saving model ...\nLoss decreased (0.002185 --> 0.002179).  Saving model ...\nEpoch: 35200, loss: 0.00224\nEpoch: 35300, loss: 0.00231\nEpoch: 35400, loss: 0.00231\nLoss decreased (0.002179 --> 0.002175).  Saving model ...\nEpoch: 35500, loss: 0.00223\nEpoch: 35600, loss: 0.00236\nLoss decreased (0.002175 --> 0.002173).  Saving model ...\nLoss decreased (0.002173 --> 0.002165).  Saving model ...\nLoss decreased (0.002165 --> 0.002165).  Saving model ...\nEpoch: 35700, loss: 0.00228\nEpoch: 35800, loss: 0.00229\nEpoch: 35900, loss: 0.00222\nEpoch: 36000, loss: 0.00221\nLoss decreased (0.002165 --> 0.002164).  Saving model ...\nLoss decreased (0.002164 --> 0.002156).  Saving model ...\nEpoch: 36100, loss: 0.00242\nEpoch: 36200, loss: 0.00217\nEpoch: 36300, loss: 0.00223\nEpoch: 36400, loss: 0.00225\nLoss decreased (0.002156 --> 0.002156).  Saving model ...\nEpoch: 36500, loss: 0.00236\nLoss decreased (0.002156 --> 0.002149).  Saving model ...\nEpoch: 36600, loss: 0.00225\nEpoch: 36700, loss: 0.00239\nLoss decreased (0.002149 --> 0.002144).  Saving model ...\nEpoch: 36800, loss: 0.00248\nLoss decreased (0.002144 --> 0.002140).  Saving model ...\nEpoch: 36900, loss: 0.00238\nEpoch: 37000, loss: 0.00233\nEpoch: 37100, loss: 0.00219\nLoss decreased (0.002140 --> 0.002135).  Saving model ...\nEpoch: 37200, loss: 0.00238\nLoss decreased (0.002135 --> 0.002134).  Saving model ...\nLoss decreased (0.002134 --> 0.002127).  Saving model ...\nEpoch: 37300, loss: 0.00213\nEpoch: 37400, loss: 0.00220\nEpoch: 37500, loss: 0.00218\nEpoch: 37600, loss: 0.00224\nLoss decreased (0.002127 --> 0.002123).  Saving model ...\nEpoch: 37700, loss: 0.00221\nEpoch: 37800, loss: 0.00218\nEpoch: 37900, loss: 0.00218\nLoss decreased (0.002123 --> 0.002119).  Saving model ...\nEpoch: 38000, loss: 0.00214\nLoss decreased (0.002119 --> 0.002113).  Saving model ...\nLoss decreased (0.002113 --> 0.002113).  Saving model ...\nLoss decreased (0.002113 --> 0.002113).  Saving model ...\nEpoch: 38100, loss: 0.00241\nEpoch: 38200, loss: 0.00222\nLoss decreased (0.002113 --> 0.002112).  Saving model ...\nLoss decreased (0.002112 --> 0.002107).  Saving model ...\nEpoch: 38300, loss: 0.00233\nLoss decreased (0.002107 --> 0.002105).  Saving model ...\nEpoch: 38400, loss: 0.00217\nEpoch: 38500, loss: 0.00214\nLoss decreased (0.002105 --> 0.002103).  Saving model ...\nEpoch: 38600, loss: 0.00213\nEpoch: 38700, loss: 0.00229\nEpoch: 38800, loss: 0.00218\nLoss decreased (0.002103 --> 0.002103).  Saving model ...\nEpoch: 38900, loss: 0.00218\nLoss decreased (0.002103 --> 0.002097).  Saving model ...\nLoss decreased (0.002097 --> 0.002097).  Saving model ...\nEpoch: 39000, loss: 0.00211\nLoss decreased (0.002097 --> 0.002096).  Saving model ...\nLoss decreased (0.002096 --> 0.002089).  Saving model ...\nEpoch: 39100, loss: 0.00247\nEpoch: 39200, loss: 0.00224\nLoss decreased (0.002089 --> 0.002084).  Saving model ...\nEpoch: 39300, loss: 0.00223\nLoss decreased (0.002084 --> 0.002083).  Saving model ...\nEpoch: 39400, loss: 0.00214\nLoss decreased (0.002083 --> 0.002081).  Saving model ...\nEpoch: 39500, loss: 0.00212\nEpoch: 39600, loss: 0.00217\nLoss decreased (0.002081 --> 0.002081).  Saving model ...\nEpoch: 39700, loss: 0.00216\nLoss decreased (0.002081 --> 0.002076).  Saving model ...\nEpoch: 39800, loss: 0.00220\nEpoch: 39900, loss: 0.00218\nEpoch: 40000, loss: 0.00212\nLoss decreased (0.002076 --> 0.002074).  Saving model ...\nEpoch: 40100, loss: 0.00220\nEpoch: 40200, loss: 0.00224\nLoss decreased (0.002074 --> 0.002073).  Saving model ...\nLoss decreased (0.002073 --> 0.002072).  Saving model ...\nEpoch: 40300, loss: 0.00242\nLoss decreased (0.002072 --> 0.002067).  Saving model ...\nEpoch: 40400, loss: 0.00211\nLoss decreased (0.002067 --> 0.002066).  Saving model ...\nLoss decreased (0.002066 --> 0.002065).  Saving model ...\nEpoch: 40500, loss: 0.00225\nLoss decreased (0.002065 --> 0.002061).  Saving model ...\nEpoch: 40600, loss: 0.00207\nLoss decreased (0.002061 --> 0.002060).  Saving model ...\nEpoch: 40700, loss: 0.00223\nLoss decreased (0.002060 --> 0.002057).  Saving model ...\nLoss decreased (0.002057 --> 0.002055).  Saving model ...\nEpoch: 40800, loss: 0.00213\nEpoch: 40900, loss: 0.00223\nEpoch: 41000, loss: 0.00219\nLoss decreased (0.002055 --> 0.002049).  Saving model ...\nEpoch: 41100, loss: 0.00214\nLoss decreased (0.002049 --> 0.002047).  Saving model ...\nEpoch: 41200, loss: 0.00213\nLoss decreased (0.002047 --> 0.002046).  Saving model ...\nEpoch: 41300, loss: 0.00213\nEpoch: 41400, loss: 0.00221\nEpoch: 41500, loss: 0.00207\nLoss decreased (0.002046 --> 0.002042).  Saving model ...\nLoss decreased (0.002042 --> 0.002037).  Saving model ...\nEpoch: 41600, loss: 0.00227\nEpoch: 41700, loss: 0.00215\nEpoch: 41800, loss: 0.00223\nLoss decreased (0.002037 --> 0.002036).  Saving model ...\nLoss decreased (0.002036 --> 0.002034).  Saving model ...\nLoss decreased (0.002034 --> 0.002034).  Saving model ...\nEpoch: 41900, loss: 0.00214\nEpoch: 42000, loss: 0.00209\nLoss decreased (0.002034 --> 0.002026).  Saving model ...\nEpoch: 42100, loss: 0.00205\nEpoch: 42200, loss: 0.00211\nEpoch: 42300, loss: 0.00212\nLoss decreased (0.002026 --> 0.002025).  Saving model ...\nLoss decreased (0.002025 --> 0.002019).  Saving model ...\nEpoch: 42400, loss: 0.00203\nLoss decreased (0.002019 --> 0.002018).  Saving model ...\nEpoch: 42500, loss: 0.00223\nEpoch: 42600, loss: 0.00209\nEpoch: 42700, loss: 0.00213\nEpoch: 42800, loss: 0.00207\nEpoch: 42900, loss: 0.00205\nLoss decreased (0.002018 --> 0.002017).  Saving model ...\nLoss decreased (0.002017 --> 0.002015).  Saving model ...\nEpoch: 43000, loss: 0.00216\nLoss decreased (0.002015 --> 0.002010).  Saving model ...\nEpoch: 43100, loss: 0.00211\nEpoch: 43200, loss: 0.00211\nLoss decreased (0.002010 --> 0.002007).  Saving model ...\nEpoch: 43300, loss: 0.00214\nEpoch: 43400, loss: 0.00218\nEpoch: 43500, loss: 0.00218\nLoss decreased (0.002007 --> 0.002003).  Saving model ...\nLoss decreased (0.002003 --> 0.002001).  Saving model ...\nEpoch: 43600, loss: 0.00225\nEpoch: 43700, loss: 0.00211\nLoss decreased (0.002001 --> 0.002000).  Saving model ...\nLoss decreased (0.002000 --> 0.001993).  Saving model ...\nEpoch: 43800, loss: 0.00207\nEpoch: 43900, loss: 0.00206\nLoss decreased (0.001993 --> 0.001992).  Saving model ...\nEpoch: 44000, loss: 0.00205\nLoss decreased (0.001992 --> 0.001984).  Saving model ...\nEpoch: 44100, loss: 0.00220\nEpoch: 44200, loss: 0.00205\nEpoch: 44300, loss: 0.00212\nEpoch: 44400, loss: 0.00219\nLoss decreased (0.001984 --> 0.001983).  Saving model ...\nEpoch: 44500, loss: 0.00203\nEpoch: 44600, loss: 0.00206\nLoss decreased (0.001983 --> 0.001981).  Saving model ...\nEpoch: 44700, loss: 0.00199\nEpoch: 44800, loss: 0.00200\nLoss decreased (0.001981 --> 0.001976).  Saving model ...\nLoss decreased (0.001976 --> 0.001971).  Saving model ...\nEpoch: 44900, loss: 0.00201\nLoss decreased (0.001971 --> 0.001971).  Saving model ...\nLoss decreased (0.001971 --> 0.001969).  Saving model ...\nEpoch: 45000, loss: 0.00226\nEpoch: 45100, loss: 0.00204\nEpoch: 45200, loss: 0.00203\nLoss decreased (0.001969 --> 0.001964).  Saving model ...\nEpoch: 45300, loss: 0.00222\nEpoch: 45400, loss: 0.00202\nEpoch: 45500, loss: 0.00204\nLoss decreased (0.001964 --> 0.001964).  Saving model ...\nEpoch: 45600, loss: 0.00198\nEpoch: 45700, loss: 0.00208\nLoss decreased (0.001964 --> 0.001963).  Saving model ...\nLoss decreased (0.001963 --> 0.001953).  Saving model ...\nEpoch: 45800, loss: 0.00215\nEpoch: 45900, loss: 0.00211\nEpoch: 46000, loss: 0.00207\nEpoch: 46100, loss: 0.00210\nEpoch: 46200, loss: 0.00199\nLoss decreased (0.001953 --> 0.001947).  Saving model ...\nEpoch: 46300, loss: 0.00207\nLoss decreased (0.001947 --> 0.001942).  Saving model ...\nEpoch: 46400, loss: 0.00197\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 46500, loss: 0.00206\nEpoch: 46600, loss: 0.00205\nLoss decreased (0.001942 --> 0.001942).  Saving model ...\nEpoch: 46700, loss: 0.00200\nLoss decreased (0.001942 --> 0.001940).  Saving model ...\nLoss decreased (0.001940 --> 0.001935).  Saving model ...\nEpoch: 46800, loss: 0.00222\nEpoch: 46900, loss: 0.00208\nEpoch: 47000, loss: 0.00215\nEpoch: 47100, loss: 0.00203\nLoss decreased (0.001935 --> 0.001934).  Saving model ...\nLoss decreased (0.001934 --> 0.001929).  Saving model ...\nEpoch: 47200, loss: 0.00209\nEpoch: 47300, loss: 0.00204\nLoss decreased (0.001929 --> 0.001926).  Saving model ...\nEpoch: 47400, loss: 0.00220\nLoss decreased (0.001926 --> 0.001926).  Saving model ...\nLoss decreased (0.001926 --> 0.001925).  Saving model ...\nEpoch: 47500, loss: 0.00201\nLoss decreased (0.001925 --> 0.001919).  Saving model ...\nEpoch: 47600, loss: 0.00197\nEpoch: 47700, loss: 0.00208\nLoss decreased (0.001919 --> 0.001918).  Saving model ...\nEpoch: 47800, loss: 0.00200\nLoss decreased (0.001918 --> 0.001915).  Saving model ...\nEpoch: 47900, loss: 0.00226\nLoss decreased (0.001915 --> 0.001914).  Saving model ...\nEpoch: 48000, loss: 0.00201\nLoss decreased (0.001914 --> 0.001913).  Saving model ...\nEpoch: 48100, loss: 0.00218\nEpoch: 48200, loss: 0.00223\nLoss decreased (0.001913 --> 0.001908).  Saving model ...\nEpoch: 48300, loss: 0.00340\nEpoch: 48400, loss: 0.00197\nLoss decreased (0.001908 --> 0.001907).  Saving model ...\nEpoch: 48500, loss: 0.00195\nEpoch: 48600, loss: 0.00203\nLoss decreased (0.001907 --> 0.001904).  Saving model ...\nEpoch: 48700, loss: 0.00221\nLoss decreased (0.001904 --> 0.001895).  Saving model ...\nEpoch: 48800, loss: 0.00199\nEpoch: 48900, loss: 0.00202\nEpoch: 49000, loss: 0.00203\nLoss decreased (0.001895 --> 0.001894).  Saving model ...\nEpoch: 49100, loss: 0.00211\nEpoch: 49200, loss: 0.00204\nLoss decreased (0.001894 --> 0.001893).  Saving model ...\nEpoch: 49300, loss: 0.00191\nLoss decreased (0.001893 --> 0.001891).  Saving model ...\nEpoch: 49400, loss: 0.00214\nLoss decreased (0.001891 --> 0.001890).  Saving model ...\nEpoch: 49500, loss: 0.00192\nLoss decreased (0.001890 --> 0.001887).  Saving model ...\nEpoch: 49600, loss: 0.00193\nEpoch: 49700, loss: 0.00190\nLoss decreased (0.001887 --> 0.001886).  Saving model ...\nEpoch: 49800, loss: 0.00225\nLoss decreased (0.001886 --> 0.001876).  Saving model ...\nEpoch: 49900, loss: 0.00210\nEpoch: 50000, loss: 0.00211\nEpoch: 50100, loss: 0.00197\nEpoch: 50200, loss: 0.00191\nEpoch: 50300, loss: 0.00202\nLoss decreased (0.001876 --> 0.001874).  Saving model ...\nLoss decreased (0.001874 --> 0.001870).  Saving model ...\nEpoch: 50400, loss: 0.00194\nEpoch: 50500, loss: 0.00196\nLoss decreased (0.001870 --> 0.001867).  Saving model ...\nEpoch: 50600, loss: 0.00205\nEpoch: 50700, loss: 0.00202\nLoss decreased (0.001867 --> 0.001866).  Saving model ...\nLoss decreased (0.001866 --> 0.001860).  Saving model ...\nEpoch: 50800, loss: 0.00191\nEpoch: 50900, loss: 0.00191\nEpoch: 51000, loss: 0.00200\nLoss decreased (0.001860 --> 0.001859).  Saving model ...\nLoss decreased (0.001859 --> 0.001858).  Saving model ...\nEpoch: 51100, loss: 0.00191\nEpoch: 51200, loss: 0.00189\nEpoch: 51300, loss: 0.00192\nLoss decreased (0.001858 --> 0.001857).  Saving model ...\nLoss decreased (0.001857 --> 0.001847).  Saving model ...\nEpoch: 51400, loss: 0.00204\nEpoch: 51500, loss: 0.00186\nEpoch: 51600, loss: 0.00211\nEpoch: 51700, loss: 0.00201\nEpoch: 51800, loss: 0.00198\nLoss decreased (0.001847 --> 0.001846).  Saving model ...\nEpoch: 51900, loss: 0.00199\nEpoch: 52000, loss: 0.00203\nLoss decreased (0.001846 --> 0.001836).  Saving model ...\nEpoch: 52100, loss: 0.00186\nLoss decreased (0.001836 --> 0.001835).  Saving model ...\nEpoch: 52200, loss: 0.00197\nEpoch: 52300, loss: 0.00209\nEpoch: 52400, loss: 0.00187\nEpoch: 52500, loss: 0.00195\nLoss decreased (0.001835 --> 0.001829).  Saving model ...\nLoss decreased (0.001829 --> 0.001829).  Saving model ...\nEpoch: 52600, loss: 0.00203\nLoss decreased (0.001829 --> 0.001827).  Saving model ...\nLoss decreased (0.001827 --> 0.001827).  Saving model ...\nEpoch: 52700, loss: 0.00188\nEpoch: 52800, loss: 0.00201\nEpoch: 52900, loss: 0.00194\nEpoch: 53000, loss: 0.00189\nLoss decreased (0.001827 --> 0.001816).  Saving model ...\nEpoch: 53100, loss: 0.00189\nEpoch: 53200, loss: 0.00197\nEpoch: 53300, loss: 0.00239\nLoss decreased (0.001816 --> 0.001815).  Saving model ...\nLoss decreased (0.001815 --> 0.001813).  Saving model ...\nLoss decreased (0.001813 --> 0.001810).  Saving model ...\nEpoch: 53400, loss: 0.00213\nEpoch: 53500, loss: 0.00192\nEpoch: 53600, loss: 0.00196\nEpoch: 53700, loss: 0.00199\nEpoch: 53800, loss: 0.00199\nLoss decreased (0.001810 --> 0.001807).  Saving model ...\nLoss decreased (0.001807 --> 0.001807).  Saving model ...\nLoss decreased (0.001807 --> 0.001805).  Saving model ...\nEpoch: 53900, loss: 0.00188\nEpoch: 54000, loss: 0.00198\nLoss decreased (0.001805 --> 0.001803).  Saving model ...\nLoss decreased (0.001803 --> 0.001802).  Saving model ...\nLoss decreased (0.001802 --> 0.001802).  Saving model ...\nEpoch: 54100, loss: 0.00183\nEpoch: 54200, loss: 0.00196\nLoss decreased (0.001802 --> 0.001800).  Saving model ...\nEpoch: 54300, loss: 0.00188\nEpoch: 54400, loss: 0.00182\nEpoch: 54500, loss: 0.00182\nEpoch: 54600, loss: 0.00184\nEpoch: 54700, loss: 0.00203\nLoss decreased (0.001800 --> 0.001798).  Saving model ...\nEpoch: 54800, loss: 0.00189\nLoss decreased (0.001798 --> 0.001797).  Saving model ...\nLoss decreased (0.001797 --> 0.001792).  Saving model ...\nEpoch: 54900, loss: 0.00184\nLoss decreased (0.001792 --> 0.001785).  Saving model ...\nLoss decreased (0.001785 --> 0.001782).  Saving model ...\nEpoch: 55000, loss: 0.00200\nEpoch: 55100, loss: 0.00182\nEpoch: 55200, loss: 0.00196\nEpoch: 55300, loss: 0.00200\nEpoch: 55400, loss: 0.00190\nLoss decreased (0.001782 --> 0.001782).  Saving model ...\nEpoch: 55500, loss: 0.00179\nEpoch: 55600, loss: 0.00196\nLoss decreased (0.001782 --> 0.001777).  Saving model ...\nEpoch: 55700, loss: 0.00203\nEpoch: 55800, loss: 0.00187\nLoss decreased (0.001777 --> 0.001776).  Saving model ...\nEpoch: 55900, loss: 0.00188\nLoss decreased (0.001776 --> 0.001772).  Saving model ...\nEpoch: 56000, loss: 0.00192\nLoss decreased (0.001772 --> 0.001770).  Saving model ...\nEpoch: 56100, loss: 0.00208\nLoss decreased (0.001770 --> 0.001770).  Saving model ...\nEpoch: 56200, loss: 0.00202\nEpoch: 56300, loss: 0.00187\nLoss decreased (0.001770 --> 0.001770).  Saving model ...\nLoss decreased (0.001770 --> 0.001766).  Saving model ...\nLoss decreased (0.001766 --> 0.001765).  Saving model ...\nEpoch: 56400, loss: 0.00187\nLoss decreased (0.001765 --> 0.001761).  Saving model ...\nEpoch: 56500, loss: 0.00182\nEpoch: 56600, loss: 0.00190\nEpoch: 56700, loss: 0.00184\nEpoch: 56800, loss: 0.00183\nLoss decreased (0.001761 --> 0.001756).  Saving model ...\nEpoch: 56900, loss: 0.00184\nEpoch: 57000, loss: 0.00177\nEpoch: 57100, loss: 0.00188\nEpoch: 57200, loss: 0.00187\nEpoch: 57300, loss: 0.00185\nEpoch: 57400, loss: 0.00201\nEpoch: 57500, loss: 0.00189\nEpoch: 57600, loss: 0.00185\nLoss decreased (0.001756 --> 0.001752).  Saving model ...\nLoss decreased (0.001752 --> 0.001749).  Saving model ...\nEpoch: 57700, loss: 0.00184\nLoss decreased (0.001749 --> 0.001743).  Saving model ...\nEpoch: 57800, loss: 0.00177\nEpoch: 57900, loss: 0.00178\nLoss decreased (0.001743 --> 0.001736).  Saving model ...\nLoss decreased (0.001736 --> 0.001734).  Saving model ...\nEpoch: 58000, loss: 0.00184\nEpoch: 58100, loss: 0.00179\nEpoch: 58200, loss: 0.00201\nEpoch: 58300, loss: 0.00181\nEpoch: 58400, loss: 0.00173\nLoss decreased (0.001734 --> 0.001730).  Saving model ...\nLoss decreased (0.001730 --> 0.001724).  Saving model ...\nEpoch: 58500, loss: 0.00190\nEpoch: 58600, loss: 0.00189\nEpoch: 58700, loss: 0.00186\nEpoch: 58800, loss: 0.00196\nLoss decreased (0.001724 --> 0.001723).  Saving model ...\nEpoch: 58900, loss: 0.00178\nLoss decreased (0.001723 --> 0.001720).  Saving model ...\nEpoch: 59000, loss: 0.00181\nEpoch: 59100, loss: 0.00186\nLoss decreased (0.001720 --> 0.001720).  Saving model ...\nEpoch: 59200, loss: 0.00195\nLoss decreased (0.001720 --> 0.001720).  Saving model ...\nEpoch: 59300, loss: 0.00173\nEpoch: 59400, loss: 0.00191\nLoss decreased (0.001720 --> 0.001719).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 59500, loss: 0.00183\nEpoch: 59600, loss: 0.00206\nEpoch: 59700, loss: 0.00196\nLoss decreased (0.001719 --> 0.001717).  Saving model ...\nLoss decreased (0.001717 --> 0.001717).  Saving model ...\nEpoch: 59800, loss: 0.00177\nLoss decreased (0.001717 --> 0.001714).  Saving model ...\nLoss decreased (0.001714 --> 0.001714).  Saving model ...\nEpoch: 59900, loss: 0.00199\nLoss decreased (0.001714 --> 0.001710).  Saving model ...\nEpoch: 60000, loss: 0.00197\nEpoch: 60100, loss: 0.00172\nEpoch: 60200, loss: 0.00177\nLoss decreased (0.001710 --> 0.001704).  Saving model ...\nEpoch: 60300, loss: 0.00176\nLoss decreased (0.001704 --> 0.001703).  Saving model ...\nLoss decreased (0.001703 --> 0.001702).  Saving model ...\nLoss decreased (0.001702 --> 0.001697).  Saving model ...\nEpoch: 60400, loss: 0.00199\nEpoch: 60500, loss: 0.00193\nLoss decreased (0.001697 --> 0.001696).  Saving model ...\nEpoch: 60600, loss: 0.00186\nEpoch: 60700, loss: 0.00188\nLoss decreased (0.001696 --> 0.001693).  Saving model ...\nLoss decreased (0.001693 --> 0.001693).  Saving model ...\nEpoch: 60800, loss: 0.00179\nEpoch: 60900, loss: 0.00197\nEpoch: 61000, loss: 0.00171\nLoss decreased (0.001693 --> 0.001688).  Saving model ...\nEpoch: 61100, loss: 0.00169\nEpoch: 61200, loss: 0.00221\nEpoch: 61300, loss: 0.00186\nEpoch: 61400, loss: 0.00172\nEpoch: 61500, loss: 0.00191\nLoss decreased (0.001688 --> 0.001684).  Saving model ...\nEpoch: 61600, loss: 0.00176\nLoss decreased (0.001684 --> 0.001683).  Saving model ...\nLoss decreased (0.001683 --> 0.001681).  Saving model ...\nEpoch: 61700, loss: 0.00193\nEpoch: 61800, loss: 0.00184\nEpoch: 61900, loss: 0.00173\nLoss decreased (0.001681 --> 0.001678).  Saving model ...\nEpoch: 62000, loss: 0.00193\nEpoch: 62100, loss: 0.00174\nEpoch: 62200, loss: 0.00177\nEpoch: 62300, loss: 0.00183\nLoss decreased (0.001678 --> 0.001675).  Saving model ...\nEpoch: 62400, loss: 0.00186\nEpoch: 62500, loss: 0.00174\nLoss decreased (0.001675 --> 0.001674).  Saving model ...\nLoss decreased (0.001674 --> 0.001671).  Saving model ...\nEpoch: 62600, loss: 0.00176\nEpoch: 62700, loss: 0.00192\nLoss decreased (0.001671 --> 0.001669).  Saving model ...\nLoss decreased (0.001669 --> 0.001664).  Saving model ...\nEpoch: 62800, loss: 0.00193\nLoss decreased (0.001664 --> 0.001663).  Saving model ...\nEpoch: 62900, loss: 0.00196\nLoss decreased (0.001663 --> 0.001663).  Saving model ...\nEpoch: 63000, loss: 0.00168\nLoss decreased (0.001663 --> 0.001660).  Saving model ...\nEpoch: 63100, loss: 0.00184\nEpoch: 63200, loss: 0.00175\nEpoch: 63300, loss: 0.00200\nEpoch: 63400, loss: 0.00182\nEpoch: 63500, loss: 0.00168\nEpoch: 63600, loss: 0.00187\nEpoch: 63700, loss: 0.00182\nEpoch: 63800, loss: 0.00186\nLoss decreased (0.001660 --> 0.001653).  Saving model ...\nLoss decreased (0.001653 --> 0.001648).  Saving model ...\nEpoch: 63900, loss: 0.00169\nEpoch: 64000, loss: 0.00187\nEpoch: 64100, loss: 0.00199\nEpoch: 64200, loss: 0.00176\nEpoch: 64300, loss: 0.00172\nEpoch: 64400, loss: 0.00215\nLoss decreased (0.001648 --> 0.001648).  Saving model ...\nEpoch: 64500, loss: 0.00195\nLoss decreased (0.001648 --> 0.001641).  Saving model ...\nLoss decreased (0.001641 --> 0.001636).  Saving model ...\nEpoch: 64600, loss: 0.00188\nEpoch: 64700, loss: 0.00191\nEpoch: 64800, loss: 0.00166\nEpoch: 64900, loss: 0.00197\nEpoch: 65000, loss: 0.00182\nEpoch: 65100, loss: 0.00188\nEpoch: 65200, loss: 0.00181\nEpoch: 65300, loss: 0.00211\nEpoch: 65400, loss: 0.00174\nLoss decreased (0.001636 --> 0.001635).  Saving model ...\nLoss decreased (0.001635 --> 0.001631).  Saving model ...\nEpoch: 65500, loss: 0.00212\nEpoch: 65600, loss: 0.00175\nLoss decreased (0.001631 --> 0.001630).  Saving model ...\nEpoch: 65700, loss: 0.00172\nEpoch: 65800, loss: 0.00209\nLoss decreased (0.001630 --> 0.001629).  Saving model ...\nLoss decreased (0.001629 --> 0.001626).  Saving model ...\nEpoch: 65900, loss: 0.00169\nEpoch: 66000, loss: 0.00191\nLoss decreased (0.001626 --> 0.001622).  Saving model ...\nLoss decreased (0.001622 --> 0.001621).  Saving model ...\nEpoch: 66100, loss: 0.00177\nEpoch: 66200, loss: 0.00173\nEpoch: 66300, loss: 0.00176\nEpoch: 66400, loss: 0.00183\nEpoch: 66500, loss: 0.00172\nLoss decreased (0.001621 --> 0.001610).  Saving model ...\nEpoch: 66600, loss: 0.00169\nEpoch: 66700, loss: 0.00177\nEpoch: 66800, loss: 0.00200\nEpoch: 66900, loss: 0.00188\nEpoch: 67000, loss: 0.00166\nLoss decreased (0.001610 --> 0.001608).  Saving model ...\nLoss decreased (0.001608 --> 0.001607).  Saving model ...\nEpoch: 67100, loss: 0.00164\nEpoch: 67200, loss: 0.00166\nEpoch: 67300, loss: 0.00175\nLoss decreased (0.001607 --> 0.001605).  Saving model ...\nEpoch: 67400, loss: 0.00169\nEpoch: 67500, loss: 0.00177\nEpoch: 67600, loss: 0.00172\nEpoch: 67700, loss: 0.00179\nLoss decreased (0.001605 --> 0.001600).  Saving model ...\nEpoch: 67800, loss: 0.00171\nEpoch: 67900, loss: 0.00180\nEpoch: 68000, loss: 0.00164\nEpoch: 68100, loss: 0.00165\nEpoch: 68200, loss: 0.00164\nLoss decreased (0.001600 --> 0.001595).  Saving model ...\nEpoch: 68300, loss: 0.00177\nEpoch: 68400, loss: 0.00164\nEpoch: 68500, loss: 0.00178\nEpoch: 68600, loss: 0.00162\nEpoch: 68700, loss: 0.00289\nEpoch: 68800, loss: 0.00162\nEpoch: 68900, loss: 0.00184\nLoss decreased (0.001595 --> 0.001593).  Saving model ...\nEpoch: 69000, loss: 0.00170\nLoss decreased (0.001593 --> 0.001592).  Saving model ...\nLoss decreased (0.001592 --> 0.001591).  Saving model ...\nEpoch: 69100, loss: 0.00164\nEpoch: 69200, loss: 0.00176\nEpoch: 69300, loss: 0.00171\nEpoch: 69400, loss: 0.00165\nEpoch: 69500, loss: 0.00172\nLoss decreased (0.001591 --> 0.001588).  Saving model ...\nEpoch: 69600, loss: 0.00171\nLoss decreased (0.001588 --> 0.001584).  Saving model ...\nEpoch: 69700, loss: 0.00167\nLoss decreased (0.001584 --> 0.001584).  Saving model ...\nLoss decreased (0.001584 --> 0.001583).  Saving model ...\nEpoch: 69800, loss: 0.00184\nLoss decreased (0.001583 --> 0.001581).  Saving model ...\nLoss decreased (0.001581 --> 0.001580).  Saving model ...\nEpoch: 69900, loss: 0.00175\nEpoch: 70000, loss: 0.00169\nLoss decreased (0.001580 --> 0.001576).  Saving model ...\nEpoch: 70100, loss: 0.00167\nEpoch: 70200, loss: 0.00184\nEpoch: 70300, loss: 0.00165\nLoss decreased (0.001576 --> 0.001574).  Saving model ...\nEpoch: 70400, loss: 0.00165\nLoss decreased (0.001574 --> 0.001574).  Saving model ...\nLoss decreased (0.001574 --> 0.001570).  Saving model ...\nEpoch: 70500, loss: 0.00178\nEpoch: 70600, loss: 0.00176\nEpoch: 70700, loss: 0.00181\nLoss decreased (0.001570 --> 0.001570).  Saving model ...\nEpoch: 70800, loss: 0.00181\nEpoch: 70900, loss: 0.00160\nEpoch: 71000, loss: 0.00184\nEpoch: 71100, loss: 0.00174\nLoss decreased (0.001570 --> 0.001569).  Saving model ...\nLoss decreased (0.001569 --> 0.001565).  Saving model ...\nEpoch: 71200, loss: 0.00283\nEpoch: 71300, loss: 0.00170\nEpoch: 71400, loss: 0.00172\nEpoch: 71500, loss: 0.00177\nLoss decreased (0.001565 --> 0.001565).  Saving model ...\nEpoch: 71600, loss: 0.00164\nLoss decreased (0.001565 --> 0.001564).  Saving model ...\nEpoch: 71700, loss: 0.00162\nEpoch: 71800, loss: 0.00188\nEpoch: 71900, loss: 0.00157\nLoss decreased (0.001564 --> 0.001564).  Saving model ...\nEpoch: 72000, loss: 0.00168\nLoss decreased (0.001564 --> 0.001560).  Saving model ...\nEpoch: 72100, loss: 0.00180\nLoss decreased (0.001560 --> 0.001560).  Saving model ...\nEpoch: 72200, loss: 0.00173\nEpoch: 72300, loss: 0.00160\nEpoch: 72400, loss: 0.00167\nEpoch: 72500, loss: 0.00178\nEpoch: 72600, loss: 0.00178\nEpoch: 72700, loss: 0.00160\nEpoch: 72800, loss: 0.00162\nLoss decreased (0.001560 --> 0.001556).  Saving model ...\nEpoch: 72900, loss: 0.00179\nLoss decreased (0.001556 --> 0.001552).  Saving model ...\nEpoch: 73000, loss: 0.00184\nEpoch: 73100, loss: 0.00174\nEpoch: 73200, loss: 0.00205\nLoss decreased (0.001552 --> 0.001552).  Saving model ...\nLoss decreased (0.001552 --> 0.001548).  Saving model ...\nEpoch: 73300, loss: 0.00184\nLoss decreased (0.001548 --> 0.001546).  Saving model ...\nEpoch: 73400, loss: 0.00169\nEpoch: 73500, loss: 0.00161\nEpoch: 73600, loss: 0.00169\nEpoch: 73700, loss: 0.00193\nEpoch: 73800, loss: 0.00172\nLoss decreased (0.001546 --> 0.001544).  Saving model ...\nEpoch: 73900, loss: 0.00156\nEpoch: 74000, loss: 0.00160\nEpoch: 74100, loss: 0.00158\nLoss decreased (0.001544 --> 0.001541).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 74200, loss: 0.00156\nEpoch: 74300, loss: 0.00175\nLoss decreased (0.001541 --> 0.001537).  Saving model ...\nEpoch: 74400, loss: 0.00164\nEpoch: 74500, loss: 0.00160\nEpoch: 74600, loss: 0.00154\nLoss decreased (0.001537 --> 0.001536).  Saving model ...\nEpoch: 74700, loss: 0.00161\nLoss decreased (0.001536 --> 0.001535).  Saving model ...\nEpoch: 74800, loss: 0.00185\nEpoch: 74900, loss: 0.00175\nEpoch: 75000, loss: 0.00157\nLoss decreased (0.001535 --> 0.001534).  Saving model ...\nEpoch: 75100, loss: 0.00154\nEpoch: 75200, loss: 0.00162\nLoss decreased (0.001534 --> 0.001531).  Saving model ...\nEpoch: 75300, loss: 0.00180\nEpoch: 75400, loss: 0.00159\nEpoch: 75500, loss: 0.00172\nEpoch: 75600, loss: 0.00218\nLoss decreased (0.001531 --> 0.001530).  Saving model ...\nLoss decreased (0.001530 --> 0.001529).  Saving model ...\nEpoch: 75700, loss: 0.00174\nEpoch: 75800, loss: 0.00155\nEpoch: 75900, loss: 0.00155\nLoss decreased (0.001529 --> 0.001528).  Saving model ...\nLoss decreased (0.001528 --> 0.001525).  Saving model ...\nEpoch: 76000, loss: 0.00165\nEpoch: 76100, loss: 0.00179\nEpoch: 76200, loss: 0.00166\nEpoch: 76300, loss: 0.00169\nLoss decreased (0.001525 --> 0.001520).  Saving model ...\nEpoch: 76400, loss: 0.00161\nEpoch: 76500, loss: 0.00165\nEpoch: 76600, loss: 0.00161\nEpoch: 76700, loss: 0.00158\nLoss decreased (0.001520 --> 0.001511).  Saving model ...\nEpoch: 76800, loss: 0.00171\nEpoch: 76900, loss: 0.00157\nEpoch: 77000, loss: 0.00159\nLoss decreased (0.001511 --> 0.001508).  Saving model ...\nEpoch: 77100, loss: 0.00172\nEpoch: 77200, loss: 0.00152\nEpoch: 77300, loss: 0.00177\nLoss decreased (0.001508 --> 0.001507).  Saving model ...\nLoss decreased (0.001507 --> 0.001503).  Saving model ...\nEpoch: 77400, loss: 0.00179\nEpoch: 77500, loss: 0.00175\nEpoch: 77600, loss: 0.00162\nLoss decreased (0.001503 --> 0.001502).  Saving model ...\nEpoch: 77700, loss: 0.00181\nEpoch: 77800, loss: 0.00186\nEpoch: 77900, loss: 0.00169\nLoss decreased (0.001502 --> 0.001501).  Saving model ...\nEpoch: 78000, loss: 0.00164\nEpoch: 78100, loss: 0.00195\nEpoch: 78200, loss: 0.00168\nEpoch: 78300, loss: 0.00157\nEpoch: 78400, loss: 0.00188\nLoss decreased (0.001501 --> 0.001498).  Saving model ...\nEpoch: 78500, loss: 0.00161\nEpoch: 78600, loss: 0.00187\nLoss decreased (0.001498 --> 0.001494).  Saving model ...\nEpoch: 78700, loss: 0.00161\nEpoch: 78800, loss: 0.00170\nLoss decreased (0.001494 --> 0.001493).  Saving model ...\nEpoch: 78900, loss: 0.00169\nLoss decreased (0.001493 --> 0.001490).  Saving model ...\nEpoch: 79000, loss: 0.00153\nEpoch: 79100, loss: 0.00169\nEpoch: 79200, loss: 0.00164\nEpoch: 79300, loss: 0.00163\nEpoch: 79400, loss: 0.00171\nEpoch: 79500, loss: 0.00161\nLoss decreased (0.001490 --> 0.001487).  Saving model ...\nEpoch: 79600, loss: 0.00355\nEpoch: 79700, loss: 0.00160\nLoss decreased (0.001487 --> 0.001485).  Saving model ...\nLoss decreased (0.001485 --> 0.001484).  Saving model ...\nEpoch: 79800, loss: 0.00171\nEpoch: 79900, loss: 0.00161\nLoss decreased (0.001484 --> 0.001480).  Saving model ...\nLoss decreased (0.001480 --> 0.001477).  Saving model ...\nEpoch: 80000, loss: 0.00156\nEpoch: 80100, loss: 0.00153\nEpoch: 80200, loss: 0.00165\nEpoch: 80300, loss: 0.00167\nEpoch: 80400, loss: 0.00161\nEpoch: 80500, loss: 0.00192\nEpoch: 80600, loss: 0.00173\nEpoch: 80700, loss: 0.00172\nEpoch: 80800, loss: 0.00151\nEpoch: 80900, loss: 0.00182\nEpoch: 81000, loss: 0.00179\nEpoch: 81100, loss: 0.00152\nEpoch: 81200, loss: 0.00155\nLoss decreased (0.001477 --> 0.001477).  Saving model ...\nEpoch: 81300, loss: 0.00161\nEpoch: 81400, loss: 0.00163\nLoss decreased (0.001477 --> 0.001473).  Saving model ...\nEpoch: 81500, loss: 0.00154\nEpoch: 81600, loss: 0.00153\nLoss decreased (0.001473 --> 0.001472).  Saving model ...\nEpoch: 81700, loss: 0.00250\nEpoch: 81800, loss: 0.00161\nEpoch: 81900, loss: 0.00157\nEpoch: 82000, loss: 0.00158\nLoss decreased (0.001472 --> 0.001470).  Saving model ...\nEpoch: 82100, loss: 0.00152\nLoss decreased (0.001470 --> 0.001468).  Saving model ...\nEpoch: 82200, loss: 0.00159\nLoss decreased (0.001468 --> 0.001468).  Saving model ...\nLoss decreased (0.001468 --> 0.001467).  Saving model ...\nEpoch: 82300, loss: 0.00159\nLoss decreased (0.001467 --> 0.001463).  Saving model ...\nEpoch: 82400, loss: 0.00161\nLoss decreased (0.001463 --> 0.001459).  Saving model ...\nEpoch: 82500, loss: 0.00157\nEpoch: 82600, loss: 0.00159\nEpoch: 82700, loss: 0.00150\nLoss decreased (0.001459 --> 0.001457).  Saving model ...\nEpoch: 82800, loss: 0.00166\nEpoch: 82900, loss: 0.00158\nEpoch: 83000, loss: 0.00154\nLoss decreased (0.001457 --> 0.001455).  Saving model ...\nEpoch: 83100, loss: 0.00165\nEpoch: 83200, loss: 0.00153\nEpoch: 83300, loss: 0.00154\nLoss decreased (0.001455 --> 0.001451).  Saving model ...\nEpoch: 83400, loss: 0.00163\nEpoch: 83500, loss: 0.00152\nEpoch: 83600, loss: 0.00177\nLoss decreased (0.001451 --> 0.001450).  Saving model ...\nEpoch: 83700, loss: 0.00193\nEpoch: 83800, loss: 0.00153\nEpoch: 83900, loss: 0.00161\nEpoch: 84000, loss: 0.00169\nEpoch: 84100, loss: 0.00161\nLoss decreased (0.001450 --> 0.001448).  Saving model ...\nEpoch: 84200, loss: 0.00179\nEpoch: 84300, loss: 0.00172\nEpoch: 84400, loss: 0.00169\nEpoch: 84500, loss: 0.00163\nEpoch: 84600, loss: 0.00145\nLoss decreased (0.001448 --> 0.001443).  Saving model ...\nLoss decreased (0.001443 --> 0.001443).  Saving model ...\nEpoch: 84700, loss: 0.00176\nEpoch: 84800, loss: 0.00147\nLoss decreased (0.001443 --> 0.001441).  Saving model ...\nEpoch: 84900, loss: 0.00166\nEpoch: 85000, loss: 0.00178\nEpoch: 85100, loss: 0.00149\nEpoch: 85200, loss: 0.00199\nLoss decreased (0.001441 --> 0.001440).  Saving model ...\nEpoch: 85300, loss: 0.00155\nEpoch: 85400, loss: 0.00151\nLoss decreased (0.001440 --> 0.001435).  Saving model ...\nEpoch: 85500, loss: 0.00147\nEpoch: 85600, loss: 0.00147\nEpoch: 85700, loss: 0.00148\nEpoch: 85800, loss: 0.00153\nLoss decreased (0.001435 --> 0.001429).  Saving model ...\nLoss decreased (0.001429 --> 0.001426).  Saving model ...\nLoss decreased (0.001426 --> 0.001426).  Saving model ...\nLoss decreased (0.001426 --> 0.001425).  Saving model ...\nEpoch: 85900, loss: 0.00152\nEpoch: 86000, loss: 0.00149\nEpoch: 86100, loss: 0.00156\nEpoch: 86200, loss: 0.00148\nLoss decreased (0.001425 --> 0.001425).  Saving model ...\nEpoch: 86300, loss: 0.00157\nLoss decreased (0.001425 --> 0.001423).  Saving model ...\nEpoch: 86400, loss: 0.00168\nEpoch: 86500, loss: 0.00175\nEpoch: 86600, loss: 0.00164\nEpoch: 86700, loss: 0.00162\nLoss decreased (0.001423 --> 0.001422).  Saving model ...\nEpoch: 86800, loss: 0.00160\nEpoch: 86900, loss: 0.00146\nLoss decreased (0.001422 --> 0.001421).  Saving model ...\nEpoch: 87000, loss: 0.00216\nEpoch: 87100, loss: 0.00152\nEpoch: 87200, loss: 0.00158\nEpoch: 87300, loss: 0.00147\nEpoch: 87400, loss: 0.00171\nEpoch: 87500, loss: 0.00169\nEpoch: 87600, loss: 0.00165\nEpoch: 87700, loss: 0.00157\nLoss decreased (0.001421 --> 0.001414).  Saving model ...\nLoss decreased (0.001414 --> 0.001412).  Saving model ...\nEpoch: 87800, loss: 0.00149\nEpoch: 87900, loss: 0.00156\nEpoch: 88000, loss: 0.00284\nEpoch: 88100, loss: 0.00159\nEpoch: 88200, loss: 0.00152\nLoss decreased (0.001412 --> 0.001411).  Saving model ...\nEpoch: 88300, loss: 0.00149\nEpoch: 88400, loss: 0.00150\nLoss decreased (0.001411 --> 0.001410).  Saving model ...\nEpoch: 88500, loss: 0.00152\nLoss decreased (0.001410 --> 0.001403).  Saving model ...\nEpoch: 88600, loss: 0.00156\nEpoch: 88700, loss: 0.00146\nEpoch: 88800, loss: 0.00166\nEpoch: 88900, loss: 0.00159\nEpoch: 89000, loss: 0.00159\nLoss decreased (0.001403 --> 0.001401).  Saving model ...\nEpoch: 89100, loss: 0.00148\nEpoch: 89200, loss: 0.00155\nEpoch: 89300, loss: 0.00153\nEpoch: 89400, loss: 0.00163\nEpoch: 89500, loss: 0.00144\nEpoch: 89600, loss: 0.00157\nEpoch: 89700, loss: 0.00150\nEpoch: 89800, loss: 0.00160\nEpoch: 89900, loss: 0.00151\nEpoch: 90000, loss: 0.00169\nLoss decreased (0.001401 --> 0.001401).  Saving model ...\nEpoch: 90100, loss: 0.00162\nLoss decreased (0.001401 --> 0.001394).  Saving model ...\nEpoch: 90200, loss: 0.00145\nEpoch: 90300, loss: 0.00145\nEpoch: 90400, loss: 0.00153\nLoss decreased (0.001394 --> 0.001393).  Saving model ...\nEpoch: 90500, loss: 0.00162\nEpoch: 90600, loss: 0.00148\nEpoch: 90700, loss: 0.00144\nEpoch: 90800, loss: 0.00179\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 90900, loss: 0.00157\nEpoch: 91000, loss: 0.00145\nEpoch: 91100, loss: 0.00197\nEpoch: 91200, loss: 0.00156\nLoss decreased (0.001393 --> 0.001393).  Saving model ...\nLoss decreased (0.001393 --> 0.001392).  Saving model ...\nEpoch: 91300, loss: 0.00140\nEpoch: 91400, loss: 0.00180\nEpoch: 91500, loss: 0.00177\nEpoch: 91600, loss: 0.00145\nLoss decreased (0.001392 --> 0.001391).  Saving model ...\nEpoch: 91700, loss: 0.00149\nLoss decreased (0.001391 --> 0.001388).  Saving model ...\nEpoch: 91800, loss: 0.00150\nLoss decreased (0.001388 --> 0.001386).  Saving model ...\nEpoch: 91900, loss: 0.00161\nLoss decreased (0.001386 --> 0.001386).  Saving model ...\nEpoch: 92000, loss: 0.00167\nEpoch: 92100, loss: 0.00143\nLoss decreased (0.001386 --> 0.001382).  Saving model ...\nEpoch: 92200, loss: 0.00177\nEpoch: 92300, loss: 0.00142\nEpoch: 92400, loss: 0.00170\nEpoch: 92500, loss: 0.00154\nEpoch: 92600, loss: 0.00148\nEpoch: 92700, loss: 0.00155\nLoss decreased (0.001382 --> 0.001375).  Saving model ...\nLoss decreased (0.001375 --> 0.001371).  Saving model ...\nEpoch: 92800, loss: 0.00143\nEpoch: 92900, loss: 0.00150\nLoss decreased (0.001371 --> 0.001371).  Saving model ...\nEpoch: 93000, loss: 0.00178\nEpoch: 93100, loss: 0.00145\nEpoch: 93200, loss: 0.00168\nEpoch: 93300, loss: 0.00149\nEpoch: 93400, loss: 0.00159\nEpoch: 93500, loss: 0.00149\nLoss decreased (0.001371 --> 0.001369).  Saving model ...\nEpoch: 93600, loss: 0.00156\nEpoch: 93700, loss: 0.00157\nEpoch: 93800, loss: 0.00147\nLoss decreased (0.001369 --> 0.001365).  Saving model ...\nEpoch: 93900, loss: 0.00173\nEpoch: 94000, loss: 0.00150\nEpoch: 94100, loss: 0.00149\nEpoch: 94200, loss: 0.00143\nEpoch: 94300, loss: 0.00162\nEpoch: 94400, loss: 0.00181\nEpoch: 94500, loss: 0.00148\nEpoch: 94600, loss: 0.00159\nEpoch: 94700, loss: 0.00155\nEpoch: 94800, loss: 0.00169\nEpoch: 94900, loss: 0.00198\nEpoch: 95000, loss: 0.00184\nEpoch: 95100, loss: 0.00173\nLoss decreased (0.001365 --> 0.001364).  Saving model ...\nLoss decreased (0.001364 --> 0.001361).  Saving model ...\nLoss decreased (0.001361 --> 0.001360).  Saving model ...\nEpoch: 95200, loss: 0.00150\nEpoch: 95300, loss: 0.00153\nLoss decreased (0.001360 --> 0.001358).  Saving model ...\nEpoch: 95400, loss: 0.00138\nEpoch: 95500, loss: 0.00141\nLoss decreased (0.001358 --> 0.001357).  Saving model ...\nEpoch: 95600, loss: 0.00154\nEpoch: 95700, loss: 0.00140\nEpoch: 95800, loss: 0.00149\nLoss decreased (0.001357 --> 0.001350).  Saving model ...\nEpoch: 95900, loss: 0.00140\nEpoch: 96000, loss: 0.00165\nEpoch: 96100, loss: 0.00160\nEpoch: 96200, loss: 0.00155\nEpoch: 96300, loss: 0.00141\nEpoch: 96400, loss: 0.00152\nEpoch: 96500, loss: 0.00158\nLoss decreased (0.001350 --> 0.001346).  Saving model ...\nEpoch: 96600, loss: 0.00156\nEpoch: 96700, loss: 0.00140\nLoss decreased (0.001346 --> 0.001343).  Saving model ...\nEpoch: 96800, loss: 0.00159\nEpoch: 96900, loss: 0.00150\nLoss decreased (0.001343 --> 0.001343).  Saving model ...\nLoss decreased (0.001343 --> 0.001338).  Saving model ...\nEpoch: 97000, loss: 0.00156\nEpoch: 97100, loss: 0.00159\nEpoch: 97200, loss: 0.00150\nEpoch: 97300, loss: 0.00159\nEpoch: 97400, loss: 0.00138\nEpoch: 97500, loss: 0.00136\nEpoch: 97600, loss: 0.00171\nLoss decreased (0.001338 --> 0.001337).  Saving model ...\nEpoch: 97700, loss: 0.00157\nEpoch: 97800, loss: 0.00138\nEpoch: 97900, loss: 0.00171\nEpoch: 98000, loss: 0.00152\nEpoch: 98100, loss: 0.00144\nEpoch: 98200, loss: 0.00147\nEpoch: 98300, loss: 0.00140\nLoss decreased (0.001337 --> 0.001336).  Saving model ...\nEpoch: 98400, loss: 0.00142\nEpoch: 98500, loss: 0.00176\nEpoch: 98600, loss: 0.00143\nEpoch: 98700, loss: 0.00153\nLoss decreased (0.001336 --> 0.001333).  Saving model ...\nLoss decreased (0.001333 --> 0.001332).  Saving model ...\nEpoch: 98800, loss: 0.00139\nEpoch: 98900, loss: 0.00146\nEpoch: 99000, loss: 0.00149\nEpoch: 99100, loss: 0.00135\nLoss decreased (0.001332 --> 0.001330).  Saving model ...\nEpoch: 99200, loss: 0.00150\nEpoch: 99300, loss: 0.00152\nEpoch: 99400, loss: 0.00153\nLoss decreased (0.001330 --> 0.001330).  Saving model ...\nEpoch: 99500, loss: 0.00163\nEpoch: 99600, loss: 0.00137\nEpoch: 99700, loss: 0.00164\nLoss decreased (0.001330 --> 0.001327).  Saving model ...\nEpoch: 99800, loss: 0.00145\nLoss decreased (0.001327 --> 0.001327).  Saving model ...\nEpoch: 99900, loss: 0.00160\nLoss decreased (0.001327 --> 0.001323).  Saving model ...\nEpoch: 100000, loss: 0.00352\nEpoch: 100100, loss: 0.00160\nLoss decreased (0.001323 --> 0.001323).  Saving model ...\nLoss decreased (0.001323 --> 0.001320).  Saving model ...\nEpoch: 100200, loss: 0.00153\nEpoch: 100300, loss: 0.00163\nEpoch: 100400, loss: 0.00150\nEpoch: 100500, loss: 0.00174\nEpoch: 100600, loss: 0.00141\nEpoch: 100700, loss: 0.00145\nEpoch: 100800, loss: 0.00169\nEpoch: 100900, loss: 0.00148\nEpoch: 101000, loss: 0.00163\nEpoch: 101100, loss: 0.00190\nLoss decreased (0.001320 --> 0.001314).  Saving model ...\nEpoch: 101200, loss: 0.00147\nEpoch: 101300, loss: 0.00146\nEpoch: 101400, loss: 0.00140\nEpoch: 101500, loss: 0.00155\nEpoch: 101600, loss: 0.00149\nLoss decreased (0.001314 --> 0.001313).  Saving model ...\nEpoch: 101700, loss: 0.00136\nEpoch: 101800, loss: 0.00177\nLoss decreased (0.001313 --> 0.001312).  Saving model ...\nEpoch: 101900, loss: 0.00140\nEpoch: 102000, loss: 0.00157\nEpoch: 102100, loss: 0.00132\nEpoch: 102200, loss: 0.00135\nLoss decreased (0.001312 --> 0.001307).  Saving model ...\nLoss decreased (0.001307 --> 0.001306).  Saving model ...\nEpoch: 102300, loss: 0.00154\nEpoch: 102400, loss: 0.00149\nEpoch: 102500, loss: 0.00157\nEpoch: 102600, loss: 0.00150\nEpoch: 102700, loss: 0.00150\nLoss decreased (0.001306 --> 0.001305).  Saving model ...\nLoss decreased (0.001305 --> 0.001304).  Saving model ...\nEpoch: 102800, loss: 0.00137\nEpoch: 102900, loss: 0.00218\nEpoch: 103000, loss: 0.00144\nEpoch: 103100, loss: 0.00136\nEpoch: 103200, loss: 0.00136\nLoss decreased (0.001304 --> 0.001304).  Saving model ...\nEpoch: 103300, loss: 0.00151\nEpoch: 103400, loss: 0.00131\nLoss decreased (0.001304 --> 0.001303).  Saving model ...\nEpoch: 103500, loss: 0.00157\nEpoch: 103600, loss: 0.00141\nEpoch: 103700, loss: 0.00142\nEpoch: 103800, loss: 0.00154\nLoss decreased (0.001303 --> 0.001296).  Saving model ...\nEpoch: 103900, loss: 0.00146\nEpoch: 104000, loss: 0.00168\nEpoch: 104100, loss: 0.00138\nEpoch: 104200, loss: 0.00139\nEpoch: 104300, loss: 0.00139\nEpoch: 104400, loss: 0.00154\nEpoch: 104500, loss: 0.00132\nEpoch: 104600, loss: 0.00134\nLoss decreased (0.001296 --> 0.001291).  Saving model ...\nLoss decreased (0.001291 --> 0.001291).  Saving model ...\nEpoch: 104700, loss: 0.00137\nEpoch: 104800, loss: 0.00158\nEpoch: 104900, loss: 0.00154\nEpoch: 105000, loss: 0.00135\nEpoch: 105100, loss: 0.00151\nEpoch: 105200, loss: 0.00145\nEpoch: 105300, loss: 0.00139\nEpoch: 105400, loss: 0.00173\nLoss decreased (0.001291 --> 0.001289).  Saving model ...\nEpoch: 105500, loss: 0.00169\nEpoch: 105600, loss: 0.00145\nLoss decreased (0.001289 --> 0.001287).  Saving model ...\nEpoch: 105700, loss: 0.00144\nEpoch: 105800, loss: 0.00134\nEpoch: 105900, loss: 0.00169\nEpoch: 106000, loss: 0.00155\nEpoch: 106100, loss: 0.00131\nLoss decreased (0.001287 --> 0.001286).  Saving model ...\nEpoch: 106200, loss: 0.00145\nEpoch: 106300, loss: 0.00141\nEpoch: 106400, loss: 0.00142\nEpoch: 106500, loss: 0.00135\nEpoch: 106600, loss: 0.00144\nEpoch: 106700, loss: 0.00149\nEpoch: 106800, loss: 0.00154\nLoss decreased (0.001286 --> 0.001285).  Saving model ...\nEpoch: 106900, loss: 0.00159\nEpoch: 107000, loss: 0.00149\nEpoch: 107100, loss: 0.00136\nEpoch: 107200, loss: 0.00153\nEpoch: 107300, loss: 0.00136\nEpoch: 107400, loss: 0.00150\nLoss decreased (0.001285 --> 0.001284).  Saving model ...\nEpoch: 107500, loss: 0.00133\nEpoch: 107600, loss: 0.00164\nEpoch: 107700, loss: 0.00160\nLoss decreased (0.001284 --> 0.001281).  Saving model ...\nEpoch: 107800, loss: 0.00165\nEpoch: 107900, loss: 0.00143\nEpoch: 108000, loss: 0.00220\nEpoch: 108100, loss: 0.00166\nLoss decreased (0.001281 --> 0.001279).  Saving model ...\nEpoch: 108200, loss: 0.00154\nEpoch: 108300, loss: 0.00147\nEpoch: 108400, loss: 0.00145\nEpoch: 108500, loss: 0.00154\nLoss decreased (0.001279 --> 0.001278).  Saving model ...\nLoss decreased (0.001278 --> 0.001275).  Saving model ...\nEpoch: 108600, loss: 0.00140\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 108700, loss: 0.00131\nEpoch: 108800, loss: 0.00143\nEpoch: 108900, loss: 0.00154\nLoss decreased (0.001275 --> 0.001272).  Saving model ...\nEpoch: 109000, loss: 0.00128\nEpoch: 109100, loss: 0.00137\nLoss decreased (0.001272 --> 0.001272).  Saving model ...\nLoss decreased (0.001272 --> 0.001270).  Saving model ...\nEpoch: 109200, loss: 0.00147\nEpoch: 109300, loss: 0.00145\nEpoch: 109400, loss: 0.00164\nLoss decreased (0.001270 --> 0.001269).  Saving model ...\nEpoch: 109500, loss: 0.00130\nLoss decreased (0.001269 --> 0.001267).  Saving model ...\nEpoch: 109600, loss: 0.00166\nEpoch: 109700, loss: 0.00151\nEpoch: 109800, loss: 0.00145\nEpoch: 109900, loss: 0.00132\nEpoch: 110000, loss: 0.00248\nEpoch: 110100, loss: 0.00155\nEpoch: 110200, loss: 0.00150\nEpoch: 110300, loss: 0.00141\nEpoch: 110400, loss: 0.00151\nEpoch: 110500, loss: 0.00127\nEpoch: 110600, loss: 0.00134\nEpoch: 110700, loss: 0.00140\nLoss decreased (0.001267 --> 0.001264).  Saving model ...\nEpoch: 110800, loss: 0.00132\nEpoch: 110900, loss: 0.00131\nLoss decreased (0.001264 --> 0.001263).  Saving model ...\nLoss decreased (0.001263 --> 0.001260).  Saving model ...\nLoss decreased (0.001260 --> 0.001258).  Saving model ...\nEpoch: 111000, loss: 0.00158\nEpoch: 111100, loss: 0.00143\nLoss decreased (0.001258 --> 0.001258).  Saving model ...\nEpoch: 111200, loss: 0.00135\nEpoch: 111300, loss: 0.00138\nEpoch: 111400, loss: 0.00144\nEpoch: 111500, loss: 0.00140\nLoss decreased (0.001258 --> 0.001257).  Saving model ...\nEpoch: 111600, loss: 0.00153\nEpoch: 111700, loss: 0.00142\nEpoch: 111800, loss: 0.00161\nEpoch: 111900, loss: 0.00134\nLoss decreased (0.001257 --> 0.001255).  Saving model ...\nEpoch: 112000, loss: 0.00164\nLoss decreased (0.001255 --> 0.001253).  Saving model ...\nEpoch: 112100, loss: 0.00157\nLoss decreased (0.001253 --> 0.001252).  Saving model ...\nEpoch: 112200, loss: 0.00131\nEpoch: 112300, loss: 0.00133\nEpoch: 112400, loss: 0.00130\nEpoch: 112500, loss: 0.00137\nEpoch: 112600, loss: 0.00140\nEpoch: 112700, loss: 0.00136\nEpoch: 112800, loss: 0.00131\nLoss decreased (0.001252 --> 0.001251).  Saving model ...\nEpoch: 112900, loss: 0.00137\nEpoch: 113000, loss: 0.00134\nEpoch: 113100, loss: 0.00127\nEpoch: 113200, loss: 0.00155\nEpoch: 113300, loss: 0.00132\nEpoch: 113400, loss: 0.00126\nEpoch: 113500, loss: 0.00144\nEpoch: 113600, loss: 0.00131\nEpoch: 113700, loss: 0.00149\nEpoch: 113800, loss: 0.00133\nEpoch: 113900, loss: 0.00146\nLoss decreased (0.001251 --> 0.001247).  Saving model ...\nLoss decreased (0.001247 --> 0.001243).  Saving model ...\nLoss decreased (0.001243 --> 0.001243).  Saving model ...\nEpoch: 114000, loss: 0.00141\nEpoch: 114100, loss: 0.00144\nEpoch: 114200, loss: 0.00133\nEpoch: 114300, loss: 0.00148\nEpoch: 114400, loss: 0.00128\nEpoch: 114500, loss: 0.00129\nEpoch: 114600, loss: 0.00127\nEpoch: 114700, loss: 0.00150\nEpoch: 114800, loss: 0.00148\nEpoch: 114900, loss: 0.00147\nEpoch: 115000, loss: 0.00171\nEpoch: 115100, loss: 0.00141\nLoss decreased (0.001243 --> 0.001241).  Saving model ...\nEpoch: 115200, loss: 0.00152\nEpoch: 115300, loss: 0.00146\nEpoch: 115400, loss: 0.00145\nEpoch: 115500, loss: 0.00142\nEpoch: 115600, loss: 0.00145\nEpoch: 115700, loss: 0.00151\nEpoch: 115800, loss: 0.00129\nLoss decreased (0.001241 --> 0.001238).  Saving model ...\nLoss decreased (0.001238 --> 0.001234).  Saving model ...\nEpoch: 115900, loss: 0.00133\nEpoch: 116000, loss: 0.00163\nEpoch: 116100, loss: 0.00129\nEpoch: 116200, loss: 0.00142\nEpoch: 116300, loss: 0.00134\nEpoch: 116400, loss: 0.00137\nLoss decreased (0.001234 --> 0.001232).  Saving model ...\nEpoch: 116500, loss: 0.00134\nEpoch: 116600, loss: 0.00138\nEpoch: 116700, loss: 0.00130\nLoss decreased (0.001232 --> 0.001231).  Saving model ...\nEpoch: 116800, loss: 0.00131\nEpoch: 116900, loss: 0.00148\nEpoch: 117000, loss: 0.00146\nEpoch: 117100, loss: 0.00147\nEpoch: 117200, loss: 0.00137\nEpoch: 117300, loss: 0.00161\nEpoch: 117400, loss: 0.00134\nEpoch: 117500, loss: 0.00141\nEpoch: 117600, loss: 0.00142\nEpoch: 117700, loss: 0.00168\nLoss decreased (0.001231 --> 0.001229).  Saving model ...\nEpoch: 117800, loss: 0.00142\nEpoch: 117900, loss: 0.00149\nLoss decreased (0.001229 --> 0.001227).  Saving model ...\nLoss decreased (0.001227 --> 0.001226).  Saving model ...\nEpoch: 118000, loss: 0.00123\nEpoch: 118100, loss: 0.00132\nLoss decreased (0.001226 --> 0.001223).  Saving model ...\nEpoch: 118200, loss: 0.00140\nEpoch: 118300, loss: 0.00127\nEpoch: 118400, loss: 0.00298\nEpoch: 118500, loss: 0.00141\nEpoch: 118600, loss: 0.00129\nEpoch: 118700, loss: 0.00151\nEpoch: 118800, loss: 0.00140\nEpoch: 118900, loss: 0.00148\nEpoch: 119000, loss: 0.00156\nEpoch: 119100, loss: 0.00161\nEpoch: 119200, loss: 0.00216\nEpoch: 119300, loss: 0.00133\nEpoch: 119400, loss: 0.00134\nEpoch: 119500, loss: 0.00132\nEpoch: 119600, loss: 0.00127\nEpoch: 119700, loss: 0.00151\nLoss decreased (0.001223 --> 0.001216).  Saving model ...\nEpoch: 119800, loss: 0.00129\nEpoch: 119900, loss: 0.00145\nEpoch: 120000, loss: 0.00129\nEpoch: 120100, loss: 0.00140\nEpoch: 120200, loss: 0.00148\nEpoch: 120300, loss: 0.00124\nEpoch: 120400, loss: 0.00162\nEpoch: 120500, loss: 0.00142\nEpoch: 120600, loss: 0.00129\nLoss decreased (0.001216 --> 0.001215).  Saving model ...\nEpoch: 120700, loss: 0.00122\nEpoch: 120800, loss: 0.00146\nLoss decreased (0.001215 --> 0.001214).  Saving model ...\nEpoch: 120900, loss: 0.00127\nEpoch: 121000, loss: 0.00122\nLoss decreased (0.001214 --> 0.001212).  Saving model ...\nEpoch: 121100, loss: 0.00154\nEpoch: 121200, loss: 0.00135\nEpoch: 121300, loss: 0.00147\nEpoch: 121400, loss: 0.00164\nLoss decreased (0.001212 --> 0.001210).  Saving model ...\nEpoch: 121500, loss: 0.00147\nEpoch: 121600, loss: 0.00122\nLoss decreased (0.001210 --> 0.001209).  Saving model ...\nEpoch: 121700, loss: 0.00136\nEpoch: 121800, loss: 0.00128\nEpoch: 121900, loss: 0.00134\nEpoch: 122000, loss: 0.00138\nEpoch: 122100, loss: 0.00148\nEpoch: 122200, loss: 0.00152\nEpoch: 122300, loss: 0.00122\nEpoch: 122400, loss: 0.00164\nEpoch: 122500, loss: 0.00123\nEpoch: 122600, loss: 0.00140\nEpoch: 122700, loss: 0.00149\nEpoch: 122800, loss: 0.00136\nEpoch: 122900, loss: 0.00122\nLoss decreased (0.001209 --> 0.001208).  Saving model ...\nEpoch: 123000, loss: 0.00137\nEpoch: 123100, loss: 0.00123\nLoss decreased (0.001208 --> 0.001204).  Saving model ...\nLoss decreased (0.001204 --> 0.001202).  Saving model ...\nEpoch: 123200, loss: 0.00223\nEpoch: 123300, loss: 0.00155\nEpoch: 123400, loss: 0.00131\nLoss decreased (0.001202 --> 0.001202).  Saving model ...\nEpoch: 123500, loss: 0.00143\nEpoch: 123600, loss: 0.00131\nLoss decreased (0.001202 --> 0.001196).  Saving model ...\nEpoch: 123700, loss: 0.00136\nEpoch: 123800, loss: 0.00121\nEpoch: 123900, loss: 0.00154\nEpoch: 124000, loss: 0.00149\nEpoch: 124100, loss: 0.00139\nEpoch: 124200, loss: 0.00132\nEpoch: 124300, loss: 0.00124\nEpoch: 124400, loss: 0.00149\nEpoch: 124500, loss: 0.00124\nEpoch: 124600, loss: 0.00140\nEpoch: 124700, loss: 0.00147\nEpoch: 124800, loss: 0.00142\nEpoch: 124900, loss: 0.00145\nLoss decreased (0.001196 --> 0.001189).  Saving model ...\nEpoch: 125000, loss: 0.00133\nEpoch: 125100, loss: 0.00178\nEpoch: 125200, loss: 0.00153\nEpoch: 125300, loss: 0.00136\nEpoch: 125400, loss: 0.00129\nEpoch: 125500, loss: 0.00159\nEpoch: 125600, loss: 0.00139\nEpoch: 125700, loss: 0.00163\nEpoch: 125800, loss: 0.00128\nEpoch: 125900, loss: 0.00137\nEpoch: 126000, loss: 0.00153\nEpoch: 126100, loss: 0.00130\nEpoch: 126200, loss: 0.00134\nEpoch: 126300, loss: 0.00144\nEpoch: 126400, loss: 0.00129\nEpoch: 126500, loss: 0.00148\nLoss decreased (0.001189 --> 0.001188).  Saving model ...\nEpoch: 126600, loss: 0.00123\nLoss decreased (0.001188 --> 0.001186).  Saving model ...\nEpoch: 126700, loss: 0.00160\nEpoch: 126800, loss: 0.00129\nEpoch: 126900, loss: 0.00120\nLoss decreased (0.001186 --> 0.001185).  Saving model ...\nEpoch: 127000, loss: 0.00136\nLoss decreased (0.001185 --> 0.001183).  Saving model ...\nEpoch: 127100, loss: 0.00135\nEpoch: 127200, loss: 0.00128\nEpoch: 127300, loss: 0.00145\nEpoch: 127400, loss: 0.00124\nEpoch: 127500, loss: 0.00119\nEpoch: 127600, loss: 0.00120\nEpoch: 127700, loss: 0.00126\nEpoch: 127800, loss: 0.00125\nEpoch: 127900, loss: 0.00121\nEpoch: 128000, loss: 0.00142\nLoss decreased (0.001183 --> 0.001182).  Saving model ...\nEpoch: 128100, loss: 0.00128\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 128200, loss: 0.00137\nEpoch: 128300, loss: 0.00133\nEpoch: 128400, loss: 0.00169\nEpoch: 128500, loss: 0.00148\nEpoch: 128600, loss: 0.00129\nEpoch: 128700, loss: 0.00123\nLoss decreased (0.001182 --> 0.001181).  Saving model ...\nEpoch: 128800, loss: 0.00151\nLoss decreased (0.001181 --> 0.001176).  Saving model ...\nLoss decreased (0.001176 --> 0.001175).  Saving model ...\nEpoch: 128900, loss: 0.00133\nEpoch: 129000, loss: 0.00132\nEpoch: 129100, loss: 0.00141\nEpoch: 129200, loss: 0.00132\nEpoch: 129300, loss: 0.00126\nEpoch: 129400, loss: 0.00140\nEpoch: 129500, loss: 0.00132\nLoss decreased (0.001175 --> 0.001174).  Saving model ...\nEpoch: 129600, loss: 0.00120\nEpoch: 129700, loss: 0.00169\nEpoch: 129800, loss: 0.00137\nEpoch: 129900, loss: 0.00136\nEpoch: 130000, loss: 0.00134\nEpoch: 130100, loss: 0.00153\nEpoch: 130200, loss: 0.00143\nLoss decreased (0.001174 --> 0.001171).  Saving model ...\nEpoch: 130300, loss: 0.00149\nLoss decreased (0.001171 --> 0.001171).  Saving model ...\nLoss decreased (0.001171 --> 0.001168).  Saving model ...\nEpoch: 130400, loss: 0.00125\nEpoch: 130500, loss: 0.00130\nEpoch: 130600, loss: 0.00139\nEpoch: 130700, loss: 0.00126\nEpoch: 130800, loss: 0.00137\nEpoch: 130900, loss: 0.00125\nEpoch: 131000, loss: 0.00124\nEpoch: 131100, loss: 0.00138\nEpoch: 131200, loss: 0.00118\nLoss decreased (0.001168 --> 0.001168).  Saving model ...\nEpoch: 131300, loss: 0.00130\nEpoch: 131400, loss: 0.00129\nEpoch: 131500, loss: 0.00140\nEpoch: 131600, loss: 0.00124\nLoss decreased (0.001168 --> 0.001166).  Saving model ...\nLoss decreased (0.001166 --> 0.001165).  Saving model ...\nEpoch: 131700, loss: 0.00117\nEpoch: 131800, loss: 0.00208\nEpoch: 131900, loss: 0.00136\nEpoch: 132000, loss: 0.00123\nEpoch: 132100, loss: 0.00138\nLoss decreased (0.001165 --> 0.001164).  Saving model ...\nEpoch: 132200, loss: 0.00124\nEpoch: 132300, loss: 0.00122\nEpoch: 132400, loss: 0.00131\nEpoch: 132500, loss: 0.00124\nEpoch: 132600, loss: 0.00121\nLoss decreased (0.001164 --> 0.001162).  Saving model ...\nLoss decreased (0.001162 --> 0.001160).  Saving model ...\nEpoch: 132700, loss: 0.00145\nEpoch: 132800, loss: 0.00124\nEpoch: 132900, loss: 0.00139\nEpoch: 133000, loss: 0.00127\nEpoch: 133100, loss: 0.00157\nEpoch: 133200, loss: 0.00139\nEpoch: 133300, loss: 0.00138\nEpoch: 133400, loss: 0.00125\nEpoch: 133500, loss: 0.00144\nLoss decreased (0.001160 --> 0.001158).  Saving model ...\nEpoch: 133600, loss: 0.00116\nLoss decreased (0.001158 --> 0.001155).  Saving model ...\nEpoch: 133700, loss: 0.00128\nEpoch: 133800, loss: 0.00131\nEpoch: 133900, loss: 0.00135\nEpoch: 134000, loss: 0.00119\nEpoch: 134100, loss: 0.00118\nEpoch: 134200, loss: 0.00121\nEpoch: 134300, loss: 0.00127\nEpoch: 134400, loss: 0.00132\nEpoch: 134500, loss: 0.00128\nEpoch: 134600, loss: 0.00147\nEpoch: 134700, loss: 0.00122\nLoss decreased (0.001155 --> 0.001150).  Saving model ...\nEpoch: 134800, loss: 0.00129\nEpoch: 134900, loss: 0.00137\nEpoch: 135000, loss: 0.00128\nEpoch: 135100, loss: 0.00135\nEpoch: 135200, loss: 0.00150\nEpoch: 135300, loss: 0.00125\nEpoch: 135400, loss: 0.00132\nEpoch: 135500, loss: 0.00129\nEpoch: 135600, loss: 0.00135\nLoss decreased (0.001150 --> 0.001148).  Saving model ...\nEpoch: 135700, loss: 0.00133\nEpoch: 135800, loss: 0.00118\nEpoch: 135900, loss: 0.00130\nEpoch: 136000, loss: 0.00146\nEpoch: 136100, loss: 0.00130\nEpoch: 136200, loss: 0.00124\nLoss decreased (0.001148 --> 0.001148).  Saving model ...\nEpoch: 136300, loss: 0.00120\nEpoch: 136400, loss: 0.00115\nEpoch: 136500, loss: 0.00147\nLoss decreased (0.001148 --> 0.001145).  Saving model ...\nEpoch: 136600, loss: 0.00142\nEpoch: 136700, loss: 0.00130\nEpoch: 136800, loss: 0.00139\nEpoch: 136900, loss: 0.00130\nLoss decreased (0.001145 --> 0.001145).  Saving model ...\nEpoch: 137000, loss: 0.00140\nEpoch: 137100, loss: 0.00126\nEpoch: 137200, loss: 0.00123\nEpoch: 137300, loss: 0.00136\nEpoch: 137400, loss: 0.00126\nEpoch: 137500, loss: 0.00132\nEpoch: 137600, loss: 0.00131\nEpoch: 137700, loss: 0.00132\nEpoch: 137800, loss: 0.00130\nEpoch: 137900, loss: 0.00129\nEpoch: 138000, loss: 0.00136\nEpoch: 138100, loss: 0.00116\nLoss decreased (0.001145 --> 0.001142).  Saving model ...\nEpoch: 138200, loss: 0.00140\nEpoch: 138300, loss: 0.00118\nEpoch: 138400, loss: 0.00120\nEpoch: 138500, loss: 0.00126\nEpoch: 138600, loss: 0.00141\nEpoch: 138700, loss: 0.00131\nEpoch: 138800, loss: 0.00139\nLoss decreased (0.001142 --> 0.001140).  Saving model ...\nLoss decreased (0.001140 --> 0.001140).  Saving model ...\nLoss decreased (0.001140 --> 0.001139).  Saving model ...\nEpoch: 138900, loss: 0.00125\nEpoch: 139000, loss: 0.00148\nLoss decreased (0.001139 --> 0.001139).  Saving model ...\nEpoch: 139100, loss: 0.00162\nLoss decreased (0.001139 --> 0.001138).  Saving model ...\nEpoch: 139200, loss: 0.00123\nEpoch: 139300, loss: 0.00144\nEpoch: 139400, loss: 0.00118\nLoss decreased (0.001138 --> 0.001135).  Saving model ...\nEpoch: 139500, loss: 0.00124\nEpoch: 139600, loss: 0.00136\nLoss decreased (0.001135 --> 0.001134).  Saving model ...\nEpoch: 139700, loss: 0.00127\nEpoch: 139800, loss: 0.00132\nLoss decreased (0.001134 --> 0.001132).  Saving model ...\nEpoch: 139900, loss: 0.00142\nEpoch: 140000, loss: 0.00139\nEpoch: 140100, loss: 0.00130\nLoss decreased (0.001132 --> 0.001131).  Saving model ...\nEpoch: 140200, loss: 0.00129\nEpoch: 140300, loss: 0.00150\nEpoch: 140400, loss: 0.00135\nLoss decreased (0.001131 --> 0.001130).  Saving model ...\nEpoch: 140500, loss: 0.00122\nEpoch: 140600, loss: 0.00118\nEpoch: 140700, loss: 0.00119\nEpoch: 140800, loss: 0.00125\nEpoch: 140900, loss: 0.00128\nLoss decreased (0.001130 --> 0.001130).  Saving model ...\nEpoch: 141000, loss: 0.00123\nLoss decreased (0.001130 --> 0.001128).  Saving model ...\nEpoch: 141100, loss: 0.00128\nEpoch: 141200, loss: 0.00128\nEpoch: 141300, loss: 0.00118\nLoss decreased (0.001128 --> 0.001127).  Saving model ...\nEpoch: 141400, loss: 0.00113\nEpoch: 141500, loss: 0.00152\nEpoch: 141600, loss: 0.00135\nEpoch: 141700, loss: 0.00127\nEpoch: 141800, loss: 0.00139\nEpoch: 141900, loss: 0.00152\nEpoch: 142000, loss: 0.00116\nLoss decreased (0.001127 --> 0.001126).  Saving model ...\nEpoch: 142100, loss: 0.00127\nEpoch: 142200, loss: 0.00125\nLoss decreased (0.001126 --> 0.001125).  Saving model ...\nEpoch: 142300, loss: 0.00144\nEpoch: 142400, loss: 0.00151\nEpoch: 142500, loss: 0.00128\nEpoch: 142600, loss: 0.00118\nLoss decreased (0.001125 --> 0.001125).  Saving model ...\nEpoch: 142700, loss: 0.00128\nLoss decreased (0.001125 --> 0.001123).  Saving model ...\nEpoch: 142800, loss: 0.00117\nEpoch: 142900, loss: 0.00129\nEpoch: 143000, loss: 0.00125\nEpoch: 143100, loss: 0.00153\nEpoch: 143200, loss: 0.00121\nEpoch: 143300, loss: 0.00125\nEpoch: 143400, loss: 0.00141\nEpoch: 143500, loss: 0.00120\nEpoch: 143600, loss: 0.00155\nLoss decreased (0.001123 --> 0.001120).  Saving model ...\nLoss decreased (0.001120 --> 0.001119).  Saving model ...\nEpoch: 143700, loss: 0.00132\nEpoch: 143800, loss: 0.00126\nEpoch: 143900, loss: 0.00124\nEpoch: 144000, loss: 0.00118\nEpoch: 144100, loss: 0.00133\nEpoch: 144200, loss: 0.00139\nEpoch: 144300, loss: 0.00117\nEpoch: 144400, loss: 0.00121\nEpoch: 144500, loss: 0.00126\nEpoch: 144600, loss: 0.00118\nEpoch: 144700, loss: 0.00156\nEpoch: 144800, loss: 0.00128\nLoss decreased (0.001119 --> 0.001115).  Saving model ...\nEpoch: 144900, loss: 0.00126\nEpoch: 145000, loss: 0.00147\nEpoch: 145100, loss: 0.00128\nLoss decreased (0.001115 --> 0.001113).  Saving model ...\nEpoch: 145200, loss: 0.00135\nEpoch: 145300, loss: 0.00158\nEpoch: 145400, loss: 0.00142\nEpoch: 145500, loss: 0.00128\nEpoch: 145600, loss: 0.00123\nEpoch: 145700, loss: 0.00142\nEpoch: 145800, loss: 0.00138\nLoss decreased (0.001113 --> 0.001108).  Saving model ...\nEpoch: 145900, loss: 0.00144\nEpoch: 146000, loss: 0.00148\nEpoch: 146100, loss: 0.00115\nEpoch: 146200, loss: 0.00130\nEpoch: 146300, loss: 0.00125\nLoss decreased (0.001108 --> 0.001108).  Saving model ...\nEpoch: 146400, loss: 0.00120\nEpoch: 146500, loss: 0.00128\nEpoch: 146600, loss: 0.00120\nEpoch: 146700, loss: 0.00125\nLoss decreased (0.001108 --> 0.001106).  Saving model ...\nEpoch: 146800, loss: 0.00116\nEpoch: 146900, loss: 0.00129\nEpoch: 147000, loss: 0.00150\nEpoch: 147100, loss: 0.00119\nEpoch: 147200, loss: 0.00147\nEpoch: 147300, loss: 0.00154\nEpoch: 147400, loss: 0.00142\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 147500, loss: 0.00141\nLoss decreased (0.001106 --> 0.001105).  Saving model ...\nLoss decreased (0.001105 --> 0.001103).  Saving model ...\nLoss decreased (0.001103 --> 0.001099).  Saving model ...\nEpoch: 147600, loss: 0.00133\nEpoch: 147700, loss: 0.00147\nEpoch: 147800, loss: 0.00112\nEpoch: 147900, loss: 0.00149\nEpoch: 148000, loss: 0.00133\nEpoch: 148100, loss: 0.00114\nEpoch: 148200, loss: 0.00127\nEpoch: 148300, loss: 0.00135\nEpoch: 148400, loss: 0.00118\nEpoch: 148500, loss: 0.00133\nEpoch: 148600, loss: 0.00133\nEpoch: 148700, loss: 0.00135\nEpoch: 148800, loss: 0.00120\nEpoch: 148900, loss: 0.00144\nEpoch: 149000, loss: 0.00143\nLoss decreased (0.001099 --> 0.001095).  Saving model ...\nEpoch: 149100, loss: 0.00140\nEpoch: 149200, loss: 0.00139\nEpoch: 149300, loss: 0.00133\nEpoch: 149400, loss: 0.00120\nEpoch: 149500, loss: 0.00137\nEpoch: 149600, loss: 0.00111\nEpoch: 149700, loss: 0.00112\nEpoch: 149800, loss: 0.00119\nLoss decreased (0.001095 --> 0.001094).  Saving model ...\nEpoch: 149900, loss: 0.00126\nEpoch: 150000, loss: 0.00119\nEpoch: 150100, loss: 0.00116\nLoss decreased (0.001094 --> 0.001093).  Saving model ...\nEpoch: 150200, loss: 0.00123\nEpoch: 150300, loss: 0.00116\nEpoch: 150400, loss: 0.00137\nEpoch: 150500, loss: 0.00118\nEpoch: 150600, loss: 0.00142\nEpoch: 150700, loss: 0.00126\nEpoch: 150800, loss: 0.00128\nEpoch: 150900, loss: 0.00124\nEpoch: 151000, loss: 0.00125\nLoss decreased (0.001093 --> 0.001092).  Saving model ...\nEpoch: 151100, loss: 0.00129\nLoss decreased (0.001092 --> 0.001091).  Saving model ...\nEpoch: 151200, loss: 0.00125\nEpoch: 151300, loss: 0.00121\nEpoch: 151400, loss: 0.00113\nEpoch: 151500, loss: 0.00129\nEpoch: 151600, loss: 0.00123\nEpoch: 151700, loss: 0.00147\nEpoch: 151800, loss: 0.00116\nEpoch: 151900, loss: 0.00124\nLoss decreased (0.001091 --> 0.001090).  Saving model ...\nEpoch: 152000, loss: 0.00112\nLoss decreased (0.001090 --> 0.001085).  Saving model ...\nEpoch: 152100, loss: 0.00131\nEpoch: 152200, loss: 0.00112\nEpoch: 152300, loss: 0.00109\nEpoch: 152400, loss: 0.00139\nEpoch: 152500, loss: 0.00132\nEpoch: 152600, loss: 0.00110\nEpoch: 152700, loss: 0.00136\nLoss decreased (0.001085 --> 0.001084).  Saving model ...\nEpoch: 152800, loss: 0.00118\nEpoch: 152900, loss: 0.00114\nEpoch: 153000, loss: 0.00116\nEpoch: 153100, loss: 0.00128\nEpoch: 153200, loss: 0.00121\nEpoch: 153300, loss: 0.00129\nEpoch: 153400, loss: 0.00122\nLoss decreased (0.001084 --> 0.001083).  Saving model ...\nLoss decreased (0.001083 --> 0.001080).  Saving model ...\nEpoch: 153500, loss: 0.00118\nEpoch: 153600, loss: 0.00122\nEpoch: 153700, loss: 0.00116\nEpoch: 153800, loss: 0.00113\nEpoch: 153900, loss: 0.00114\nEpoch: 154000, loss: 0.00117\nEpoch: 154100, loss: 0.00143\nEpoch: 154200, loss: 0.00115\nEpoch: 154300, loss: 0.00125\nEpoch: 154400, loss: 0.00125\nEpoch: 154500, loss: 0.00116\nLoss decreased (0.001080 --> 0.001077).  Saving model ...\nEpoch: 154600, loss: 0.00127\nEpoch: 154700, loss: 0.00139\nLoss decreased (0.001077 --> 0.001076).  Saving model ...\nEpoch: 154800, loss: 0.00120\nEpoch: 154900, loss: 0.00123\nEpoch: 155000, loss: 0.00124\nEpoch: 155100, loss: 0.00109\nEpoch: 155200, loss: 0.00125\nEpoch: 155300, loss: 0.00143\nEpoch: 155400, loss: 0.00154\nEpoch: 155500, loss: 0.00175\nEpoch: 155600, loss: 0.00121\nEpoch: 155700, loss: 0.00114\nEpoch: 155800, loss: 0.00118\nLoss decreased (0.001076 --> 0.001073).  Saving model ...\nEpoch: 155900, loss: 0.00114\nEpoch: 156000, loss: 0.00114\nEpoch: 156100, loss: 0.00125\nEpoch: 156200, loss: 0.00126\nEpoch: 156300, loss: 0.00168\nEpoch: 156400, loss: 0.00148\nEpoch: 156500, loss: 0.00127\nEpoch: 156600, loss: 0.00112\nEpoch: 156700, loss: 0.00125\nEpoch: 156800, loss: 0.00113\nEpoch: 156900, loss: 0.00141\nLoss decreased (0.001073 --> 0.001067).  Saving model ...\nEpoch: 157000, loss: 0.00140\nEpoch: 157100, loss: 0.00115\nEpoch: 157200, loss: 0.00125\nEpoch: 157300, loss: 0.00119\nEpoch: 157400, loss: 0.00124\nEpoch: 157500, loss: 0.00129\nEpoch: 157600, loss: 0.00137\nLoss decreased (0.001067 --> 0.001067).  Saving model ...\nEpoch: 157700, loss: 0.00121\nEpoch: 157800, loss: 0.00141\nEpoch: 157900, loss: 0.00156\nEpoch: 158000, loss: 0.00122\nEpoch: 158100, loss: 0.00114\nEpoch: 158200, loss: 0.00145\nEpoch: 158300, loss: 0.00114\nEpoch: 158400, loss: 0.00113\nEpoch: 158500, loss: 0.00135\nEpoch: 158600, loss: 0.00177\nEpoch: 158700, loss: 0.00129\nLoss decreased (0.001067 --> 0.001065).  Saving model ...\nEpoch: 158800, loss: 0.00127\nEpoch: 158900, loss: 0.00109\nEpoch: 159000, loss: 0.00112\nLoss decreased (0.001065 --> 0.001063).  Saving model ...\nEpoch: 159100, loss: 0.00130\nEpoch: 159200, loss: 0.00118\nEpoch: 159300, loss: 0.00119\nLoss decreased (0.001063 --> 0.001061).  Saving model ...\nEpoch: 159400, loss: 0.00111\nEpoch: 159500, loss: 0.00116\nEpoch: 159600, loss: 0.00121\nEpoch: 159700, loss: 0.00134\nEpoch: 159800, loss: 0.00115\nEpoch: 159900, loss: 0.00128\nEpoch: 160000, loss: 0.00123\nEpoch: 160100, loss: 0.00128\nEpoch: 160200, loss: 0.00125\nEpoch: 160300, loss: 0.00112\nLoss decreased (0.001061 --> 0.001059).  Saving model ...\nLoss decreased (0.001059 --> 0.001058).  Saving model ...\nEpoch: 160400, loss: 0.00115\nEpoch: 160500, loss: 0.00128\nEpoch: 160600, loss: 0.00109\nEpoch: 160700, loss: 0.00123\nEpoch: 160800, loss: 0.00116\nLoss decreased (0.001058 --> 0.001056).  Saving model ...\nEpoch: 160900, loss: 0.00126\nEpoch: 161000, loss: 0.00131\nEpoch: 161100, loss: 0.00110\nEpoch: 161200, loss: 0.00117\nLoss decreased (0.001056 --> 0.001055).  Saving model ...\nEpoch: 161300, loss: 0.00148\nLoss decreased (0.001055 --> 0.001052).  Saving model ...\nEpoch: 161400, loss: 0.00164\nEpoch: 161500, loss: 0.00112\nEpoch: 161600, loss: 0.00117\nEpoch: 161700, loss: 0.00121\nEpoch: 161800, loss: 0.00111\nEpoch: 161900, loss: 0.00142\nEpoch: 162000, loss: 0.00129\nEpoch: 162100, loss: 0.00137\nEpoch: 162200, loss: 0.00112\nEpoch: 162300, loss: 0.00120\nEpoch: 162400, loss: 0.00110\nEpoch: 162500, loss: 0.00117\nEpoch: 162600, loss: 0.00127\nEpoch: 162700, loss: 0.00117\nEpoch: 162800, loss: 0.00148\nEpoch: 162900, loss: 0.00148\nEpoch: 163000, loss: 0.00112\nEpoch: 163100, loss: 0.00111\nLoss decreased (0.001052 --> 0.001051).  Saving model ...\nLoss decreased (0.001051 --> 0.001050).  Saving model ...\nEpoch: 163200, loss: 0.00111\nEpoch: 163300, loss: 0.00136\nEpoch: 163400, loss: 0.00130\nLoss decreased (0.001050 --> 0.001050).  Saving model ...\nEpoch: 163500, loss: 0.00137\nEpoch: 163600, loss: 0.00125\nLoss decreased (0.001050 --> 0.001048).  Saving model ...\nEpoch: 163700, loss: 0.00118\nEpoch: 163800, loss: 0.00156\nEpoch: 163900, loss: 0.00121\nEpoch: 164000, loss: 0.00126\nEpoch: 164100, loss: 0.00125\nLoss decreased (0.001048 --> 0.001045).  Saving model ...\nEpoch: 164200, loss: 0.00126\nEpoch: 164300, loss: 0.00111\nLoss decreased (0.001045 --> 0.001044).  Saving model ...\nEpoch: 164400, loss: 0.00114\nEpoch: 164500, loss: 0.00110\nEpoch: 164600, loss: 0.00127\nEpoch: 164700, loss: 0.00132\nEpoch: 164800, loss: 0.00120\nEpoch: 164900, loss: 0.00118\nEpoch: 165000, loss: 0.00110\nEpoch: 165100, loss: 0.00143\nEpoch: 165200, loss: 0.00133\nEpoch: 165300, loss: 0.00126\nLoss decreased (0.001044 --> 0.001043).  Saving model ...\nEpoch: 165400, loss: 0.00140\nEpoch: 165500, loss: 0.00127\nEpoch: 165600, loss: 0.00128\nLoss decreased (0.001043 --> 0.001043).  Saving model ...\nLoss decreased (0.001043 --> 0.001040).  Saving model ...\nLoss decreased (0.001040 --> 0.001039).  Saving model ...\nEpoch: 165700, loss: 0.00134\nLoss decreased (0.001039 --> 0.001037).  Saving model ...\nEpoch: 165800, loss: 0.00154\nEpoch: 165900, loss: 0.00130\nEpoch: 166000, loss: 0.00139\nEpoch: 166100, loss: 0.00106\nEpoch: 166200, loss: 0.00120\nEpoch: 166300, loss: 0.00125\nEpoch: 166400, loss: 0.00118\nEpoch: 166500, loss: 0.00129\nEpoch: 166600, loss: 0.00141\nEpoch: 166700, loss: 0.00134\nEpoch: 166800, loss: 0.00126\nEpoch: 166900, loss: 0.00109\nEpoch: 167000, loss: 0.00110\nEpoch: 167100, loss: 0.00145\nEpoch: 167200, loss: 0.00108\nEpoch: 167300, loss: 0.00136\nEpoch: 167400, loss: 0.00129\nEpoch: 167500, loss: 0.00129\nEpoch: 167600, loss: 0.00138\nEpoch: 167700, loss: 0.00124\nEpoch: 167800, loss: 0.00121\nEpoch: 167900, loss: 0.00120\nLoss decreased (0.001037 --> 0.001037).  Saving model ...\nLoss decreased (0.001037 --> 0.001033).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.001033 --> 0.001033).  Saving model ...\nEpoch: 168000, loss: 0.00125\nEpoch: 168100, loss: 0.00105\nEpoch: 168200, loss: 0.00121\nEpoch: 168300, loss: 0.00107\nEpoch: 168400, loss: 0.00122\nEpoch: 168500, loss: 0.00116\nLoss decreased (0.001033 --> 0.001033).  Saving model ...\nEpoch: 168600, loss: 0.00141\nEpoch: 168700, loss: 0.00114\nEpoch: 168800, loss: 0.00134\nEpoch: 168900, loss: 0.00126\nEpoch: 169000, loss: 0.00118\nLoss decreased (0.001033 --> 0.001033).  Saving model ...\nEpoch: 169100, loss: 0.00103\nLoss decreased (0.001033 --> 0.001032).  Saving model ...\nEpoch: 169200, loss: 0.00113\nEpoch: 169300, loss: 0.00119\nEpoch: 169400, loss: 0.00114\nEpoch: 169500, loss: 0.00120\nEpoch: 169600, loss: 0.00104\nEpoch: 169700, loss: 0.00134\nEpoch: 169800, loss: 0.00116\nLoss decreased (0.001032 --> 0.001029).  Saving model ...\nEpoch: 169900, loss: 0.00118\nEpoch: 170000, loss: 0.00124\nLoss decreased (0.001029 --> 0.001025).  Saving model ...\nEpoch: 170100, loss: 0.00138\nEpoch: 170200, loss: 0.00111\nEpoch: 170300, loss: 0.00141\nEpoch: 170400, loss: 0.00131\nEpoch: 170500, loss: 0.00125\nEpoch: 170600, loss: 0.00140\nEpoch: 170700, loss: 0.00112\nEpoch: 170800, loss: 0.00125\nEpoch: 170900, loss: 0.00128\nLoss decreased (0.001025 --> 0.001024).  Saving model ...\nEpoch: 171000, loss: 0.00116\nLoss decreased (0.001024 --> 0.001022).  Saving model ...\nEpoch: 171100, loss: 0.00122\nLoss decreased (0.001022 --> 0.001021).  Saving model ...\nEpoch: 171200, loss: 0.00116\nEpoch: 171300, loss: 0.00112\nEpoch: 171400, loss: 0.00111\nEpoch: 171500, loss: 0.00118\nEpoch: 171600, loss: 0.00133\nEpoch: 171700, loss: 0.00130\nEpoch: 171800, loss: 0.00115\nEpoch: 171900, loss: 0.00102\nEpoch: 172000, loss: 0.00124\nEpoch: 172100, loss: 0.00121\nEpoch: 172200, loss: 0.00111\nEpoch: 172300, loss: 0.00104\nEpoch: 172400, loss: 0.00118\nEpoch: 172500, loss: 0.00142\nEpoch: 172600, loss: 0.00133\nEpoch: 172700, loss: 0.00104\nEpoch: 172800, loss: 0.00104\nEpoch: 172900, loss: 0.00106\nLoss decreased (0.001021 --> 0.001017).  Saving model ...\nEpoch: 173000, loss: 0.00138\nEpoch: 173100, loss: 0.00105\nEpoch: 173200, loss: 0.00127\nEpoch: 173300, loss: 0.00144\nLoss decreased (0.001017 --> 0.001016).  Saving model ...\nEpoch: 173400, loss: 0.00116\nEpoch: 173500, loss: 0.00122\nEpoch: 173600, loss: 0.00118\nEpoch: 173700, loss: 0.00120\nEpoch: 173800, loss: 0.00107\nEpoch: 173900, loss: 0.00127\nEpoch: 174000, loss: 0.00104\nEpoch: 174100, loss: 0.00110\nEpoch: 174200, loss: 0.00114\nEpoch: 174300, loss: 0.00120\nLoss decreased (0.001016 --> 0.001014).  Saving model ...\nEpoch: 174400, loss: 0.00119\nEpoch: 174500, loss: 0.00105\nEpoch: 174600, loss: 0.00119\nEpoch: 174700, loss: 0.00138\nEpoch: 174800, loss: 0.00116\nEpoch: 174900, loss: 0.00119\nEpoch: 175000, loss: 0.00103\nEpoch: 175100, loss: 0.00107\nLoss decreased (0.001014 --> 0.001009).  Saving model ...\nLoss decreased (0.001009 --> 0.001008).  Saving model ...\nEpoch: 175200, loss: 0.00135\nEpoch: 175300, loss: 0.00110\nEpoch: 175400, loss: 0.00128\nEpoch: 175500, loss: 0.00114\nEpoch: 175600, loss: 0.00146\nEpoch: 175700, loss: 0.00106\nEpoch: 175800, loss: 0.00118\nLoss decreased (0.001008 --> 0.001005).  Saving model ...\nEpoch: 175900, loss: 0.00108\nEpoch: 176000, loss: 0.00117\nEpoch: 176100, loss: 0.00117\nEpoch: 176200, loss: 0.00132\nEpoch: 176300, loss: 0.00124\nLoss decreased (0.001005 --> 0.001004).  Saving model ...\nLoss decreased (0.001004 --> 0.001004).  Saving model ...\nLoss decreased (0.001004 --> 0.001004).  Saving model ...\nEpoch: 176400, loss: 0.00114\nEpoch: 176500, loss: 0.00111\nEpoch: 176600, loss: 0.00108\nEpoch: 176700, loss: 0.00110\nEpoch: 176800, loss: 0.00104\nLoss decreased (0.001004 --> 0.001004).  Saving model ...\nEpoch: 176900, loss: 0.00111\nEpoch: 177000, loss: 0.00111\nEpoch: 177100, loss: 0.00111\nEpoch: 177200, loss: 0.00124\nLoss decreased (0.001004 --> 0.001000).  Saving model ...\nLoss decreased (0.001000 --> 0.000999).  Saving model ...\nEpoch: 177300, loss: 0.00145\nEpoch: 177400, loss: 0.00107\nLoss decreased (0.000999 --> 0.000997).  Saving model ...\nEpoch: 177500, loss: 0.00123\nEpoch: 177600, loss: 0.00135\nEpoch: 177700, loss: 0.00118\nEpoch: 177800, loss: 0.00108\nEpoch: 177900, loss: 0.00111\nEpoch: 178000, loss: 0.00102\nEpoch: 178100, loss: 0.00108\nEpoch: 178200, loss: 0.00116\nEpoch: 178300, loss: 0.00117\nEpoch: 178400, loss: 0.00134\nEpoch: 178500, loss: 0.00113\nEpoch: 178600, loss: 0.00112\nLoss decreased (0.000997 --> 0.000997).  Saving model ...\nEpoch: 178700, loss: 0.00116\nEpoch: 178800, loss: 0.00147\nEpoch: 178900, loss: 0.00131\nEpoch: 179000, loss: 0.00109\nEpoch: 179100, loss: 0.00128\nEpoch: 179200, loss: 0.00102\nEpoch: 179300, loss: 0.00111\nEpoch: 179400, loss: 0.00104\nEpoch: 179500, loss: 0.00101\nEpoch: 179600, loss: 0.00108\nEpoch: 179700, loss: 0.00130\nLoss decreased (0.000997 --> 0.000994).  Saving model ...\nEpoch: 179800, loss: 0.00119\nEpoch: 179900, loss: 0.00114\nEpoch: 180000, loss: 0.00158\nEpoch: 180100, loss: 0.00114\nEpoch: 180200, loss: 0.00123\nEpoch: 180300, loss: 0.00117\nEpoch: 180400, loss: 0.00132\nEpoch: 180500, loss: 0.00180\nEpoch: 180600, loss: 0.00106\nEpoch: 180700, loss: 0.00118\nLoss decreased (0.000994 --> 0.000989).  Saving model ...\nEpoch: 180800, loss: 0.00115\nEpoch: 180900, loss: 0.00108\nEpoch: 181000, loss: 0.00105\nEpoch: 181100, loss: 0.00102\nEpoch: 181200, loss: 0.00127\nEpoch: 181300, loss: 0.00137\nEpoch: 181400, loss: 0.00124\nEpoch: 181500, loss: 0.00114\nLoss decreased (0.000989 --> 0.000988).  Saving model ...\nEpoch: 181600, loss: 0.00102\nEpoch: 181700, loss: 0.00112\nEpoch: 181800, loss: 0.00124\nEpoch: 181900, loss: 0.00115\nEpoch: 182000, loss: 0.00118\nEpoch: 182100, loss: 0.00131\nEpoch: 182200, loss: 0.00105\nEpoch: 182300, loss: 0.00122\nLoss decreased (0.000988 --> 0.000985).  Saving model ...\nEpoch: 182400, loss: 0.00143\nEpoch: 182500, loss: 0.00102\nEpoch: 182600, loss: 0.00132\nLoss decreased (0.000985 --> 0.000983).  Saving model ...\nEpoch: 182700, loss: 0.00104\nEpoch: 182800, loss: 0.00103\nEpoch: 182900, loss: 0.00122\nEpoch: 183000, loss: 0.00159\nEpoch: 183100, loss: 0.00135\nEpoch: 183200, loss: 0.00110\nEpoch: 183300, loss: 0.00114\nEpoch: 183400, loss: 0.00146\nEpoch: 183500, loss: 0.00099\nLoss decreased (0.000983 --> 0.000983).  Saving model ...\nEpoch: 183600, loss: 0.00109\nEpoch: 183700, loss: 0.00132\nEpoch: 183800, loss: 0.00107\nEpoch: 183900, loss: 0.00128\nEpoch: 184000, loss: 0.00144\nEpoch: 184100, loss: 0.00103\nEpoch: 184200, loss: 0.00126\nEpoch: 184300, loss: 0.00104\nEpoch: 184400, loss: 0.00107\nEpoch: 184500, loss: 0.00108\nEpoch: 184600, loss: 0.00128\nEpoch: 184700, loss: 0.00126\nLoss decreased (0.000983 --> 0.000982).  Saving model ...\nLoss decreased (0.000982 --> 0.000978).  Saving model ...\nEpoch: 184800, loss: 0.00134\nEpoch: 184900, loss: 0.00111\nEpoch: 185000, loss: 0.00134\nLoss decreased (0.000978 --> 0.000978).  Saving model ...\nEpoch: 185100, loss: 0.00120\nLoss decreased (0.000978 --> 0.000976).  Saving model ...\nEpoch: 185200, loss: 0.00127\nEpoch: 185300, loss: 0.00117\nEpoch: 185400, loss: 0.00114\nEpoch: 185500, loss: 0.00134\nEpoch: 185600, loss: 0.00108\nEpoch: 185700, loss: 0.00106\nEpoch: 185800, loss: 0.00112\nEpoch: 185900, loss: 0.00121\nEpoch: 186000, loss: 0.00131\nEpoch: 186100, loss: 0.00124\nEpoch: 186200, loss: 0.00110\nLoss decreased (0.000976 --> 0.000976).  Saving model ...\nEpoch: 186300, loss: 0.00098\nEpoch: 186400, loss: 0.00102\nEpoch: 186500, loss: 0.00108\nEpoch: 186600, loss: 0.00120\nLoss decreased (0.000976 --> 0.000973).  Saving model ...\nEpoch: 186700, loss: 0.00117\nEpoch: 186800, loss: 0.00128\nEpoch: 186900, loss: 0.00117\nEpoch: 187000, loss: 0.00118\nLoss decreased (0.000973 --> 0.000971).  Saving model ...\nEpoch: 187100, loss: 0.00130\nEpoch: 187200, loss: 0.00146\nEpoch: 187300, loss: 0.00104\nEpoch: 187400, loss: 0.00113\nEpoch: 187500, loss: 0.00101\nEpoch: 187600, loss: 0.00102\nLoss decreased (0.000971 --> 0.000971).  Saving model ...\nEpoch: 187700, loss: 0.00098\nLoss decreased (0.000971 --> 0.000968).  Saving model ...\nLoss decreased (0.000968 --> 0.000968).  Saving model ...\nEpoch: 187800, loss: 0.00112\nEpoch: 187900, loss: 0.00103\nEpoch: 188000, loss: 0.00122\nEpoch: 188100, loss: 0.00123\nEpoch: 188200, loss: 0.00103\nEpoch: 188300, loss: 0.00098\nEpoch: 188400, loss: 0.00108\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 188500, loss: 0.00138\nEpoch: 188600, loss: 0.00124\nEpoch: 188700, loss: 0.00108\nEpoch: 188800, loss: 0.00106\nEpoch: 188900, loss: 0.00125\nEpoch: 189000, loss: 0.00146\nEpoch: 189100, loss: 0.00136\nEpoch: 189200, loss: 0.00117\nEpoch: 189300, loss: 0.00132\nEpoch: 189400, loss: 0.00110\nEpoch: 189500, loss: 0.00138\nEpoch: 189600, loss: 0.00122\nEpoch: 189700, loss: 0.00115\nEpoch: 189800, loss: 0.00104\nEpoch: 189900, loss: 0.00098\nLoss decreased (0.000968 --> 0.000963).  Saving model ...\nEpoch: 190000, loss: 0.00111\nEpoch: 190100, loss: 0.00109\nEpoch: 190200, loss: 0.00113\nEpoch: 190300, loss: 0.00104\nEpoch: 190400, loss: 0.00112\nEpoch: 190500, loss: 0.00108\nEpoch: 190600, loss: 0.00118\nEpoch: 190700, loss: 0.00118\nEpoch: 190800, loss: 0.00100\nEpoch: 190900, loss: 0.00139\nEpoch: 191000, loss: 0.00125\nLoss decreased (0.000963 --> 0.000961).  Saving model ...\nEpoch: 191100, loss: 0.00120\nEpoch: 191200, loss: 0.00119\nEpoch: 191300, loss: 0.00110\nEpoch: 191400, loss: 0.00113\nEpoch: 191500, loss: 0.00117\nEpoch: 191600, loss: 0.00100\nEpoch: 191700, loss: 0.00107\nEpoch: 191800, loss: 0.00119\nEpoch: 191900, loss: 0.00110\nEpoch: 192000, loss: 0.00117\nEpoch: 192100, loss: 0.00098\nEpoch: 192200, loss: 0.00117\nEpoch: 192300, loss: 0.00101\nLoss decreased (0.000961 --> 0.000961).  Saving model ...\nLoss decreased (0.000961 --> 0.000956).  Saving model ...\nEpoch: 192400, loss: 0.00140\nEpoch: 192500, loss: 0.00110\nEpoch: 192600, loss: 0.00110\nEpoch: 192700, loss: 0.00116\nEpoch: 192800, loss: 0.00116\nEpoch: 192900, loss: 0.00163\nEpoch: 193000, loss: 0.00099\nEpoch: 193100, loss: 0.00096\nEpoch: 193200, loss: 0.00133\nEpoch: 193300, loss: 0.00118\nEpoch: 193400, loss: 0.00098\nEpoch: 193500, loss: 0.00133\nEpoch: 193600, loss: 0.00099\nEpoch: 193700, loss: 0.00166\nEpoch: 193800, loss: 0.00113\nEpoch: 193900, loss: 0.00108\nEpoch: 194000, loss: 0.00112\nEpoch: 194100, loss: 0.00101\nEpoch: 194200, loss: 0.00113\nEpoch: 194300, loss: 0.00121\nLoss decreased (0.000956 --> 0.000953).  Saving model ...\nEpoch: 194400, loss: 0.00124\nEpoch: 194500, loss: 0.00102\nEpoch: 194600, loss: 0.00132\nEpoch: 194700, loss: 0.00113\nEpoch: 194800, loss: 0.00106\nEpoch: 194900, loss: 0.00110\nLoss decreased (0.000953 --> 0.000953).  Saving model ...\nEpoch: 195000, loss: 0.00109\nEpoch: 195100, loss: 0.00107\nEpoch: 195200, loss: 0.00120\nEpoch: 195300, loss: 0.00138\nEpoch: 195400, loss: 0.00109\nEpoch: 195500, loss: 0.00100\nEpoch: 195600, loss: 0.00099\nEpoch: 195700, loss: 0.00109\nLoss decreased (0.000953 --> 0.000950).  Saving model ...\nEpoch: 195800, loss: 0.00255\nEpoch: 195900, loss: 0.00115\nEpoch: 196000, loss: 0.00099\nEpoch: 196100, loss: 0.00132\nEpoch: 196200, loss: 0.00107\nEpoch: 196300, loss: 0.00120\nEpoch: 196400, loss: 0.00115\nEpoch: 196500, loss: 0.00105\nEpoch: 196600, loss: 0.00100\nLoss decreased (0.000950 --> 0.000948).  Saving model ...\nEpoch: 196700, loss: 0.00114\nEpoch: 196800, loss: 0.00119\nEpoch: 196900, loss: 0.00133\nLoss decreased (0.000948 --> 0.000947).  Saving model ...\nEpoch: 197000, loss: 0.00119\nEpoch: 197100, loss: 0.00126\nEpoch: 197200, loss: 0.00104\nEpoch: 197300, loss: 0.00098\nEpoch: 197400, loss: 0.00107\nEpoch: 197500, loss: 0.00101\nEpoch: 197600, loss: 0.00123\nEpoch: 197700, loss: 0.00109\nEpoch: 197800, loss: 0.00124\nEpoch: 197900, loss: 0.00111\nEpoch: 198000, loss: 0.00122\nEpoch: 198100, loss: 0.00104\nEpoch: 198200, loss: 0.00127\nEpoch: 198300, loss: 0.00108\nEpoch: 198400, loss: 0.00101\nEpoch: 198500, loss: 0.00103\nEpoch: 198600, loss: 0.00101\nLoss decreased (0.000947 --> 0.000941).  Saving model ...\nEpoch: 198700, loss: 0.00105\nEpoch: 198800, loss: 0.00113\nEpoch: 198900, loss: 0.00118\nEpoch: 199000, loss: 0.00110\nEpoch: 199100, loss: 0.00115\nEpoch: 199200, loss: 0.00114\nEpoch: 199300, loss: 0.00118\nEpoch: 199400, loss: 0.00101\nEpoch: 199500, loss: 0.00103\nEpoch: 199600, loss: 0.00136\nEpoch: 199700, loss: 0.00112\nEpoch: 199800, loss: 0.00105\nEpoch: 199900, loss: 0.00099\nLoss decreased (0.000941 --> 0.000939).  Saving model ...\nEpoch: 200000, loss: 0.00134\nEpoch: 200100, loss: 0.00100\nLoss decreased (0.000939 --> 0.000937).  Saving model ...\nLoss decreased (0.000937 --> 0.000936).  Saving model ...\nEpoch: 200200, loss: 0.00126\nEpoch: 200300, loss: 0.00112\nEpoch: 200400, loss: 0.00107\nEpoch: 200500, loss: 0.00108\nEpoch: 200600, loss: 0.00126\nEpoch: 200700, loss: 0.00103\nEpoch: 200800, loss: 0.00108\nEpoch: 200900, loss: 0.00105\nLoss decreased (0.000936 --> 0.000933).  Saving model ...\nLoss decreased (0.000933 --> 0.000933).  Saving model ...\nEpoch: 201000, loss: 0.00115\nEpoch: 201100, loss: 0.00126\nEpoch: 201200, loss: 0.00108\nEpoch: 201300, loss: 0.00119\nEpoch: 201400, loss: 0.00106\nEpoch: 201500, loss: 0.00096\nEpoch: 201600, loss: 0.00103\nEpoch: 201700, loss: 0.00109\nEpoch: 201800, loss: 0.00095\nEpoch: 201900, loss: 0.00123\nEpoch: 202000, loss: 0.00103\nEpoch: 202100, loss: 0.00104\nEpoch: 202200, loss: 0.00122\nEpoch: 202300, loss: 0.00114\nEpoch: 202400, loss: 0.00104\nEpoch: 202500, loss: 0.00101\nLoss decreased (0.000933 --> 0.000931).  Saving model ...\nEpoch: 202600, loss: 0.00101\nLoss decreased (0.000931 --> 0.000930).  Saving model ...\nEpoch: 202700, loss: 0.00101\nEpoch: 202800, loss: 0.00100\nEpoch: 202900, loss: 0.00100\nEpoch: 203000, loss: 0.00117\nEpoch: 203100, loss: 0.00107\nEpoch: 203200, loss: 0.00099\nEpoch: 203300, loss: 0.00146\nEpoch: 203400, loss: 0.00113\nEpoch: 203500, loss: 0.00124\nEpoch: 203600, loss: 0.00099\nEpoch: 203700, loss: 0.00117\nLoss decreased (0.000930 --> 0.000929).  Saving model ...\nEpoch: 203800, loss: 0.00102\nEpoch: 203900, loss: 0.00096\nEpoch: 204000, loss: 0.00131\nEpoch: 204100, loss: 0.00107\nEpoch: 204200, loss: 0.00096\nEpoch: 204300, loss: 0.00117\nEpoch: 204400, loss: 0.00108\nEpoch: 204500, loss: 0.00110\nLoss decreased (0.000929 --> 0.000929).  Saving model ...\nLoss decreased (0.000929 --> 0.000928).  Saving model ...\nEpoch: 204600, loss: 0.00134\nLoss decreased (0.000928 --> 0.000928).  Saving model ...\nLoss decreased (0.000928 --> 0.000926).  Saving model ...\nEpoch: 204700, loss: 0.00104\nEpoch: 204800, loss: 0.00115\nEpoch: 204900, loss: 0.00119\nLoss decreased (0.000926 --> 0.000924).  Saving model ...\nEpoch: 205000, loss: 0.00106\nEpoch: 205100, loss: 0.00100\nEpoch: 205200, loss: 0.00124\nEpoch: 205300, loss: 0.00117\nEpoch: 205400, loss: 0.00151\nEpoch: 205500, loss: 0.00105\nEpoch: 205600, loss: 0.00115\nEpoch: 205700, loss: 0.00105\nEpoch: 205800, loss: 0.00122\nEpoch: 205900, loss: 0.00096\nLoss decreased (0.000924 --> 0.000923).  Saving model ...\nEpoch: 206000, loss: 0.00098\nEpoch: 206100, loss: 0.00101\nEpoch: 206200, loss: 0.00099\nEpoch: 206300, loss: 0.00101\nEpoch: 206400, loss: 0.00099\nEpoch: 206500, loss: 0.00106\nLoss decreased (0.000923 --> 0.000922).  Saving model ...\nEpoch: 206600, loss: 0.00108\nLoss decreased (0.000922 --> 0.000920).  Saving model ...\nEpoch: 206700, loss: 0.00111\nEpoch: 206800, loss: 0.00098\nEpoch: 206900, loss: 0.00127\nEpoch: 207000, loss: 0.00102\nEpoch: 207100, loss: 0.00096\nEpoch: 207200, loss: 0.00103\nEpoch: 207300, loss: 0.00100\nLoss decreased (0.000920 --> 0.000920).  Saving model ...\nEpoch: 207400, loss: 0.00118\nEpoch: 207500, loss: 0.00100\nEpoch: 207600, loss: 0.00112\nLoss decreased (0.000920 --> 0.000916).  Saving model ...\nEpoch: 207700, loss: 0.00095\nEpoch: 207800, loss: 0.00115\nEpoch: 207900, loss: 0.00167\nEpoch: 208000, loss: 0.00108\nEpoch: 208100, loss: 0.00094\nEpoch: 208200, loss: 0.00123\nEpoch: 208300, loss: 0.00120\nEpoch: 208400, loss: 0.00101\nEpoch: 208500, loss: 0.00140\nLoss decreased (0.000916 --> 0.000916).  Saving model ...\nEpoch: 208600, loss: 0.00112\nEpoch: 208700, loss: 0.00128\nLoss decreased (0.000916 --> 0.000916).  Saving model ...\nEpoch: 208800, loss: 0.00105\nEpoch: 208900, loss: 0.00093\nEpoch: 209000, loss: 0.00127\nEpoch: 209100, loss: 0.00108\nEpoch: 209200, loss: 0.00102\nLoss decreased (0.000916 --> 0.000915).  Saving model ...\nEpoch: 209300, loss: 0.00112\nEpoch: 209400, loss: 0.00101\nEpoch: 209500, loss: 0.00096\nLoss decreased (0.000915 --> 0.000913).  Saving model ...\nLoss decreased (0.000913 --> 0.000909).  Saving model ...\nEpoch: 209600, loss: 0.00098\nEpoch: 209700, loss: 0.00096\nEpoch: 209800, loss: 0.00095\nEpoch: 209900, loss: 0.00109\nEpoch: 210000, loss: 0.00093\nEpoch: 210100, loss: 0.00100\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 210200, loss: 0.00143\nEpoch: 210300, loss: 0.00117\nEpoch: 210400, loss: 0.00124\nEpoch: 210500, loss: 0.00113\nEpoch: 210600, loss: 0.00105\nEpoch: 210700, loss: 0.00092\nEpoch: 210800, loss: 0.00100\nEpoch: 210900, loss: 0.00125\nEpoch: 211000, loss: 0.00096\nLoss decreased (0.000909 --> 0.000908).  Saving model ...\nEpoch: 211100, loss: 0.00098\nLoss decreased (0.000908 --> 0.000907).  Saving model ...\nEpoch: 211200, loss: 0.00096\nEpoch: 211300, loss: 0.00098\nEpoch: 211400, loss: 0.00113\nEpoch: 211500, loss: 0.00105\nEpoch: 211600, loss: 0.00127\nEpoch: 211700, loss: 0.00106\nLoss decreased (0.000907 --> 0.000905).  Saving model ...\nEpoch: 211800, loss: 0.00103\nEpoch: 211900, loss: 0.00095\nEpoch: 212000, loss: 0.00107\nEpoch: 212100, loss: 0.00111\nEpoch: 212200, loss: 0.00119\nEpoch: 212300, loss: 0.00134\nLoss decreased (0.000905 --> 0.000905).  Saving model ...\nEpoch: 212400, loss: 0.00110\nEpoch: 212500, loss: 0.00103\nEpoch: 212600, loss: 0.00093\nEpoch: 212700, loss: 0.00102\nEpoch: 212800, loss: 0.00106\nEpoch: 212900, loss: 0.00114\nEpoch: 213000, loss: 0.00109\nEpoch: 213100, loss: 0.00115\nEpoch: 213200, loss: 0.00122\nLoss decreased (0.000905 --> 0.000904).  Saving model ...\nLoss decreased (0.000904 --> 0.000903).  Saving model ...\nEpoch: 213300, loss: 0.00121\nEpoch: 213400, loss: 0.00101\nEpoch: 213500, loss: 0.00131\nEpoch: 213600, loss: 0.00103\nEpoch: 213700, loss: 0.00114\nEpoch: 213800, loss: 0.00156\nEpoch: 213900, loss: 0.00111\nEpoch: 214000, loss: 0.00105\nEpoch: 214100, loss: 0.00129\nEpoch: 214200, loss: 0.00117\nEpoch: 214300, loss: 0.00110\nEpoch: 214400, loss: 0.00109\nEpoch: 214500, loss: 0.00096\nLoss decreased (0.000903 --> 0.000897).  Saving model ...\nEpoch: 214600, loss: 0.00136\nEpoch: 214700, loss: 0.00101\nEpoch: 214800, loss: 0.00119\nEpoch: 214900, loss: 0.00111\nLoss decreased (0.000897 --> 0.000893).  Saving model ...\nEpoch: 215000, loss: 0.00098\nEpoch: 215100, loss: 0.00136\nEpoch: 215200, loss: 0.00100\nEpoch: 215300, loss: 0.00095\nLoss decreased (0.000893 --> 0.000892).  Saving model ...\nEpoch: 215400, loss: 0.00110\nEpoch: 215500, loss: 0.00092\nLoss decreased (0.000892 --> 0.000890).  Saving model ...\nEpoch: 215600, loss: 0.00120\nEpoch: 215700, loss: 0.00122\nEpoch: 215800, loss: 0.00142\nEpoch: 215900, loss: 0.00132\nEpoch: 216000, loss: 0.00120\nEpoch: 216100, loss: 0.00109\nEpoch: 216200, loss: 0.00095\nEpoch: 216300, loss: 0.00095\nEpoch: 216400, loss: 0.00141\nLoss decreased (0.000890 --> 0.000889).  Saving model ...\nEpoch: 216500, loss: 0.00124\nEpoch: 216600, loss: 0.00096\nEpoch: 216700, loss: 0.00101\nEpoch: 216800, loss: 0.00120\nEpoch: 216900, loss: 0.00091\nEpoch: 217000, loss: 0.00090\nEpoch: 217100, loss: 0.00103\nEpoch: 217200, loss: 0.00099\nEpoch: 217300, loss: 0.00109\nEpoch: 217400, loss: 0.00103\nEpoch: 217500, loss: 0.00105\nLoss decreased (0.000889 --> 0.000887).  Saving model ...\nEpoch: 217600, loss: 0.00092\nEpoch: 217700, loss: 0.00098\nEpoch: 217800, loss: 0.00120\nEpoch: 217900, loss: 0.00115\nEpoch: 218000, loss: 0.00090\nEpoch: 218100, loss: 0.00101\nEpoch: 218200, loss: 0.00095\nEpoch: 218300, loss: 0.00091\nLoss decreased (0.000887 --> 0.000884).  Saving model ...\nEpoch: 218400, loss: 0.00094\nEpoch: 218500, loss: 0.00090\nLoss decreased (0.000884 --> 0.000884).  Saving model ...\nEpoch: 218600, loss: 0.00099\nEpoch: 218700, loss: 0.00094\nEpoch: 218800, loss: 0.00120\nEpoch: 218900, loss: 0.00101\nLoss decreased (0.000884 --> 0.000881).  Saving model ...\nEpoch: 219000, loss: 0.00134\nEpoch: 219100, loss: 0.00101\nEpoch: 219200, loss: 0.00100\nEpoch: 219300, loss: 0.00090\nEpoch: 219400, loss: 0.00108\nEpoch: 219500, loss: 0.00306\nEpoch: 219600, loss: 0.00096\nEpoch: 219700, loss: 0.00115\nEpoch: 219800, loss: 0.00091\nEpoch: 219900, loss: 0.00093\nEpoch: 220000, loss: 0.00116\nLoss decreased (0.000881 --> 0.000879).  Saving model ...\nEpoch: 220100, loss: 0.00103\nEpoch: 220200, loss: 0.00101\nEpoch: 220300, loss: 0.00094\nEpoch: 220400, loss: 0.00095\nEpoch: 220500, loss: 0.00093\nEpoch: 220600, loss: 0.00106\nEpoch: 220700, loss: 0.00108\nEpoch: 220800, loss: 0.00118\nEpoch: 220900, loss: 0.00092\nEpoch: 221000, loss: 0.00156\nEpoch: 221100, loss: 0.00094\nLoss decreased (0.000879 --> 0.000874).  Saving model ...\nEpoch: 221200, loss: 0.00112\nEpoch: 221300, loss: 0.00092\nEpoch: 221400, loss: 0.00127\nEpoch: 221500, loss: 0.00093\nEpoch: 221600, loss: 0.00099\nEpoch: 221700, loss: 0.00098\nEpoch: 221800, loss: 0.00124\nEpoch: 221900, loss: 0.00121\nLoss decreased (0.000874 --> 0.000871).  Saving model ...\nEpoch: 222000, loss: 0.00116\nLoss decreased (0.000871 --> 0.000871).  Saving model ...\nEpoch: 222100, loss: 0.00090\nEpoch: 222200, loss: 0.00103\nEpoch: 222300, loss: 0.00094\nEpoch: 222400, loss: 0.00129\nEpoch: 222500, loss: 0.00090\nEpoch: 222600, loss: 0.00099\nEpoch: 222700, loss: 0.00117\nEpoch: 222800, loss: 0.00119\nEpoch: 222900, loss: 0.00129\nEpoch: 223000, loss: 0.00091\nLoss decreased (0.000871 --> 0.000870).  Saving model ...\nEpoch: 223100, loss: 0.00100\nEpoch: 223200, loss: 0.00134\nEpoch: 223300, loss: 0.00111\nEpoch: 223400, loss: 0.00116\nEpoch: 223500, loss: 0.00097\nEpoch: 223600, loss: 0.00110\nLoss decreased (0.000870 --> 0.000867).  Saving model ...\nEpoch: 223700, loss: 0.00092\nEpoch: 223800, loss: 0.00103\nEpoch: 223900, loss: 0.00110\nEpoch: 224000, loss: 0.00103\nEpoch: 224100, loss: 0.00124\nEpoch: 224200, loss: 0.00115\nEpoch: 224300, loss: 0.00114\nLoss decreased (0.000867 --> 0.000866).  Saving model ...\nEpoch: 224400, loss: 0.00101\nEpoch: 224500, loss: 0.00100\nEpoch: 224600, loss: 0.00090\nEpoch: 224700, loss: 0.00095\nEpoch: 224800, loss: 0.00094\nEpoch: 224900, loss: 0.00099\nEpoch: 225000, loss: 0.00103\nEpoch: 225100, loss: 0.00119\nEpoch: 225200, loss: 0.00103\nEpoch: 225300, loss: 0.00118\nLoss decreased (0.000866 --> 0.000864).  Saving model ...\nEpoch: 225400, loss: 0.00093\nEpoch: 225500, loss: 0.00106\nEpoch: 225600, loss: 0.00092\nLoss decreased (0.000864 --> 0.000863).  Saving model ...\nEpoch: 225700, loss: 0.00102\nEpoch: 225800, loss: 0.00115\nEpoch: 225900, loss: 0.00089\nEpoch: 226000, loss: 0.00157\nEpoch: 226100, loss: 0.00122\nEpoch: 226200, loss: 0.00098\nEpoch: 226300, loss: 0.00136\nEpoch: 226400, loss: 0.00103\nEpoch: 226500, loss: 0.00108\nEpoch: 226600, loss: 0.00105\nLoss decreased (0.000863 --> 0.000862).  Saving model ...\nEpoch: 226700, loss: 0.00089\nEpoch: 226800, loss: 0.00101\nEpoch: 226900, loss: 0.00114\nEpoch: 227000, loss: 0.00097\nLoss decreased (0.000862 --> 0.000860).  Saving model ...\nEpoch: 227100, loss: 0.00092\nEpoch: 227200, loss: 0.00106\nEpoch: 227300, loss: 0.00125\nEpoch: 227400, loss: 0.00104\nEpoch: 227500, loss: 0.00093\nEpoch: 227600, loss: 0.00089\nEpoch: 227700, loss: 0.00101\nEpoch: 227800, loss: 0.00095\nEpoch: 227900, loss: 0.00113\nEpoch: 228000, loss: 0.00111\nLoss decreased (0.000860 --> 0.000857).  Saving model ...\nEpoch: 228100, loss: 0.00120\nEpoch: 228200, loss: 0.00118\nEpoch: 228300, loss: 0.00093\nEpoch: 228400, loss: 0.00097\nEpoch: 228500, loss: 0.00106\nEpoch: 228600, loss: 0.00093\nEpoch: 228700, loss: 0.00096\nEpoch: 228800, loss: 0.00089\nEpoch: 228900, loss: 0.00093\nEpoch: 229000, loss: 0.00103\nEpoch: 229100, loss: 0.00111\nEpoch: 229200, loss: 0.00088\nEpoch: 229300, loss: 0.00098\nLoss decreased (0.000857 --> 0.000856).  Saving model ...\nEpoch: 229400, loss: 0.00098\nEpoch: 229500, loss: 0.00101\nEpoch: 229600, loss: 0.00105\nEpoch: 229700, loss: 0.00092\nEpoch: 229800, loss: 0.00110\nEpoch: 229900, loss: 0.00098\nEpoch: 230000, loss: 0.00105\nEpoch: 230100, loss: 0.00098\nEpoch: 230200, loss: 0.00089\nLoss decreased (0.000856 --> 0.000855).  Saving model ...\nLoss decreased (0.000855 --> 0.000853).  Saving model ...\nLoss decreased (0.000853 --> 0.000853).  Saving model ...\nEpoch: 230300, loss: 0.00097\nEpoch: 230400, loss: 0.00124\nEpoch: 230500, loss: 0.00119\nEpoch: 230600, loss: 0.00088\nEpoch: 230700, loss: 0.00101\nEpoch: 230800, loss: 0.00112\nEpoch: 230900, loss: 0.00121\nEpoch: 231000, loss: 0.00103\nEpoch: 231100, loss: 0.00125\nEpoch: 231200, loss: 0.00094\nLoss decreased (0.000853 --> 0.000849).  Saving model ...\nEpoch: 231300, loss: 0.00097\nEpoch: 231400, loss: 0.00093\nEpoch: 231500, loss: 0.00092\nEpoch: 231600, loss: 0.00099\nEpoch: 231700, loss: 0.00103\nEpoch: 231800, loss: 0.00101\nEpoch: 231900, loss: 0.00092\nEpoch: 232000, loss: 0.00110\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 232100, loss: 0.00096\nEpoch: 232200, loss: 0.00098\nEpoch: 232300, loss: 0.00136\nEpoch: 232400, loss: 0.00090\nEpoch: 232500, loss: 0.00107\nEpoch: 232600, loss: 0.00121\nEpoch: 232700, loss: 0.00108\nEpoch: 232800, loss: 0.00108\nEpoch: 232900, loss: 0.00108\nEpoch: 233000, loss: 0.00102\nEpoch: 233100, loss: 0.00087\nEpoch: 233200, loss: 0.00119\nEpoch: 233300, loss: 0.00093\nEpoch: 233400, loss: 0.00105\nEpoch: 233500, loss: 0.00095\nEpoch: 233600, loss: 0.00109\nEpoch: 233700, loss: 0.00115\nEpoch: 233800, loss: 0.00090\nLoss decreased (0.000849 --> 0.000848).  Saving model ...\nLoss decreased (0.000848 --> 0.000845).  Saving model ...\nEpoch: 233900, loss: 0.00126\nEpoch: 234000, loss: 0.00091\nEpoch: 234100, loss: 0.00114\nEpoch: 234200, loss: 0.00087\nEpoch: 234300, loss: 0.00172\nEpoch: 234400, loss: 0.00130\nEpoch: 234500, loss: 0.00089\nEpoch: 234600, loss: 0.00091\nEpoch: 234700, loss: 0.00101\nEpoch: 234800, loss: 0.00126\nEpoch: 234900, loss: 0.00099\nEpoch: 235000, loss: 0.00113\nEpoch: 235100, loss: 0.00089\nEpoch: 235200, loss: 0.00090\nEpoch: 235300, loss: 0.00088\nEpoch: 235400, loss: 0.00093\nEpoch: 235500, loss: 0.00107\nEpoch: 235600, loss: 0.00094\nEpoch: 235700, loss: 0.00096\nEpoch: 235800, loss: 0.00093\nLoss decreased (0.000845 --> 0.000842).  Saving model ...\nEpoch: 235900, loss: 0.00108\nEpoch: 236000, loss: 0.00103\nEpoch: 236100, loss: 0.00093\nLoss decreased (0.000842 --> 0.000840).  Saving model ...\nEpoch: 236200, loss: 0.00097\nEpoch: 236300, loss: 0.00095\nEpoch: 236400, loss: 0.00111\nEpoch: 236500, loss: 0.00098\nEpoch: 236600, loss: 0.00105\nLoss decreased (0.000840 --> 0.000836).  Saving model ...\nEpoch: 236700, loss: 0.00134\nEpoch: 236800, loss: 0.00112\nEpoch: 236900, loss: 0.00259\nEpoch: 237000, loss: 0.00099\nEpoch: 237100, loss: 0.00103\nEpoch: 237200, loss: 0.00093\nEpoch: 237300, loss: 0.00087\nEpoch: 237400, loss: 0.00112\nEpoch: 237500, loss: 0.00102\nLoss decreased (0.000836 --> 0.000836).  Saving model ...\nEpoch: 237600, loss: 0.00095\nEpoch: 237700, loss: 0.00091\nEpoch: 237800, loss: 0.00109\nEpoch: 237900, loss: 0.00129\nEpoch: 238000, loss: 0.00112\nEpoch: 238100, loss: 0.00107\nEpoch: 238200, loss: 0.00090\nLoss decreased (0.000836 --> 0.000833).  Saving model ...\nEpoch: 238300, loss: 0.00088\nEpoch: 238400, loss: 0.00092\nEpoch: 238500, loss: 0.00109\nEpoch: 238600, loss: 0.00091\nEpoch: 238700, loss: 0.00106\nEpoch: 238800, loss: 0.00088\nEpoch: 238900, loss: 0.00096\nEpoch: 239000, loss: 0.00086\nEpoch: 239100, loss: 0.00087\nEpoch: 239200, loss: 0.00096\nEpoch: 239300, loss: 0.00097\nEpoch: 239400, loss: 0.00099\nEpoch: 239500, loss: 0.00091\nEpoch: 239600, loss: 0.00119\nEpoch: 239700, loss: 0.00118\nEpoch: 239800, loss: 0.00090\nEpoch: 239900, loss: 0.00087\nEpoch: 240000, loss: 0.00099\nEpoch: 240100, loss: 0.00092\nEpoch: 240200, loss: 0.00084\nLoss decreased (0.000833 --> 0.000831).  Saving model ...\nEpoch: 240300, loss: 0.00087\nEpoch: 240400, loss: 0.00131\nEpoch: 240500, loss: 0.00115\nEpoch: 240600, loss: 0.00096\nEpoch: 240700, loss: 0.00086\nEpoch: 240800, loss: 0.00090\nEpoch: 240900, loss: 0.00123\nEpoch: 241000, loss: 0.00103\nEpoch: 241100, loss: 0.00100\nEpoch: 241200, loss: 0.00177\nEpoch: 241300, loss: 0.00096\nEpoch: 241400, loss: 0.00106\nEpoch: 241500, loss: 0.00120\nEpoch: 241600, loss: 0.00093\nEpoch: 241700, loss: 0.00097\nEpoch: 241800, loss: 0.00097\nEpoch: 241900, loss: 0.00103\nLoss decreased (0.000831 --> 0.000829).  Saving model ...\nEpoch: 242000, loss: 0.00093\nEpoch: 242100, loss: 0.00087\nEpoch: 242200, loss: 0.00114\nEpoch: 242300, loss: 0.00106\nEpoch: 242400, loss: 0.00087\nEpoch: 242500, loss: 0.00091\nEpoch: 242600, loss: 0.00088\nEpoch: 242700, loss: 0.00089\nEpoch: 242800, loss: 0.00109\nEpoch: 242900, loss: 0.00088\nLoss decreased (0.000829 --> 0.000828).  Saving model ...\nEpoch: 243000, loss: 0.00112\nEpoch: 243100, loss: 0.00124\nEpoch: 243200, loss: 0.00098\nEpoch: 243300, loss: 0.00128\nEpoch: 243400, loss: 0.00097\nEpoch: 243500, loss: 0.00099\nLoss decreased (0.000828 --> 0.000827).  Saving model ...\nEpoch: 243600, loss: 0.00088\nEpoch: 243700, loss: 0.00088\nEpoch: 243800, loss: 0.00099\nEpoch: 243900, loss: 0.00108\nLoss decreased (0.000827 --> 0.000827).  Saving model ...\nLoss decreased (0.000827 --> 0.000826).  Saving model ...\nEpoch: 244000, loss: 0.00083\nLoss decreased (0.000826 --> 0.000826).  Saving model ...\nEpoch: 244100, loss: 0.00232\nEpoch: 244200, loss: 0.00158\nEpoch: 244300, loss: 0.00084\nEpoch: 244400, loss: 0.00123\nEpoch: 244500, loss: 0.00111\nEpoch: 244600, loss: 0.00101\nEpoch: 244700, loss: 0.00091\nLoss decreased (0.000826 --> 0.000825).  Saving model ...\nEpoch: 244800, loss: 0.00087\nEpoch: 244900, loss: 0.00112\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nEpoch: 245000, loss: 0.00087\nEpoch: 245100, loss: 0.00118\nLoss decreased (0.000825 --> 0.000824).  Saving model ...\nEpoch: 245200, loss: 0.00101\nEpoch: 245300, loss: 0.00087\nEpoch: 245400, loss: 0.00088\nEpoch: 245500, loss: 0.00099\nEpoch: 245600, loss: 0.00170\nLoss decreased (0.000824 --> 0.000822).  Saving model ...\nEpoch: 245700, loss: 0.00120\nEpoch: 245800, loss: 0.00094\nEpoch: 245900, loss: 0.00112\nLoss decreased (0.000822 --> 0.000822).  Saving model ...\nEpoch: 246000, loss: 0.00113\nEpoch: 246100, loss: 0.00096\nLoss decreased (0.000822 --> 0.000819).  Saving model ...\nEpoch: 246200, loss: 0.00101\nEpoch: 246300, loss: 0.00101\nEpoch: 246400, loss: 0.00128\nEpoch: 246500, loss: 0.00094\nEpoch: 246600, loss: 0.00108\nEpoch: 246700, loss: 0.00090\nEpoch: 246800, loss: 0.00103\nEpoch: 246900, loss: 0.00091\nEpoch: 247000, loss: 0.00108\nEpoch: 247100, loss: 0.00103\nEpoch: 247200, loss: 0.00096\nEpoch: 247300, loss: 0.00096\nEpoch: 247400, loss: 0.00116\nLoss decreased (0.000819 --> 0.000819).  Saving model ...\nEpoch: 247500, loss: 0.00115\nEpoch: 247600, loss: 0.00085\nEpoch: 247700, loss: 0.00111\nEpoch: 247800, loss: 0.00124\nEpoch: 247900, loss: 0.00102\nEpoch: 248000, loss: 0.00104\nEpoch: 248100, loss: 0.00083\nLoss decreased (0.000819 --> 0.000817).  Saving model ...\nEpoch: 248200, loss: 0.00125\nEpoch: 248300, loss: 0.00102\nEpoch: 248400, loss: 0.00102\nEpoch: 248500, loss: 0.00089\nEpoch: 248600, loss: 0.00094\nEpoch: 248700, loss: 0.00095\nEpoch: 248800, loss: 0.00096\nEpoch: 248900, loss: 0.00084\nEpoch: 249000, loss: 0.00101\nEpoch: 249100, loss: 0.00097\nEpoch: 249200, loss: 0.00088\nEpoch: 249300, loss: 0.00089\nEpoch: 249400, loss: 0.00105\nEpoch: 249500, loss: 0.00094\nEpoch: 249600, loss: 0.00097\nEpoch: 249700, loss: 0.00100\nLoss decreased (0.000817 --> 0.000816).  Saving model ...\nEpoch: 249800, loss: 0.00088\nEpoch: 249900, loss: 0.00112\nLoss decreased (0.000816 --> 0.000816).  Saving model ...\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Loading model with minimum loss "},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    train_loss= checkpoint['train_loss']\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n    \n    model.eval()\n    \n    return model, train_loss","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_best, train_loss = load_checkpoint('checkpoint.pth')\nmodel_best= model_best\ntrain_loss= train_loss\nprint(model_best)","execution_count":20,"outputs":[{"output_type":"stream","text":"LSTM1(\n  (lstm): LSTM(60, 8, num_layers=3, batch_first=True)\n  (fc_1): Linear(in_features=8, out_features=128, bias=True)\n  (fc): Linear(in_features=128, out_features=1, bias=True)\n  (relu): ReLU()\n)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Testing the model... "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a datastructure with 60 time stamps and 1 output(for entire data, previously done just on training set)\n\nx= []\ny= []\n\nfor i in range(60, len(data_scaled)):\n    x.append(data_scaled[i-60:i, :])\n    y.append(data_scaled[i, 0])\n    \n# Converting them to numpy arrays\nx, y= np.array(x), np.array(y)","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tensors = Variable(torch.Tensor(x)).cuda()\n# X_test_tensors = Variable(torch.Tensor(X_test)).cuda()\n\ny_tensors = Variable(torch.Tensor(y)).cuda()\n# y_test_tensors = Variable(torch.Tensor(y_test)) .cuda()\n\n\nX_tensors_final = torch.reshape(X_tensors,   (X_tensors.shape[0], 1, X_tensors.shape[1]))","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On complete dataset\n\ntrain_predict = model_best(X_tensors_final.cuda())#forward pass\n\ndata_predict = train_predict.data.cpu()\ndata_predict = data_predict.numpy() #numpy conversion\n\ndataY_plot = y_tensors.data.cpu()\ndataY_plot = dataY_plot.numpy()\n\ndata_predict = mm.inverse_transform(data_predict) #reverse transformation\ndataY_plot = mm.inverse_transform(dataY_plot.reshape(-1, 1))\nplt.figure(figsize=(10,6)) #plotting\n\nplt.plot(dataY_plot, label='Actuall Data') #actual plot\nplt.plot(data_predict, label='Predicted Data') #predicted plot\nplt.title('Time-Series Prediction')\nplt.legend()\nplt.show() ","execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only on Test set\n# We droped the last 30 values for testing above(during data preprocessing)\n\nX_test= X_train_tensors_final[-30:, :].cuda()\ny_test= y_train_tensors[-30:, :].cuda()\n\n\ntest_predict= model_best(X_test) # Forward pass\n\npredicted= test_predict.data.cpu()\npredicted= predicted.numpy()\n\nreal_test= y_test.data.cpu()\nreal_test= real_test.numpy()\n\n\npredicted= mm.inverse_transform(predicted)\nreal_test= mm.inverse_transform(real_test)\n\nplt.figure(figsize=(10,6)) #plotting\nplt.plot(real_test, label='Actuall Data') #actual plot\nplt.plot(predicted, label='Predicted Data') #predicted plot\nplt.title('Time-Series Prediction on Test Data')\nplt.legend()\nplt.show() ","execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## R2 score calculation over Test set..."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\ncoefficient_of_dermination = r2_score(real_test, predicted)\nprint(coefficient_of_dermination)","execution_count":25,"outputs":[{"output_type":"stream","text":"0.8706176947916204\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Saving the model... "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the LSTM model\nimport joblib\njoblib.dump(model_best, '1DayGrowth%Approch2.pkl')","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"['1DayGrowth%Approch2.pkl']"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}