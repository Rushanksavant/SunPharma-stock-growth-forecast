{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df1= pd.read_csv(\"../input/sunpharma-data-redefined/Previous30DayClose.csv\", index_col= \"Date\", parse_dates= True)\ndf1","execution_count":2,"outputs":[{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"                  Open        High         Low       Close   Adj Close  \\\nDate                                                                     \n2009-12-01  145.354996  155.500000  145.354996  153.990005  144.129593   \n2009-12-02  156.399994  160.000000  148.660004  149.720001  140.133011   \n2009-12-03  150.014999  152.100006  148.750000  149.475006  139.903687   \n2009-12-04  148.500000  150.880005  145.110001  148.270004  138.775833   \n2009-12-07  148.509995  149.789993  144.110001  145.190002  135.893051   \n...                ...         ...         ...         ...         ...   \n2021-02-08  640.900024  644.849976  632.549988  634.650024  629.150024   \n2021-02-09  631.900024  639.250000  621.000000  623.599976  623.599976   \n2021-02-10  623.049988  631.549988  614.099976  627.099976  627.099976   \n2021-02-11  622.000000  648.750000  622.000000  643.650024  643.650024   \n2021-02-12  654.400024  654.400024  622.349976  627.150024  627.150024   \n\n                Volume  Growth% of Close from previous 30 days  \nDate                                                            \n2009-12-01   4338810.0                               -7.006953  \n2009-12-02   4689480.0                               -3.519906  \n2009-12-03   2534030.0                               -4.933935  \n2009-12-04   1216690.0                               -2.461731  \n2009-12-07   1559620.0                                1.456706  \n...                ...                                     ...  \n2021-02-08   6479801.0                               -7.374151  \n2021-02-09   5794015.0                               -6.141756  \n2021-02-10   6902162.0                               -8.188483  \n2021-02-11  11433578.0                               -8.770296  \n2021-02-12  15471776.0                               -4.695848  \n\n[2757 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Open</th>\n      <th>High</th>\n      <th>Low</th>\n      <th>Close</th>\n      <th>Adj Close</th>\n      <th>Volume</th>\n      <th>Growth% of Close from previous 30 days</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2009-12-01</th>\n      <td>145.354996</td>\n      <td>155.500000</td>\n      <td>145.354996</td>\n      <td>153.990005</td>\n      <td>144.129593</td>\n      <td>4338810.0</td>\n      <td>-7.006953</td>\n    </tr>\n    <tr>\n      <th>2009-12-02</th>\n      <td>156.399994</td>\n      <td>160.000000</td>\n      <td>148.660004</td>\n      <td>149.720001</td>\n      <td>140.133011</td>\n      <td>4689480.0</td>\n      <td>-3.519906</td>\n    </tr>\n    <tr>\n      <th>2009-12-03</th>\n      <td>150.014999</td>\n      <td>152.100006</td>\n      <td>148.750000</td>\n      <td>149.475006</td>\n      <td>139.903687</td>\n      <td>2534030.0</td>\n      <td>-4.933935</td>\n    </tr>\n    <tr>\n      <th>2009-12-04</th>\n      <td>148.500000</td>\n      <td>150.880005</td>\n      <td>145.110001</td>\n      <td>148.270004</td>\n      <td>138.775833</td>\n      <td>1216690.0</td>\n      <td>-2.461731</td>\n    </tr>\n    <tr>\n      <th>2009-12-07</th>\n      <td>148.509995</td>\n      <td>149.789993</td>\n      <td>144.110001</td>\n      <td>145.190002</td>\n      <td>135.893051</td>\n      <td>1559620.0</td>\n      <td>1.456706</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2021-02-08</th>\n      <td>640.900024</td>\n      <td>644.849976</td>\n      <td>632.549988</td>\n      <td>634.650024</td>\n      <td>629.150024</td>\n      <td>6479801.0</td>\n      <td>-7.374151</td>\n    </tr>\n    <tr>\n      <th>2021-02-09</th>\n      <td>631.900024</td>\n      <td>639.250000</td>\n      <td>621.000000</td>\n      <td>623.599976</td>\n      <td>623.599976</td>\n      <td>5794015.0</td>\n      <td>-6.141756</td>\n    </tr>\n    <tr>\n      <th>2021-02-10</th>\n      <td>623.049988</td>\n      <td>631.549988</td>\n      <td>614.099976</td>\n      <td>627.099976</td>\n      <td>627.099976</td>\n      <td>6902162.0</td>\n      <td>-8.188483</td>\n    </tr>\n    <tr>\n      <th>2021-02-11</th>\n      <td>622.000000</td>\n      <td>648.750000</td>\n      <td>622.000000</td>\n      <td>643.650024</td>\n      <td>643.650024</td>\n      <td>11433578.0</td>\n      <td>-8.770296</td>\n    </tr>\n    <tr>\n      <th>2021-02-12</th>\n      <td>654.400024</td>\n      <td>654.400024</td>\n      <td>622.349976</td>\n      <td>627.150024</td>\n      <td>627.150024</td>\n      <td>15471776.0</td>\n      <td>-4.695848</td>\n    </tr>\n  </tbody>\n</table>\n<p>2757 rows Ã— 7 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"ggplot\")\n\ndf1[\"Close\"].plot(label=\"Close\", title=\"Sunpharma Closing price\", figsize=(10,6))","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"<AxesSubplot:title={'center':'Sunpharma Closing price'}, xlabel='Date'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Data Preprocessing...  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features and Target segmentation\ndata = df1.iloc[:, 6:7].values # .values to convert it to numpy array","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(data)","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"numpy.ndarray"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Normalization\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nmm = MinMaxScaler(feature_range= (0, 1))\nss = StandardScaler()\n\ndata_scaled = mm.fit_transform(data)\ndata_scaled","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"array([[0.33311583],\n       [0.37967157],\n       [0.3607928 ],\n       ...,\n       [0.31734117],\n       [0.30957336],\n       [0.36397152]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keeping last 30 values(about a month) for testing\ntrain_data = data_scaled[:2726, :]\ntest_data= data_scaled[2726:, :]\n\nprint(train_data.shape)\nprint(test_data.shape)","execution_count":7,"outputs":[{"output_type":"stream","text":"(2726, 1)\n(31, 1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a datastructure with 60 time stamps and 1 output(only on training set)\n\nx_train= []\ny_train= []\n\nfor i in range(90, len(train_data)):\n    x_train.append(train_data[i-90:i, :])\n    y_train.append(train_data[i, 0])\n    \n# Converting them to numpy arrays\nx_train, y_train= np.array(x_train), np.array(y_train)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x_train. shape, y_train.shape","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"((2636, 90, 1), (2636,))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train= y_train.reshape(y_train.shape[0], 1)\ny_train.shape","execution_count":10,"outputs":[{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"(2636, 1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Importing dependencies\nimport torch \nimport torch.nn as nn\nfrom torch.autograd import Variable ","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tensors = Variable(torch.Tensor(x_train)).cuda()\n# X_test_tensors = Variable(torch.Tensor(X_test)).cuda()\n\ny_train_tensors = Variable(torch.Tensor(y_train)).cuda()\n# y_test_tensors = Variable(torch.Tensor(y_test)) .cuda()","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshaping to rows, timestamps, features\n\nX_train_tensors_final = torch.reshape(X_train_tensors,   (X_train_tensors.shape[0], 1, X_train_tensors.shape[1]))\n\n# X_test_tensors_final = torch.reshape(X_test_tensors,  (X_test_tensors.shape[0], 1, X_test_tensors.shape[1]))\n\n\n# Printing shapes \nprint(\"Training Shape\", X_train_tensors_final.shape, y_train_tensors.shape)\n# print(\"Testing Shape\", X_test_tensors_final.shape, y_test_tensors.shape)","execution_count":13,"outputs":[{"output_type":"stream","text":"Training Shape torch.Size([2636, 1, 90]) torch.Size([2636, 1])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Network Defination...."},{"metadata":{"trusted":true},"cell_type":"code","source":"class LSTM1(nn.Module):\n    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n        super(LSTM1, self).__init__()\n        self.num_classes = num_classes # number of classes\n        self.num_layers = num_layers # number of layers\n        self.input_size = input_size # input size\n        self.hidden_size = hidden_size # hidden state\n        self.seq_length = seq_length # sequence length\n\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                          num_layers=num_layers, batch_first=True) # lstm\n        self.fc_1 =  nn.Linear(hidden_size, 128) # fully connected 1\n        self.fc = nn.Linear(128, num_classes) # fully connected last layer\n\n        self.relu = nn.ReLU()\n    \n    def forward(self,x):\n        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda() # hidden state\n        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).cuda() # internal state\n        # Propagate input through LSTM\n        output, (hn, cn) = self.lstm(x, (h_0, c_0)) # lstm with input, hidden, and internal state\n        output = output.view(-1, self.hidden_size) # reshaping the output for fc layer\n        out = self.relu(output)\n        out = self.fc_1(out) #first Dense\n        out = self.relu(out) #relu\n        out = self.fc(out) #Final Output\n        return out","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Important parameters.... "},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 75000 \nlearning_rate = 0.003 \n\ninput_size = 90 # number of features\nhidden_size = 8 # number of features in hidden state\nnum_layers = 3 # number of stacked lstm layers\n\nnum_classes = 1 # number of output classes(coz regression)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm1 = LSTM1(num_classes, input_size, hidden_size, num_layers, X_train_tensors_final.shape[1]).cuda() #our lstm class","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion = torch.nn.MSELoss()    # mean-squared error for regression\noptimizer = torch.optim.Adam(lstm1.parameters(), lr=learning_rate)","execution_count":21,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training Loop.... "},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_min = np.Inf\n\nfor epoch in range(num_epochs):\n    outputs = lstm1.forward(X_train_tensors_final) #forward pass\n    optimizer.zero_grad() #caluclate the gradient, manually setting to 0\n\n    # obtain the loss function\n    loss = criterion(outputs, y_train_tensors)\n\n    loss.backward() #calculates the loss of the loss function\n \n    # save model if loss has decreased\n    if loss.item() <= loss_min:\n        print('Loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        loss_min,\n        loss.item()))\n        checkpoint = {'model': lstm1,\n              'state_dict': lstm1.state_dict(),\n              'optimizer' : optimizer.state_dict(),\n              'train_loss': loss.item()}\n\n        torch.save(checkpoint, 'checkpoint.pth')\n        loss_min = loss.item()\n\n    optimizer.step() #improve from loss, i.e backprop\n    if epoch % 100 == 0:\n        print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item())) ","execution_count":22,"outputs":[{"output_type":"stream","text":"Loss decreased (inf --> 0.236942).  Saving model ...\nEpoch: 0, loss: 0.23694\nLoss decreased (0.236942 --> 0.197605).  Saving model ...\nLoss decreased (0.197605 --> 0.162571).  Saving model ...\nLoss decreased (0.162571 --> 0.131743).  Saving model ...\nLoss decreased (0.131743 --> 0.104989).  Saving model ...\nLoss decreased (0.104989 --> 0.082191).  Saving model ...\nLoss decreased (0.082191 --> 0.063205).  Saving model ...\nLoss decreased (0.063205 --> 0.047835).  Saving model ...\nLoss decreased (0.047835 --> 0.035862).  Saving model ...\nLoss decreased (0.035862 --> 0.027075).  Saving model ...\nLoss decreased (0.027075 --> 0.021207).  Saving model ...\nLoss decreased (0.021207 --> 0.017944).  Saving model ...\nLoss decreased (0.017944 --> 0.016868).  Saving model ...\nLoss decreased (0.016868 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nEpoch: 100, loss: 0.01687\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016867).  Saving model ...\nLoss decreased (0.016867 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nEpoch: 200, loss: 0.01687\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016866).  Saving model ...\nLoss decreased (0.016866 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016865).  Saving model ...\nLoss decreased (0.016865 --> 0.016864).  Saving model ...\nLoss decreased (0.016864 --> 0.016864).  Saving model ...\nLoss decreased (0.016864 --> 0.016864).  Saving model ...\nLoss decreased (0.016864 --> 0.016864).  Saving model ...\nLoss decreased (0.016864 --> 0.016864).  Saving model ...\nLoss decreased (0.016864 --> 0.016864).  Saving model ...\nLoss decreased (0.016864 --> 0.016864).  Saving model ...\nLoss decreased (0.016864 --> 0.016864).  Saving model ...\nLoss decreased (0.016864 --> 0.016864).  Saving model ...\nLoss decreased (0.016864 --> 0.016864).  Saving model ...\nLoss decreased (0.016864 --> 0.016863).  Saving model ...\nLoss decreased (0.016863 --> 0.016863).  Saving model ...\nLoss decreased (0.016863 --> 0.016863).  Saving model ...\nLoss decreased (0.016863 --> 0.016863).  Saving model ...\nLoss decreased (0.016863 --> 0.016863).  Saving model ...\nLoss decreased (0.016863 --> 0.016863).  Saving model ...\nLoss decreased (0.016863 --> 0.016863).  Saving model ...\nLoss decreased (0.016863 --> 0.016863).  Saving model ...\nLoss decreased (0.016863 --> 0.016862).  Saving model ...\nLoss decreased (0.016862 --> 0.016862).  Saving model ...\nLoss decreased (0.016862 --> 0.016862).  Saving model ...\nLoss decreased (0.016862 --> 0.016862).  Saving model ...\nLoss decreased (0.016862 --> 0.016862).  Saving model ...\nLoss decreased (0.016862 --> 0.016861).  Saving model ...\nLoss decreased (0.016861 --> 0.016861).  Saving model ...\nLoss decreased (0.016861 --> 0.016861).  Saving model ...\nLoss decreased (0.016861 --> 0.016861).  Saving model ...\nLoss decreased (0.016861 --> 0.016860).  Saving model ...\nLoss decreased (0.016860 --> 0.016860).  Saving model ...\nLoss decreased (0.016860 --> 0.016860).  Saving model ...\nLoss decreased (0.016860 --> 0.016859).  Saving model ...\nLoss decreased (0.016859 --> 0.016859).  Saving model ...\nLoss decreased (0.016859 --> 0.016859).  Saving model ...\nLoss decreased (0.016859 --> 0.016858).  Saving model ...\nLoss decreased (0.016858 --> 0.016858).  Saving model ...\nLoss decreased (0.016858 --> 0.016858).  Saving model ...\nLoss decreased (0.016858 --> 0.016857).  Saving model ...\nLoss decreased (0.016857 --> 0.016857).  Saving model ...\nLoss decreased (0.016857 --> 0.016856).  Saving model ...\nLoss decreased (0.016856 --> 0.016856).  Saving model ...\nLoss decreased (0.016856 --> 0.016855).  Saving model ...\nLoss decreased (0.016855 --> 0.016854).  Saving model ...\nLoss decreased (0.016854 --> 0.016854).  Saving model ...\nLoss decreased (0.016854 --> 0.016853).  Saving model ...\nLoss decreased (0.016853 --> 0.016852).  Saving model ...\nLoss decreased (0.016852 --> 0.016851).  Saving model ...\nLoss decreased (0.016851 --> 0.016850).  Saving model ...\nLoss decreased (0.016850 --> 0.016849).  Saving model ...\nLoss decreased (0.016849 --> 0.016848).  Saving model ...\nLoss decreased (0.016848 --> 0.016847).  Saving model ...\nLoss decreased (0.016847 --> 0.016846).  Saving model ...\nLoss decreased (0.016846 --> 0.016845).  Saving model ...\nLoss decreased (0.016845 --> 0.016843).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.016843 --> 0.016842).  Saving model ...\nLoss decreased (0.016842 --> 0.016840).  Saving model ...\nLoss decreased (0.016840 --> 0.016838).  Saving model ...\nLoss decreased (0.016838 --> 0.016836).  Saving model ...\nLoss decreased (0.016836 --> 0.016834).  Saving model ...\nLoss decreased (0.016834 --> 0.016832).  Saving model ...\nLoss decreased (0.016832 --> 0.016829).  Saving model ...\nLoss decreased (0.016829 --> 0.016826).  Saving model ...\nLoss decreased (0.016826 --> 0.016823).  Saving model ...\nLoss decreased (0.016823 --> 0.016819).  Saving model ...\nLoss decreased (0.016819 --> 0.016815).  Saving model ...\nLoss decreased (0.016815 --> 0.016811).  Saving model ...\nLoss decreased (0.016811 --> 0.016806).  Saving model ...\nLoss decreased (0.016806 --> 0.016801).  Saving model ...\nLoss decreased (0.016801 --> 0.016795).  Saving model ...\nLoss decreased (0.016795 --> 0.016789).  Saving model ...\nLoss decreased (0.016789 --> 0.016782).  Saving model ...\nLoss decreased (0.016782 --> 0.016775).  Saving model ...\nLoss decreased (0.016775 --> 0.016767).  Saving model ...\nLoss decreased (0.016767 --> 0.016760).  Saving model ...\nLoss decreased (0.016760 --> 0.016752).  Saving model ...\nEpoch: 300, loss: 0.01675\nLoss decreased (0.016752 --> 0.016744).  Saving model ...\nLoss decreased (0.016744 --> 0.016734).  Saving model ...\nLoss decreased (0.016734 --> 0.016724).  Saving model ...\nLoss decreased (0.016724 --> 0.016713).  Saving model ...\nLoss decreased (0.016713 --> 0.016701).  Saving model ...\nLoss decreased (0.016701 --> 0.016687).  Saving model ...\nLoss decreased (0.016687 --> 0.016672).  Saving model ...\nLoss decreased (0.016672 --> 0.016655).  Saving model ...\nLoss decreased (0.016655 --> 0.016637).  Saving model ...\nLoss decreased (0.016637 --> 0.016617).  Saving model ...\nLoss decreased (0.016617 --> 0.016594).  Saving model ...\nLoss decreased (0.016594 --> 0.016568).  Saving model ...\nLoss decreased (0.016568 --> 0.016539).  Saving model ...\nLoss decreased (0.016539 --> 0.016506).  Saving model ...\nLoss decreased (0.016506 --> 0.016470).  Saving model ...\nLoss decreased (0.016470 --> 0.016429).  Saving model ...\nLoss decreased (0.016429 --> 0.016383).  Saving model ...\nLoss decreased (0.016383 --> 0.016330).  Saving model ...\nLoss decreased (0.016330 --> 0.016269).  Saving model ...\nLoss decreased (0.016269 --> 0.016200).  Saving model ...\nLoss decreased (0.016200 --> 0.016122).  Saving model ...\nLoss decreased (0.016122 --> 0.016034).  Saving model ...\nLoss decreased (0.016034 --> 0.015936).  Saving model ...\nLoss decreased (0.015936 --> 0.015823).  Saving model ...\nLoss decreased (0.015823 --> 0.015695).  Saving model ...\nLoss decreased (0.015695 --> 0.015520).  Saving model ...\nLoss decreased (0.015520 --> 0.015300).  Saving model ...\nLoss decreased (0.015300 --> 0.015040).  Saving model ...\nLoss decreased (0.015040 --> 0.014744).  Saving model ...\nLoss decreased (0.014744 --> 0.014418).  Saving model ...\nLoss decreased (0.014418 --> 0.014083).  Saving model ...\nLoss decreased (0.014083 --> 0.013756).  Saving model ...\nLoss decreased (0.013756 --> 0.013394).  Saving model ...\nLoss decreased (0.013394 --> 0.012997).  Saving model ...\nLoss decreased (0.012997 --> 0.012558).  Saving model ...\nLoss decreased (0.012558 --> 0.012082).  Saving model ...\nLoss decreased (0.012082 --> 0.011585).  Saving model ...\nLoss decreased (0.011585 --> 0.011115).  Saving model ...\nLoss decreased (0.011115 --> 0.010516).  Saving model ...\nLoss decreased (0.010516 --> 0.010031).  Saving model ...\nLoss decreased (0.010031 --> 0.009527).  Saving model ...\nLoss decreased (0.009527 --> 0.009057).  Saving model ...\nLoss decreased (0.009057 --> 0.008750).  Saving model ...\nLoss decreased (0.008750 --> 0.008385).  Saving model ...\nLoss decreased (0.008385 --> 0.008131).  Saving model ...\nLoss decreased (0.008131 --> 0.007878).  Saving model ...\nLoss decreased (0.007878 --> 0.007536).  Saving model ...\nLoss decreased (0.007536 --> 0.007257).  Saving model ...\nLoss decreased (0.007257 --> 0.007084).  Saving model ...\nLoss decreased (0.007084 --> 0.006890).  Saving model ...\nLoss decreased (0.006890 --> 0.006606).  Saving model ...\nLoss decreased (0.006606 --> 0.006418).  Saving model ...\nLoss decreased (0.006418 --> 0.006243).  Saving model ...\nLoss decreased (0.006243 --> 0.005964).  Saving model ...\nLoss decreased (0.005964 --> 0.005753).  Saving model ...\nLoss decreased (0.005753 --> 0.005591).  Saving model ...\nLoss decreased (0.005591 --> 0.005354).  Saving model ...\nLoss decreased (0.005354 --> 0.005176).  Saving model ...\nLoss decreased (0.005176 --> 0.005028).  Saving model ...\nLoss decreased (0.005028 --> 0.004816).  Saving model ...\nLoss decreased (0.004816 --> 0.004622).  Saving model ...\nLoss decreased (0.004622 --> 0.004479).  Saving model ...\nLoss decreased (0.004479 --> 0.004362).  Saving model ...\nLoss decreased (0.004362 --> 0.004206).  Saving model ...\nLoss decreased (0.004206 --> 0.004025).  Saving model ...\nLoss decreased (0.004025 --> 0.003845).  Saving model ...\nLoss decreased (0.003845 --> 0.003715).  Saving model ...\nLoss decreased (0.003715 --> 0.003620).  Saving model ...\nLoss decreased (0.003620 --> 0.003617).  Saving model ...\nLoss decreased (0.003617 --> 0.003373).  Saving model ...\nLoss decreased (0.003373 --> 0.003311).  Saving model ...\nLoss decreased (0.003311 --> 0.003191).  Saving model ...\nLoss decreased (0.003191 --> 0.003098).  Saving model ...\nLoss decreased (0.003098 --> 0.003044).  Saving model ...\nLoss decreased (0.003044 --> 0.003023).  Saving model ...\nLoss decreased (0.003023 --> 0.003014).  Saving model ...\nLoss decreased (0.003014 --> 0.002999).  Saving model ...\nLoss decreased (0.002999 --> 0.002953).  Saving model ...\nLoss decreased (0.002953 --> 0.002866).  Saving model ...\nLoss decreased (0.002866 --> 0.002801).  Saving model ...\nLoss decreased (0.002801 --> 0.002749).  Saving model ...\nLoss decreased (0.002749 --> 0.002704).  Saving model ...\nLoss decreased (0.002704 --> 0.002662).  Saving model ...\nEpoch: 400, loss: 0.00270\nLoss decreased (0.002662 --> 0.002621).  Saving model ...\nLoss decreased (0.002621 --> 0.002579).  Saving model ...\nLoss decreased (0.002579 --> 0.002538).  Saving model ...\nLoss decreased (0.002538 --> 0.002499).  Saving model ...\nLoss decreased (0.002499 --> 0.002461).  Saving model ...\nLoss decreased (0.002461 --> 0.002427).  Saving model ...\nLoss decreased (0.002427 --> 0.002417).  Saving model ...\nLoss decreased (0.002417 --> 0.002396).  Saving model ...\nLoss decreased (0.002396 --> 0.002368).  Saving model ...\nLoss decreased (0.002368 --> 0.002360).  Saving model ...\nLoss decreased (0.002360 --> 0.002323).  Saving model ...\nLoss decreased (0.002323 --> 0.002317).  Saving model ...\nLoss decreased (0.002317 --> 0.002291).  Saving model ...\nLoss decreased (0.002291 --> 0.002270).  Saving model ...\nLoss decreased (0.002270 --> 0.002262).  Saving model ...\nLoss decreased (0.002262 --> 0.002235).  Saving model ...\nLoss decreased (0.002235 --> 0.002218).  Saving model ...\nLoss decreased (0.002218 --> 0.002209).  Saving model ...\nLoss decreased (0.002209 --> 0.002185).  Saving model ...\nLoss decreased (0.002185 --> 0.002164).  Saving model ...\nLoss decreased (0.002164 --> 0.002154).  Saving model ...\nLoss decreased (0.002154 --> 0.002140).  Saving model ...\nLoss decreased (0.002140 --> 0.002121).  Saving model ...\nLoss decreased (0.002121 --> 0.002099).  Saving model ...\nLoss decreased (0.002099 --> 0.002082).  Saving model ...\nLoss decreased (0.002082 --> 0.002070).  Saving model ...\nLoss decreased (0.002070 --> 0.002060).  Saving model ...\nLoss decreased (0.002060 --> 0.002056).  Saving model ...\nLoss decreased (0.002056 --> 0.002051).  Saving model ...\nLoss decreased (0.002051 --> 0.002050).  Saving model ...\nLoss decreased (0.002050 --> 0.002039).  Saving model ...\nLoss decreased (0.002039 --> 0.002021).  Saving model ...\nLoss decreased (0.002021 --> 0.001992).  Saving model ...\nLoss decreased (0.001992 --> 0.001967).  Saving model ...\nLoss decreased (0.001967 --> 0.001949).  Saving model ...\nLoss decreased (0.001949 --> 0.001940).  Saving model ...\nLoss decreased (0.001940 --> 0.001938).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.001938 --> 0.001901).  Saving model ...\nLoss decreased (0.001901 --> 0.001856).  Saving model ...\nLoss decreased (0.001856 --> 0.001820).  Saving model ...\nLoss decreased (0.001820 --> 0.001814).  Saving model ...\nLoss decreased (0.001814 --> 0.001787).  Saving model ...\nLoss decreased (0.001787 --> 0.001783).  Saving model ...\nLoss decreased (0.001783 --> 0.001758).  Saving model ...\nLoss decreased (0.001758 --> 0.001753).  Saving model ...\nLoss decreased (0.001753 --> 0.001737).  Saving model ...\nLoss decreased (0.001737 --> 0.001721).  Saving model ...\nLoss decreased (0.001721 --> 0.001715).  Saving model ...\nLoss decreased (0.001715 --> 0.001710).  Saving model ...\nLoss decreased (0.001710 --> 0.001698).  Saving model ...\nLoss decreased (0.001698 --> 0.001686).  Saving model ...\nLoss decreased (0.001686 --> 0.001677).  Saving model ...\nLoss decreased (0.001677 --> 0.001670).  Saving model ...\nLoss decreased (0.001670 --> 0.001666).  Saving model ...\nLoss decreased (0.001666 --> 0.001663).  Saving model ...\nLoss decreased (0.001663 --> 0.001662).  Saving model ...\nLoss decreased (0.001662 --> 0.001617).  Saving model ...\nLoss decreased (0.001617 --> 0.001609).  Saving model ...\nLoss decreased (0.001609 --> 0.001585).  Saving model ...\nEpoch: 500, loss: 0.00159\nLoss decreased (0.001585 --> 0.001570).  Saving model ...\nLoss decreased (0.001570 --> 0.001564).  Saving model ...\nLoss decreased (0.001564 --> 0.001562).  Saving model ...\nLoss decreased (0.001562 --> 0.001546).  Saving model ...\nLoss decreased (0.001546 --> 0.001542).  Saving model ...\nLoss decreased (0.001542 --> 0.001531).  Saving model ...\nLoss decreased (0.001531 --> 0.001522).  Saving model ...\nLoss decreased (0.001522 --> 0.001517).  Saving model ...\nLoss decreased (0.001517 --> 0.001517).  Saving model ...\nLoss decreased (0.001517 --> 0.001517).  Saving model ...\nLoss decreased (0.001517 --> 0.001512).  Saving model ...\nLoss decreased (0.001512 --> 0.001506).  Saving model ...\nLoss decreased (0.001506 --> 0.001500).  Saving model ...\nLoss decreased (0.001500 --> 0.001494).  Saving model ...\nLoss decreased (0.001494 --> 0.001488).  Saving model ...\nLoss decreased (0.001488 --> 0.001483).  Saving model ...\nLoss decreased (0.001483 --> 0.001479).  Saving model ...\nLoss decreased (0.001479 --> 0.001476).  Saving model ...\nLoss decreased (0.001476 --> 0.001472).  Saving model ...\nLoss decreased (0.001472 --> 0.001469).  Saving model ...\nLoss decreased (0.001469 --> 0.001467).  Saving model ...\nLoss decreased (0.001467 --> 0.001464).  Saving model ...\nLoss decreased (0.001464 --> 0.001463).  Saving model ...\nLoss decreased (0.001463 --> 0.001437).  Saving model ...\nLoss decreased (0.001437 --> 0.001433).  Saving model ...\nLoss decreased (0.001433 --> 0.001419).  Saving model ...\nLoss decreased (0.001419 --> 0.001409).  Saving model ...\nLoss decreased (0.001409 --> 0.001408).  Saving model ...\nLoss decreased (0.001408 --> 0.001394).  Saving model ...\nLoss decreased (0.001394 --> 0.001393).  Saving model ...\nLoss decreased (0.001393 --> 0.001388).  Saving model ...\nLoss decreased (0.001388 --> 0.001379).  Saving model ...\nLoss decreased (0.001379 --> 0.001371).  Saving model ...\nLoss decreased (0.001371 --> 0.001368).  Saving model ...\nLoss decreased (0.001368 --> 0.001366).  Saving model ...\nLoss decreased (0.001366 --> 0.001359).  Saving model ...\nLoss decreased (0.001359 --> 0.001358).  Saving model ...\nLoss decreased (0.001358 --> 0.001355).  Saving model ...\nLoss decreased (0.001355 --> 0.001349).  Saving model ...\nLoss decreased (0.001349 --> 0.001346).  Saving model ...\nLoss decreased (0.001346 --> 0.001344).  Saving model ...\nLoss decreased (0.001344 --> 0.001344).  Saving model ...\nLoss decreased (0.001344 --> 0.001340).  Saving model ...\nLoss decreased (0.001340 --> 0.001336).  Saving model ...\nLoss decreased (0.001336 --> 0.001331).  Saving model ...\nLoss decreased (0.001331 --> 0.001328).  Saving model ...\nLoss decreased (0.001328 --> 0.001325).  Saving model ...\nLoss decreased (0.001325 --> 0.001323).  Saving model ...\nEpoch: 600, loss: 0.00132\nLoss decreased (0.001323 --> 0.001323).  Saving model ...\nLoss decreased (0.001323 --> 0.001322).  Saving model ...\nLoss decreased (0.001322 --> 0.001322).  Saving model ...\nLoss decreased (0.001322 --> 0.001304).  Saving model ...\nLoss decreased (0.001304 --> 0.001295).  Saving model ...\nLoss decreased (0.001295 --> 0.001289).  Saving model ...\nLoss decreased (0.001289 --> 0.001281).  Saving model ...\nLoss decreased (0.001281 --> 0.001276).  Saving model ...\nLoss decreased (0.001276 --> 0.001276).  Saving model ...\nLoss decreased (0.001276 --> 0.001263).  Saving model ...\nLoss decreased (0.001263 --> 0.001252).  Saving model ...\nLoss decreased (0.001252 --> 0.001245).  Saving model ...\nLoss decreased (0.001245 --> 0.001238).  Saving model ...\nLoss decreased (0.001238 --> 0.001237).  Saving model ...\nLoss decreased (0.001237 --> 0.001226).  Saving model ...\nLoss decreased (0.001226 --> 0.001218).  Saving model ...\nLoss decreased (0.001218 --> 0.001214).  Saving model ...\nLoss decreased (0.001214 --> 0.001213).  Saving model ...\nLoss decreased (0.001213 --> 0.001202).  Saving model ...\nLoss decreased (0.001202 --> 0.001200).  Saving model ...\nLoss decreased (0.001200 --> 0.001196).  Saving model ...\nLoss decreased (0.001196 --> 0.001189).  Saving model ...\nEpoch: 700, loss: 0.00119\nLoss decreased (0.001189 --> 0.001184).  Saving model ...\nLoss decreased (0.001184 --> 0.001181).  Saving model ...\nLoss decreased (0.001181 --> 0.001179).  Saving model ...\nLoss decreased (0.001179 --> 0.001175).  Saving model ...\nLoss decreased (0.001175 --> 0.001172).  Saving model ...\nLoss decreased (0.001172 --> 0.001169).  Saving model ...\nLoss decreased (0.001169 --> 0.001168).  Saving model ...\nLoss decreased (0.001168 --> 0.001167).  Saving model ...\nLoss decreased (0.001167 --> 0.001166).  Saving model ...\nLoss decreased (0.001166 --> 0.001165).  Saving model ...\nLoss decreased (0.001165 --> 0.001164).  Saving model ...\nLoss decreased (0.001164 --> 0.001159).  Saving model ...\nLoss decreased (0.001159 --> 0.001158).  Saving model ...\nLoss decreased (0.001158 --> 0.001152).  Saving model ...\nLoss decreased (0.001152 --> 0.001150).  Saving model ...\nLoss decreased (0.001150 --> 0.001144).  Saving model ...\nLoss decreased (0.001144 --> 0.001139).  Saving model ...\nLoss decreased (0.001139 --> 0.001135).  Saving model ...\nLoss decreased (0.001135 --> 0.001132).  Saving model ...\nLoss decreased (0.001132 --> 0.001129).  Saving model ...\nLoss decreased (0.001129 --> 0.001128).  Saving model ...\nLoss decreased (0.001128 --> 0.001127).  Saving model ...\nLoss decreased (0.001127 --> 0.001124).  Saving model ...\nLoss decreased (0.001124 --> 0.001124).  Saving model ...\nLoss decreased (0.001124 --> 0.001122).  Saving model ...\nLoss decreased (0.001122 --> 0.001120).  Saving model ...\nLoss decreased (0.001120 --> 0.001120).  Saving model ...\nLoss decreased (0.001120 --> 0.001119).  Saving model ...\nLoss decreased (0.001119 --> 0.001117).  Saving model ...\nLoss decreased (0.001117 --> 0.001116).  Saving model ...\nLoss decreased (0.001116 --> 0.001115).  Saving model ...\nLoss decreased (0.001115 --> 0.001115).  Saving model ...\nLoss decreased (0.001115 --> 0.001115).  Saving model ...\nLoss decreased (0.001115 --> 0.001114).  Saving model ...\nLoss decreased (0.001114 --> 0.001113).  Saving model ...\nLoss decreased (0.001113 --> 0.001112).  Saving model ...\nLoss decreased (0.001112 --> 0.001111).  Saving model ...\nLoss decreased (0.001111 --> 0.001110).  Saving model ...\nLoss decreased (0.001110 --> 0.001109).  Saving model ...\nLoss decreased (0.001109 --> 0.001109).  Saving model ...\nLoss decreased (0.001109 --> 0.001108).  Saving model ...\nLoss decreased (0.001108 --> 0.001108).  Saving model ...\nEpoch: 800, loss: 0.00111\nLoss decreased (0.001108 --> 0.001107).  Saving model ...\nLoss decreased (0.001107 --> 0.001106).  Saving model ...\nLoss decreased (0.001106 --> 0.001105).  Saving model ...\nLoss decreased (0.001105 --> 0.001104).  Saving model ...\nLoss decreased (0.001104 --> 0.001103).  Saving model ...\nLoss decreased (0.001103 --> 0.001102).  Saving model ...\nLoss decreased (0.001102 --> 0.001102).  Saving model ...\nLoss decreased (0.001102 --> 0.001101).  Saving model ...\nLoss decreased (0.001101 --> 0.001100).  Saving model ...\nLoss decreased (0.001100 --> 0.001099).  Saving model ...\nLoss decreased (0.001099 --> 0.001099).  Saving model ...\nLoss decreased (0.001099 --> 0.001098).  Saving model ...\nLoss decreased (0.001098 --> 0.001097).  Saving model ...\nLoss decreased (0.001097 --> 0.001097).  Saving model ...\nLoss decreased (0.001097 --> 0.001097).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.001097 --> 0.001096).  Saving model ...\nLoss decreased (0.001096 --> 0.001091).  Saving model ...\nLoss decreased (0.001091 --> 0.001089).  Saving model ...\nLoss decreased (0.001089 --> 0.001086).  Saving model ...\nLoss decreased (0.001086 --> 0.001082).  Saving model ...\nLoss decreased (0.001082 --> 0.001078).  Saving model ...\nLoss decreased (0.001078 --> 0.001076).  Saving model ...\nLoss decreased (0.001076 --> 0.001074).  Saving model ...\nLoss decreased (0.001074 --> 0.001073).  Saving model ...\nLoss decreased (0.001073 --> 0.001073).  Saving model ...\nLoss decreased (0.001073 --> 0.001070).  Saving model ...\nLoss decreased (0.001070 --> 0.001069).  Saving model ...\nLoss decreased (0.001069 --> 0.001067).  Saving model ...\nLoss decreased (0.001067 --> 0.001066).  Saving model ...\nLoss decreased (0.001066 --> 0.001064).  Saving model ...\nLoss decreased (0.001064 --> 0.001064).  Saving model ...\nLoss decreased (0.001064 --> 0.001063).  Saving model ...\nLoss decreased (0.001063 --> 0.001062).  Saving model ...\nLoss decreased (0.001062 --> 0.001061).  Saving model ...\nLoss decreased (0.001061 --> 0.001060).  Saving model ...\nLoss decreased (0.001060 --> 0.001060).  Saving model ...\nLoss decreased (0.001060 --> 0.001060).  Saving model ...\nLoss decreased (0.001060 --> 0.001059).  Saving model ...\nLoss decreased (0.001059 --> 0.001059).  Saving model ...\nLoss decreased (0.001059 --> 0.001058).  Saving model ...\nLoss decreased (0.001058 --> 0.001057).  Saving model ...\nLoss decreased (0.001057 --> 0.001056).  Saving model ...\nLoss decreased (0.001056 --> 0.001055).  Saving model ...\nLoss decreased (0.001055 --> 0.001055).  Saving model ...\nLoss decreased (0.001055 --> 0.001054).  Saving model ...\nLoss decreased (0.001054 --> 0.001054).  Saving model ...\nLoss decreased (0.001054 --> 0.001053).  Saving model ...\nLoss decreased (0.001053 --> 0.001053).  Saving model ...\nEpoch: 900, loss: 0.00105\nLoss decreased (0.001053 --> 0.001053).  Saving model ...\nLoss decreased (0.001053 --> 0.001053).  Saving model ...\nLoss decreased (0.001053 --> 0.001052).  Saving model ...\nLoss decreased (0.001052 --> 0.001045).  Saving model ...\nLoss decreased (0.001045 --> 0.001042).  Saving model ...\nLoss decreased (0.001042 --> 0.001042).  Saving model ...\nLoss decreased (0.001042 --> 0.001039).  Saving model ...\nLoss decreased (0.001039 --> 0.001036).  Saving model ...\nLoss decreased (0.001036 --> 0.001036).  Saving model ...\nLoss decreased (0.001036 --> 0.001032).  Saving model ...\nLoss decreased (0.001032 --> 0.001032).  Saving model ...\nLoss decreased (0.001032 --> 0.001030).  Saving model ...\nLoss decreased (0.001030 --> 0.001029).  Saving model ...\nLoss decreased (0.001029 --> 0.001029).  Saving model ...\nLoss decreased (0.001029 --> 0.001027).  Saving model ...\nLoss decreased (0.001027 --> 0.001026).  Saving model ...\nLoss decreased (0.001026 --> 0.001025).  Saving model ...\nLoss decreased (0.001025 --> 0.001025).  Saving model ...\nLoss decreased (0.001025 --> 0.001021).  Saving model ...\nLoss decreased (0.001021 --> 0.001021).  Saving model ...\nEpoch: 1000, loss: 0.00108\nLoss decreased (0.001021 --> 0.001016).  Saving model ...\nLoss decreased (0.001016 --> 0.001013).  Saving model ...\nLoss decreased (0.001013 --> 0.001011).  Saving model ...\nLoss decreased (0.001011 --> 0.001009).  Saving model ...\nLoss decreased (0.001009 --> 0.001008).  Saving model ...\nLoss decreased (0.001008 --> 0.001006).  Saving model ...\nLoss decreased (0.001006 --> 0.001005).  Saving model ...\nLoss decreased (0.001005 --> 0.001005).  Saving model ...\nLoss decreased (0.001005 --> 0.001005).  Saving model ...\nLoss decreased (0.001005 --> 0.001004).  Saving model ...\nLoss decreased (0.001004 --> 0.001003).  Saving model ...\nLoss decreased (0.001003 --> 0.001003).  Saving model ...\nLoss decreased (0.001003 --> 0.001003).  Saving model ...\nLoss decreased (0.001003 --> 0.001000).  Saving model ...\nLoss decreased (0.001000 --> 0.000996).  Saving model ...\nLoss decreased (0.000996 --> 0.000995).  Saving model ...\nLoss decreased (0.000995 --> 0.000992).  Saving model ...\nLoss decreased (0.000992 --> 0.000990).  Saving model ...\nLoss decreased (0.000990 --> 0.000989).  Saving model ...\nLoss decreased (0.000989 --> 0.000989).  Saving model ...\nEpoch: 1100, loss: 0.00099\nLoss decreased (0.000989 --> 0.000989).  Saving model ...\nLoss decreased (0.000989 --> 0.000987).  Saving model ...\nLoss decreased (0.000987 --> 0.000985).  Saving model ...\nLoss decreased (0.000985 --> 0.000985).  Saving model ...\nLoss decreased (0.000985 --> 0.000984).  Saving model ...\nLoss decreased (0.000984 --> 0.000983).  Saving model ...\nLoss decreased (0.000983 --> 0.000982).  Saving model ...\nLoss decreased (0.000982 --> 0.000982).  Saving model ...\nLoss decreased (0.000982 --> 0.000982).  Saving model ...\nLoss decreased (0.000982 --> 0.000982).  Saving model ...\nLoss decreased (0.000982 --> 0.000982).  Saving model ...\nLoss decreased (0.000982 --> 0.000981).  Saving model ...\nLoss decreased (0.000981 --> 0.000981).  Saving model ...\nLoss decreased (0.000981 --> 0.000981).  Saving model ...\nLoss decreased (0.000981 --> 0.000980).  Saving model ...\nLoss decreased (0.000980 --> 0.000980).  Saving model ...\nLoss decreased (0.000980 --> 0.000979).  Saving model ...\nLoss decreased (0.000979 --> 0.000979).  Saving model ...\nLoss decreased (0.000979 --> 0.000979).  Saving model ...\nLoss decreased (0.000979 --> 0.000978).  Saving model ...\nLoss decreased (0.000978 --> 0.000978).  Saving model ...\nLoss decreased (0.000978 --> 0.000978).  Saving model ...\nLoss decreased (0.000978 --> 0.000977).  Saving model ...\nLoss decreased (0.000977 --> 0.000977).  Saving model ...\nLoss decreased (0.000977 --> 0.000977).  Saving model ...\nLoss decreased (0.000977 --> 0.000977).  Saving model ...\nLoss decreased (0.000977 --> 0.000976).  Saving model ...\nLoss decreased (0.000976 --> 0.000976).  Saving model ...\nLoss decreased (0.000976 --> 0.000976).  Saving model ...\nLoss decreased (0.000976 --> 0.000976).  Saving model ...\nLoss decreased (0.000976 --> 0.000975).  Saving model ...\nLoss decreased (0.000975 --> 0.000975).  Saving model ...\nLoss decreased (0.000975 --> 0.000975).  Saving model ...\nLoss decreased (0.000975 --> 0.000974).  Saving model ...\nLoss decreased (0.000974 --> 0.000974).  Saving model ...\nLoss decreased (0.000974 --> 0.000974).  Saving model ...\nLoss decreased (0.000974 --> 0.000974).  Saving model ...\nLoss decreased (0.000974 --> 0.000974).  Saving model ...\nLoss decreased (0.000974 --> 0.000973).  Saving model ...\nEpoch: 1200, loss: 0.00099\nLoss decreased (0.000973 --> 0.000970).  Saving model ...\nLoss decreased (0.000970 --> 0.000969).  Saving model ...\nLoss decreased (0.000969 --> 0.000968).  Saving model ...\nLoss decreased (0.000968 --> 0.000966).  Saving model ...\nLoss decreased (0.000966 --> 0.000965).  Saving model ...\nLoss decreased (0.000965 --> 0.000964).  Saving model ...\nLoss decreased (0.000964 --> 0.000963).  Saving model ...\nLoss decreased (0.000963 --> 0.000963).  Saving model ...\nLoss decreased (0.000963 --> 0.000962).  Saving model ...\nLoss decreased (0.000962 --> 0.000962).  Saving model ...\nLoss decreased (0.000962 --> 0.000961).  Saving model ...\nLoss decreased (0.000961 --> 0.000961).  Saving model ...\nLoss decreased (0.000961 --> 0.000960).  Saving model ...\nLoss decreased (0.000960 --> 0.000960).  Saving model ...\nLoss decreased (0.000960 --> 0.000960).  Saving model ...\nLoss decreased (0.000960 --> 0.000959).  Saving model ...\nLoss decreased (0.000959 --> 0.000959).  Saving model ...\nLoss decreased (0.000959 --> 0.000959).  Saving model ...\nLoss decreased (0.000959 --> 0.000959).  Saving model ...\nLoss decreased (0.000959 --> 0.000958).  Saving model ...\nLoss decreased (0.000958 --> 0.000958).  Saving model ...\nLoss decreased (0.000958 --> 0.000958).  Saving model ...\nLoss decreased (0.000958 --> 0.000958).  Saving model ...\nLoss decreased (0.000958 --> 0.000958).  Saving model ...\nLoss decreased (0.000958 --> 0.000957).  Saving model ...\nLoss decreased (0.000957 --> 0.000957).  Saving model ...\nLoss decreased (0.000957 --> 0.000957).  Saving model ...\nLoss decreased (0.000957 --> 0.000957).  Saving model ...\nLoss decreased (0.000957 --> 0.000956).  Saving model ...\nLoss decreased (0.000956 --> 0.000956).  Saving model ...\nLoss decreased (0.000956 --> 0.000956).  Saving model ...\nLoss decreased (0.000956 --> 0.000956).  Saving model ...\nLoss decreased (0.000956 --> 0.000955).  Saving model ...\nLoss decreased (0.000955 --> 0.000955).  Saving model ...\nLoss decreased (0.000955 --> 0.000955).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000955 --> 0.000955).  Saving model ...\nLoss decreased (0.000955 --> 0.000954).  Saving model ...\nLoss decreased (0.000954 --> 0.000954).  Saving model ...\nLoss decreased (0.000954 --> 0.000954).  Saving model ...\nLoss decreased (0.000954 --> 0.000954).  Saving model ...\nLoss decreased (0.000954 --> 0.000954).  Saving model ...\nLoss decreased (0.000954 --> 0.000953).  Saving model ...\nLoss decreased (0.000953 --> 0.000953).  Saving model ...\nLoss decreased (0.000953 --> 0.000953).  Saving model ...\nLoss decreased (0.000953 --> 0.000953).  Saving model ...\nLoss decreased (0.000953 --> 0.000953).  Saving model ...\nLoss decreased (0.000953 --> 0.000952).  Saving model ...\nLoss decreased (0.000952 --> 0.000952).  Saving model ...\nLoss decreased (0.000952 --> 0.000952).  Saving model ...\nLoss decreased (0.000952 --> 0.000952).  Saving model ...\nLoss decreased (0.000952 --> 0.000951).  Saving model ...\nLoss decreased (0.000951 --> 0.000951).  Saving model ...\nLoss decreased (0.000951 --> 0.000951).  Saving model ...\nLoss decreased (0.000951 --> 0.000951).  Saving model ...\nLoss decreased (0.000951 --> 0.000951).  Saving model ...\nLoss decreased (0.000951 --> 0.000950).  Saving model ...\nLoss decreased (0.000950 --> 0.000950).  Saving model ...\nLoss decreased (0.000950 --> 0.000950).  Saving model ...\nLoss decreased (0.000950 --> 0.000950).  Saving model ...\nLoss decreased (0.000950 --> 0.000950).  Saving model ...\nLoss decreased (0.000950 --> 0.000949).  Saving model ...\nLoss decreased (0.000949 --> 0.000949).  Saving model ...\nLoss decreased (0.000949 --> 0.000949).  Saving model ...\nLoss decreased (0.000949 --> 0.000949).  Saving model ...\nLoss decreased (0.000949 --> 0.000948).  Saving model ...\nLoss decreased (0.000948 --> 0.000948).  Saving model ...\nLoss decreased (0.000948 --> 0.000948).  Saving model ...\nLoss decreased (0.000948 --> 0.000948).  Saving model ...\nLoss decreased (0.000948 --> 0.000948).  Saving model ...\nLoss decreased (0.000948 --> 0.000947).  Saving model ...\nLoss decreased (0.000947 --> 0.000947).  Saving model ...\nLoss decreased (0.000947 --> 0.000947).  Saving model ...\nLoss decreased (0.000947 --> 0.000947).  Saving model ...\nLoss decreased (0.000947 --> 0.000947).  Saving model ...\nLoss decreased (0.000947 --> 0.000946).  Saving model ...\nLoss decreased (0.000946 --> 0.000946).  Saving model ...\nLoss decreased (0.000946 --> 0.000946).  Saving model ...\nEpoch: 1300, loss: 0.00095\nLoss decreased (0.000946 --> 0.000946).  Saving model ...\nLoss decreased (0.000946 --> 0.000946).  Saving model ...\nLoss decreased (0.000946 --> 0.000945).  Saving model ...\nLoss decreased (0.000945 --> 0.000945).  Saving model ...\nLoss decreased (0.000945 --> 0.000945).  Saving model ...\nLoss decreased (0.000945 --> 0.000945).  Saving model ...\nLoss decreased (0.000945 --> 0.000945).  Saving model ...\nLoss decreased (0.000945 --> 0.000944).  Saving model ...\nLoss decreased (0.000944 --> 0.000944).  Saving model ...\nLoss decreased (0.000944 --> 0.000944).  Saving model ...\nLoss decreased (0.000944 --> 0.000944).  Saving model ...\nLoss decreased (0.000944 --> 0.000944).  Saving model ...\nLoss decreased (0.000944 --> 0.000944).  Saving model ...\nLoss decreased (0.000944 --> 0.000941).  Saving model ...\nLoss decreased (0.000941 --> 0.000939).  Saving model ...\nLoss decreased (0.000939 --> 0.000938).  Saving model ...\nLoss decreased (0.000938 --> 0.000937).  Saving model ...\nLoss decreased (0.000937 --> 0.000936).  Saving model ...\nLoss decreased (0.000936 --> 0.000935).  Saving model ...\nLoss decreased (0.000935 --> 0.000935).  Saving model ...\nLoss decreased (0.000935 --> 0.000934).  Saving model ...\nLoss decreased (0.000934 --> 0.000933).  Saving model ...\nLoss decreased (0.000933 --> 0.000933).  Saving model ...\nEpoch: 1400, loss: 0.00102\nLoss decreased (0.000933 --> 0.000930).  Saving model ...\nLoss decreased (0.000930 --> 0.000929).  Saving model ...\nLoss decreased (0.000929 --> 0.000928).  Saving model ...\nLoss decreased (0.000928 --> 0.000927).  Saving model ...\nLoss decreased (0.000927 --> 0.000925).  Saving model ...\nLoss decreased (0.000925 --> 0.000923).  Saving model ...\nLoss decreased (0.000923 --> 0.000923).  Saving model ...\nLoss decreased (0.000923 --> 0.000922).  Saving model ...\nLoss decreased (0.000922 --> 0.000922).  Saving model ...\nLoss decreased (0.000922 --> 0.000921).  Saving model ...\nLoss decreased (0.000921 --> 0.000921).  Saving model ...\nLoss decreased (0.000921 --> 0.000921).  Saving model ...\nLoss decreased (0.000921 --> 0.000920).  Saving model ...\nLoss decreased (0.000920 --> 0.000920).  Saving model ...\nLoss decreased (0.000920 --> 0.000920).  Saving model ...\nLoss decreased (0.000920 --> 0.000920).  Saving model ...\nLoss decreased (0.000920 --> 0.000919).  Saving model ...\nLoss decreased (0.000919 --> 0.000919).  Saving model ...\nLoss decreased (0.000919 --> 0.000918).  Saving model ...\nLoss decreased (0.000918 --> 0.000917).  Saving model ...\nEpoch: 1500, loss: 0.00092\nLoss decreased (0.000917 --> 0.000916).  Saving model ...\nLoss decreased (0.000916 --> 0.000914).  Saving model ...\nLoss decreased (0.000914 --> 0.000913).  Saving model ...\nLoss decreased (0.000913 --> 0.000912).  Saving model ...\nLoss decreased (0.000912 --> 0.000911).  Saving model ...\nLoss decreased (0.000911 --> 0.000910).  Saving model ...\nLoss decreased (0.000910 --> 0.000910).  Saving model ...\nLoss decreased (0.000910 --> 0.000909).  Saving model ...\nLoss decreased (0.000909 --> 0.000909).  Saving model ...\nLoss decreased (0.000909 --> 0.000909).  Saving model ...\nLoss decreased (0.000909 --> 0.000908).  Saving model ...\nLoss decreased (0.000908 --> 0.000908).  Saving model ...\nLoss decreased (0.000908 --> 0.000908).  Saving model ...\nLoss decreased (0.000908 --> 0.000908).  Saving model ...\nLoss decreased (0.000908 --> 0.000908).  Saving model ...\nLoss decreased (0.000908 --> 0.000908).  Saving model ...\nLoss decreased (0.000908 --> 0.000908).  Saving model ...\nLoss decreased (0.000908 --> 0.000907).  Saving model ...\nLoss decreased (0.000907 --> 0.000906).  Saving model ...\nLoss decreased (0.000906 --> 0.000906).  Saving model ...\nLoss decreased (0.000906 --> 0.000904).  Saving model ...\nEpoch: 1600, loss: 0.00092\nLoss decreased (0.000904 --> 0.000902).  Saving model ...\nLoss decreased (0.000902 --> 0.000902).  Saving model ...\nLoss decreased (0.000902 --> 0.000900).  Saving model ...\nLoss decreased (0.000900 --> 0.000899).  Saving model ...\nLoss decreased (0.000899 --> 0.000899).  Saving model ...\nLoss decreased (0.000899 --> 0.000899).  Saving model ...\nLoss decreased (0.000899 --> 0.000898).  Saving model ...\nLoss decreased (0.000898 --> 0.000898).  Saving model ...\nLoss decreased (0.000898 --> 0.000897).  Saving model ...\nLoss decreased (0.000897 --> 0.000897).  Saving model ...\nLoss decreased (0.000897 --> 0.000897).  Saving model ...\nLoss decreased (0.000897 --> 0.000897).  Saving model ...\nLoss decreased (0.000897 --> 0.000895).  Saving model ...\nLoss decreased (0.000895 --> 0.000893).  Saving model ...\nLoss decreased (0.000893 --> 0.000892).  Saving model ...\nLoss decreased (0.000892 --> 0.000891).  Saving model ...\nLoss decreased (0.000891 --> 0.000891).  Saving model ...\nEpoch: 1700, loss: 0.00090\nLoss decreased (0.000891 --> 0.000890).  Saving model ...\nLoss decreased (0.000890 --> 0.000889).  Saving model ...\nLoss decreased (0.000889 --> 0.000889).  Saving model ...\nLoss decreased (0.000889 --> 0.000888).  Saving model ...\nLoss decreased (0.000888 --> 0.000888).  Saving model ...\nLoss decreased (0.000888 --> 0.000888).  Saving model ...\nLoss decreased (0.000888 --> 0.000887).  Saving model ...\nLoss decreased (0.000887 --> 0.000887).  Saving model ...\nLoss decreased (0.000887 --> 0.000887).  Saving model ...\nLoss decreased (0.000887 --> 0.000886).  Saving model ...\nLoss decreased (0.000886 --> 0.000886).  Saving model ...\nLoss decreased (0.000886 --> 0.000886).  Saving model ...\nLoss decreased (0.000886 --> 0.000886).  Saving model ...\nLoss decreased (0.000886 --> 0.000886).  Saving model ...\nLoss decreased (0.000886 --> 0.000884).  Saving model ...\nLoss decreased (0.000884 --> 0.000882).  Saving model ...\nLoss decreased (0.000882 --> 0.000882).  Saving model ...\nLoss decreased (0.000882 --> 0.000880).  Saving model ...\nLoss decreased (0.000880 --> 0.000880).  Saving model ...\nLoss decreased (0.000880 --> 0.000879).  Saving model ...\nLoss decreased (0.000879 --> 0.000878).  Saving model ...\nLoss decreased (0.000878 --> 0.000878).  Saving model ...\nEpoch: 1800, loss: 0.00088\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000878 --> 0.000876).  Saving model ...\nLoss decreased (0.000876 --> 0.000875).  Saving model ...\nLoss decreased (0.000875 --> 0.000873).  Saving model ...\nLoss decreased (0.000873 --> 0.000873).  Saving model ...\nLoss decreased (0.000873 --> 0.000871).  Saving model ...\nLoss decreased (0.000871 --> 0.000870).  Saving model ...\nLoss decreased (0.000870 --> 0.000870).  Saving model ...\nLoss decreased (0.000870 --> 0.000869).  Saving model ...\nEpoch: 1900, loss: 0.00133\nLoss decreased (0.000869 --> 0.000868).  Saving model ...\nLoss decreased (0.000868 --> 0.000867).  Saving model ...\nLoss decreased (0.000867 --> 0.000866).  Saving model ...\nLoss decreased (0.000866 --> 0.000866).  Saving model ...\nLoss decreased (0.000866 --> 0.000865).  Saving model ...\nLoss decreased (0.000865 --> 0.000864).  Saving model ...\nLoss decreased (0.000864 --> 0.000864).  Saving model ...\nLoss decreased (0.000864 --> 0.000864).  Saving model ...\nLoss decreased (0.000864 --> 0.000863).  Saving model ...\nLoss decreased (0.000863 --> 0.000863).  Saving model ...\nLoss decreased (0.000863 --> 0.000863).  Saving model ...\nLoss decreased (0.000863 --> 0.000862).  Saving model ...\nLoss decreased (0.000862 --> 0.000862).  Saving model ...\nLoss decreased (0.000862 --> 0.000862).  Saving model ...\nLoss decreased (0.000862 --> 0.000862).  Saving model ...\nLoss decreased (0.000862 --> 0.000862).  Saving model ...\nLoss decreased (0.000862 --> 0.000862).  Saving model ...\nEpoch: 2000, loss: 0.00094\nLoss decreased (0.000862 --> 0.000860).  Saving model ...\nLoss decreased (0.000860 --> 0.000860).  Saving model ...\nLoss decreased (0.000860 --> 0.000858).  Saving model ...\nLoss decreased (0.000858 --> 0.000858).  Saving model ...\nLoss decreased (0.000858 --> 0.000858).  Saving model ...\nLoss decreased (0.000858 --> 0.000856).  Saving model ...\nLoss decreased (0.000856 --> 0.000855).  Saving model ...\nLoss decreased (0.000855 --> 0.000854).  Saving model ...\nLoss decreased (0.000854 --> 0.000854).  Saving model ...\nLoss decreased (0.000854 --> 0.000853).  Saving model ...\nLoss decreased (0.000853 --> 0.000853).  Saving model ...\nLoss decreased (0.000853 --> 0.000853).  Saving model ...\nEpoch: 2100, loss: 0.00091\nLoss decreased (0.000853 --> 0.000852).  Saving model ...\nLoss decreased (0.000852 --> 0.000851).  Saving model ...\nLoss decreased (0.000851 --> 0.000851).  Saving model ...\nLoss decreased (0.000851 --> 0.000850).  Saving model ...\nLoss decreased (0.000850 --> 0.000850).  Saving model ...\nLoss decreased (0.000850 --> 0.000849).  Saving model ...\nLoss decreased (0.000849 --> 0.000849).  Saving model ...\nLoss decreased (0.000849 --> 0.000848).  Saving model ...\nLoss decreased (0.000848 --> 0.000848).  Saving model ...\nLoss decreased (0.000848 --> 0.000848).  Saving model ...\nLoss decreased (0.000848 --> 0.000848).  Saving model ...\nLoss decreased (0.000848 --> 0.000847).  Saving model ...\nLoss decreased (0.000847 --> 0.000847).  Saving model ...\nLoss decreased (0.000847 --> 0.000847).  Saving model ...\nEpoch: 2200, loss: 0.00091\nLoss decreased (0.000847 --> 0.000845).  Saving model ...\nLoss decreased (0.000845 --> 0.000844).  Saving model ...\nLoss decreased (0.000844 --> 0.000843).  Saving model ...\nLoss decreased (0.000843 --> 0.000843).  Saving model ...\nLoss decreased (0.000843 --> 0.000843).  Saving model ...\nLoss decreased (0.000843 --> 0.000842).  Saving model ...\nLoss decreased (0.000842 --> 0.000842).  Saving model ...\nLoss decreased (0.000842 --> 0.000842).  Saving model ...\nLoss decreased (0.000842 --> 0.000842).  Saving model ...\nLoss decreased (0.000842 --> 0.000841).  Saving model ...\nLoss decreased (0.000841 --> 0.000841).  Saving model ...\nLoss decreased (0.000841 --> 0.000841).  Saving model ...\nLoss decreased (0.000841 --> 0.000841).  Saving model ...\nLoss decreased (0.000841 --> 0.000841).  Saving model ...\nLoss decreased (0.000841 --> 0.000841).  Saving model ...\nLoss decreased (0.000841 --> 0.000841).  Saving model ...\nLoss decreased (0.000841 --> 0.000841).  Saving model ...\nLoss decreased (0.000841 --> 0.000841).  Saving model ...\nEpoch: 2300, loss: 0.00085\nLoss decreased (0.000841 --> 0.000840).  Saving model ...\nLoss decreased (0.000840 --> 0.000840).  Saving model ...\nLoss decreased (0.000840 --> 0.000839).  Saving model ...\nLoss decreased (0.000839 --> 0.000838).  Saving model ...\nLoss decreased (0.000838 --> 0.000838).  Saving model ...\nLoss decreased (0.000838 --> 0.000838).  Saving model ...\nLoss decreased (0.000838 --> 0.000837).  Saving model ...\nLoss decreased (0.000837 --> 0.000837).  Saving model ...\nLoss decreased (0.000837 --> 0.000837).  Saving model ...\nLoss decreased (0.000837 --> 0.000836).  Saving model ...\nLoss decreased (0.000836 --> 0.000836).  Saving model ...\nLoss decreased (0.000836 --> 0.000836).  Saving model ...\nLoss decreased (0.000836 --> 0.000836).  Saving model ...\nLoss decreased (0.000836 --> 0.000836).  Saving model ...\nLoss decreased (0.000836 --> 0.000835).  Saving model ...\nLoss decreased (0.000835 --> 0.000834).  Saving model ...\nEpoch: 2400, loss: 0.00086\nLoss decreased (0.000834 --> 0.000833).  Saving model ...\nLoss decreased (0.000833 --> 0.000833).  Saving model ...\nLoss decreased (0.000833 --> 0.000832).  Saving model ...\nLoss decreased (0.000832 --> 0.000832).  Saving model ...\nLoss decreased (0.000832 --> 0.000831).  Saving model ...\nLoss decreased (0.000831 --> 0.000831).  Saving model ...\nLoss decreased (0.000831 --> 0.000831).  Saving model ...\nLoss decreased (0.000831 --> 0.000831).  Saving model ...\nLoss decreased (0.000831 --> 0.000831).  Saving model ...\nLoss decreased (0.000831 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nLoss decreased (0.000830 --> 0.000830).  Saving model ...\nEpoch: 2500, loss: 0.00085\nLoss decreased (0.000830 --> 0.000829).  Saving model ...\nLoss decreased (0.000829 --> 0.000829).  Saving model ...\nLoss decreased (0.000829 --> 0.000828).  Saving model ...\nLoss decreased (0.000828 --> 0.000828).  Saving model ...\nLoss decreased (0.000828 --> 0.000828).  Saving model ...\nLoss decreased (0.000828 --> 0.000827).  Saving model ...\nLoss decreased (0.000827 --> 0.000827).  Saving model ...\nLoss decreased (0.000827 --> 0.000827).  Saving model ...\nLoss decreased (0.000827 --> 0.000826).  Saving model ...\nLoss decreased (0.000826 --> 0.000826).  Saving model ...\nLoss decreased (0.000826 --> 0.000826).  Saving model ...\nLoss decreased (0.000826 --> 0.000826).  Saving model ...\nLoss decreased (0.000826 --> 0.000826).  Saving model ...\nLoss decreased (0.000826 --> 0.000826).  Saving model ...\nLoss decreased (0.000826 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000825).  Saving model ...\nLoss decreased (0.000825 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000824).  Saving model ...\nLoss decreased (0.000824 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000823).  Saving model ...\nLoss decreased (0.000823 --> 0.000822).  Saving model ...\nLoss decreased (0.000822 --> 0.000822).  Saving model ...\nEpoch: 2600, loss: 0.00082\nLoss decreased (0.000822 --> 0.000822).  Saving model ...\nLoss decreased (0.000822 --> 0.000821).  Saving model ...\nLoss decreased (0.000821 --> 0.000820).  Saving model ...\nLoss decreased (0.000820 --> 0.000820).  Saving model ...\nLoss decreased (0.000820 --> 0.000819).  Saving model ...\nLoss decreased (0.000819 --> 0.000819).  Saving model ...\nLoss decreased (0.000819 --> 0.000819).  Saving model ...\nLoss decreased (0.000819 --> 0.000819).  Saving model ...\nLoss decreased (0.000819 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nEpoch: 2700, loss: 0.00082\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000818).  Saving model ...\nLoss decreased (0.000818 --> 0.000817).  Saving model ...\nLoss decreased (0.000817 --> 0.000817).  Saving model ...\nLoss decreased (0.000817 --> 0.000816).  Saving model ...\nLoss decreased (0.000816 --> 0.000816).  Saving model ...\nLoss decreased (0.000816 --> 0.000816).  Saving model ...\nLoss decreased (0.000816 --> 0.000815).  Saving model ...\nLoss decreased (0.000815 --> 0.000815).  Saving model ...\nLoss decreased (0.000815 --> 0.000815).  Saving model ...\nLoss decreased (0.000815 --> 0.000814).  Saving model ...\nLoss decreased (0.000814 --> 0.000814).  Saving model ...\nLoss decreased (0.000814 --> 0.000814).  Saving model ...\nLoss decreased (0.000814 --> 0.000814).  Saving model ...\nEpoch: 2800, loss: 0.00081\nLoss decreased (0.000814 --> 0.000814).  Saving model ...\nLoss decreased (0.000814 --> 0.000814).  Saving model ...\nLoss decreased (0.000814 --> 0.000814).  Saving model ...\nLoss decreased (0.000814 --> 0.000814).  Saving model ...\nLoss decreased (0.000814 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000813).  Saving model ...\nLoss decreased (0.000813 --> 0.000812).  Saving model ...\nLoss decreased (0.000812 --> 0.000811).  Saving model ...\nLoss decreased (0.000811 --> 0.000811).  Saving model ...\nLoss decreased (0.000811 --> 0.000810).  Saving model ...\nLoss decreased (0.000810 --> 0.000810).  Saving model ...\nLoss decreased (0.000810 --> 0.000810).  Saving model ...\nEpoch: 2900, loss: 0.00081\nLoss decreased (0.000810 --> 0.000809).  Saving model ...\nLoss decreased (0.000809 --> 0.000809).  Saving model ...\nLoss decreased (0.000809 --> 0.000808).  Saving model ...\nLoss decreased (0.000808 --> 0.000807).  Saving model ...\nLoss decreased (0.000807 --> 0.000807).  Saving model ...\nLoss decreased (0.000807 --> 0.000807).  Saving model ...\nLoss decreased (0.000807 --> 0.000807).  Saving model ...\nLoss decreased (0.000807 --> 0.000806).  Saving model ...\nLoss decreased (0.000806 --> 0.000806).  Saving model ...\nLoss decreased (0.000806 --> 0.000806).  Saving model ...\nLoss decreased (0.000806 --> 0.000806).  Saving model ...\nLoss decreased (0.000806 --> 0.000806).  Saving model ...\nEpoch: 3000, loss: 0.00082\nLoss decreased (0.000806 --> 0.000806).  Saving model ...\nLoss decreased (0.000806 --> 0.000805).  Saving model ...\nLoss decreased (0.000805 --> 0.000805).  Saving model ...\nLoss decreased (0.000805 --> 0.000805).  Saving model ...\nLoss decreased (0.000805 --> 0.000804).  Saving model ...\nLoss decreased (0.000804 --> 0.000804).  Saving model ...\nLoss decreased (0.000804 --> 0.000804).  Saving model ...\nLoss decreased (0.000804 --> 0.000804).  Saving model ...\nLoss decreased (0.000804 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000803).  Saving model ...\nLoss decreased (0.000803 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nEpoch: 3100, loss: 0.00080\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000802).  Saving model ...\nLoss decreased (0.000802 --> 0.000801).  Saving model ...\nLoss decreased (0.000801 --> 0.000801).  Saving model ...\nLoss decreased (0.000801 --> 0.000801).  Saving model ...\nLoss decreased (0.000801 --> 0.000801).  Saving model ...\nLoss decreased (0.000801 --> 0.000800).  Saving model ...\nLoss decreased (0.000800 --> 0.000800).  Saving model ...\nLoss decreased (0.000800 --> 0.000799).  Saving model ...\nEpoch: 3200, loss: 0.00080\nLoss decreased (0.000799 --> 0.000799).  Saving model ...\nLoss decreased (0.000799 --> 0.000799).  Saving model ...\nLoss decreased (0.000799 --> 0.000799).  Saving model ...\nLoss decreased (0.000799 --> 0.000799).  Saving model ...\nLoss decreased (0.000799 --> 0.000799).  Saving model ...\nLoss decreased (0.000799 --> 0.000799).  Saving model ...\nLoss decreased (0.000799 --> 0.000799).  Saving model ...\nLoss decreased (0.000799 --> 0.000799).  Saving model ...\nLoss decreased (0.000799 --> 0.000799).  Saving model ...\nLoss decreased (0.000799 --> 0.000798).  Saving model ...\nLoss decreased (0.000798 --> 0.000798).  Saving model ...\nLoss decreased (0.000798 --> 0.000798).  Saving model ...\nLoss decreased (0.000798 --> 0.000798).  Saving model ...\nLoss decreased (0.000798 --> 0.000798).  Saving model ...\nLoss decreased (0.000798 --> 0.000798).  Saving model ...\nLoss decreased (0.000798 --> 0.000798).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000798 --> 0.000798).  Saving model ...\nLoss decreased (0.000798 --> 0.000797).  Saving model ...\nLoss decreased (0.000797 --> 0.000797).  Saving model ...\nLoss decreased (0.000797 --> 0.000797).  Saving model ...\nEpoch: 3300, loss: 0.00080\nLoss decreased (0.000797 --> 0.000796).  Saving model ...\nLoss decreased (0.000796 --> 0.000796).  Saving model ...\nLoss decreased (0.000796 --> 0.000796).  Saving model ...\nLoss decreased (0.000796 --> 0.000796).  Saving model ...\nLoss decreased (0.000796 --> 0.000795).  Saving model ...\nLoss decreased (0.000795 --> 0.000795).  Saving model ...\nLoss decreased (0.000795 --> 0.000795).  Saving model ...\nLoss decreased (0.000795 --> 0.000795).  Saving model ...\nLoss decreased (0.000795 --> 0.000795).  Saving model ...\nLoss decreased (0.000795 --> 0.000794).  Saving model ...\nLoss decreased (0.000794 --> 0.000794).  Saving model ...\nLoss decreased (0.000794 --> 0.000793).  Saving model ...\nEpoch: 3400, loss: 0.00080\nLoss decreased (0.000793 --> 0.000793).  Saving model ...\nLoss decreased (0.000793 --> 0.000792).  Saving model ...\nLoss decreased (0.000792 --> 0.000792).  Saving model ...\nLoss decreased (0.000792 --> 0.000791).  Saving model ...\nLoss decreased (0.000791 --> 0.000791).  Saving model ...\nLoss decreased (0.000791 --> 0.000791).  Saving model ...\nLoss decreased (0.000791 --> 0.000791).  Saving model ...\nEpoch: 3500, loss: 0.00080\nLoss decreased (0.000791 --> 0.000790).  Saving model ...\nLoss decreased (0.000790 --> 0.000790).  Saving model ...\nLoss decreased (0.000790 --> 0.000789).  Saving model ...\nLoss decreased (0.000789 --> 0.000789).  Saving model ...\nLoss decreased (0.000789 --> 0.000789).  Saving model ...\nLoss decreased (0.000789 --> 0.000789).  Saving model ...\nEpoch: 3600, loss: 0.00081\nLoss decreased (0.000789 --> 0.000787).  Saving model ...\nLoss decreased (0.000787 --> 0.000787).  Saving model ...\nLoss decreased (0.000787 --> 0.000787).  Saving model ...\nLoss decreased (0.000787 --> 0.000786).  Saving model ...\nLoss decreased (0.000786 --> 0.000786).  Saving model ...\nLoss decreased (0.000786 --> 0.000785).  Saving model ...\nEpoch: 3700, loss: 0.00093\nLoss decreased (0.000785 --> 0.000784).  Saving model ...\nLoss decreased (0.000784 --> 0.000784).  Saving model ...\nLoss decreased (0.000784 --> 0.000784).  Saving model ...\nLoss decreased (0.000784 --> 0.000784).  Saving model ...\nLoss decreased (0.000784 --> 0.000783).  Saving model ...\nLoss decreased (0.000783 --> 0.000783).  Saving model ...\nLoss decreased (0.000783 --> 0.000783).  Saving model ...\nLoss decreased (0.000783 --> 0.000783).  Saving model ...\nLoss decreased (0.000783 --> 0.000783).  Saving model ...\nLoss decreased (0.000783 --> 0.000783).  Saving model ...\nLoss decreased (0.000783 --> 0.000783).  Saving model ...\nEpoch: 3800, loss: 0.00079\nLoss decreased (0.000783 --> 0.000783).  Saving model ...\nLoss decreased (0.000783 --> 0.000783).  Saving model ...\nLoss decreased (0.000783 --> 0.000782).  Saving model ...\nLoss decreased (0.000782 --> 0.000782).  Saving model ...\nLoss decreased (0.000782 --> 0.000782).  Saving model ...\nLoss decreased (0.000782 --> 0.000782).  Saving model ...\nLoss decreased (0.000782 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000781).  Saving model ...\nLoss decreased (0.000781 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nLoss decreased (0.000780 --> 0.000780).  Saving model ...\nEpoch: 3900, loss: 0.00104\nLoss decreased (0.000780 --> 0.000779).  Saving model ...\nLoss decreased (0.000779 --> 0.000779).  Saving model ...\nLoss decreased (0.000779 --> 0.000779).  Saving model ...\nLoss decreased (0.000779 --> 0.000779).  Saving model ...\nLoss decreased (0.000779 --> 0.000778).  Saving model ...\nLoss decreased (0.000778 --> 0.000778).  Saving model ...\nLoss decreased (0.000778 --> 0.000778).  Saving model ...\nLoss decreased (0.000778 --> 0.000778).  Saving model ...\nLoss decreased (0.000778 --> 0.000778).  Saving model ...\nLoss decreased (0.000778 --> 0.000778).  Saving model ...\nLoss decreased (0.000778 --> 0.000778).  Saving model ...\nEpoch: 4000, loss: 0.00078\nLoss decreased (0.000778 --> 0.000778).  Saving model ...\nLoss decreased (0.000778 --> 0.000777).  Saving model ...\nLoss decreased (0.000777 --> 0.000777).  Saving model ...\nLoss decreased (0.000777 --> 0.000776).  Saving model ...\nEpoch: 4100, loss: 0.00078\nLoss decreased (0.000776 --> 0.000776).  Saving model ...\nLoss decreased (0.000776 --> 0.000775).  Saving model ...\nLoss decreased (0.000775 --> 0.000775).  Saving model ...\nLoss decreased (0.000775 --> 0.000775).  Saving model ...\nLoss decreased (0.000775 --> 0.000774).  Saving model ...\nEpoch: 4200, loss: 0.00078\nLoss decreased (0.000774 --> 0.000773).  Saving model ...\nLoss decreased (0.000773 --> 0.000773).  Saving model ...\nLoss decreased (0.000773 --> 0.000773).  Saving model ...\nLoss decreased (0.000773 --> 0.000772).  Saving model ...\nLoss decreased (0.000772 --> 0.000772).  Saving model ...\nLoss decreased (0.000772 --> 0.000772).  Saving model ...\nLoss decreased (0.000772 --> 0.000772).  Saving model ...\nLoss decreased (0.000772 --> 0.000772).  Saving model ...\nLoss decreased (0.000772 --> 0.000772).  Saving model ...\nLoss decreased (0.000772 --> 0.000772).  Saving model ...\nEpoch: 4300, loss: 0.00095\nLoss decreased (0.000772 --> 0.000771).  Saving model ...\nLoss decreased (0.000771 --> 0.000770).  Saving model ...\nLoss decreased (0.000770 --> 0.000770).  Saving model ...\nLoss decreased (0.000770 --> 0.000770).  Saving model ...\nLoss decreased (0.000770 --> 0.000770).  Saving model ...\nLoss decreased (0.000770 --> 0.000770).  Saving model ...\nLoss decreased (0.000770 --> 0.000770).  Saving model ...\nLoss decreased (0.000770 --> 0.000769).  Saving model ...\nLoss decreased (0.000769 --> 0.000769).  Saving model ...\nLoss decreased (0.000769 --> 0.000769).  Saving model ...\nLoss decreased (0.000769 --> 0.000769).  Saving model ...\nLoss decreased (0.000769 --> 0.000769).  Saving model ...\nEpoch: 4400, loss: 0.00085\nLoss decreased (0.000769 --> 0.000769).  Saving model ...\nLoss decreased (0.000769 --> 0.000769).  Saving model ...\nLoss decreased (0.000769 --> 0.000769).  Saving model ...\nLoss decreased (0.000769 --> 0.000769).  Saving model ...\nLoss decreased (0.000769 --> 0.000768).  Saving model ...\nLoss decreased (0.000768 --> 0.000768).  Saving model ...\nLoss decreased (0.000768 --> 0.000768).  Saving model ...\nLoss decreased (0.000768 --> 0.000768).  Saving model ...\nLoss decreased (0.000768 --> 0.000768).  Saving model ...\nLoss decreased (0.000768 --> 0.000767).  Saving model ...\nLoss decreased (0.000767 --> 0.000767).  Saving model ...\nLoss decreased (0.000767 --> 0.000767).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 4500, loss: 0.00082\nLoss decreased (0.000767 --> 0.000767).  Saving model ...\nLoss decreased (0.000767 --> 0.000766).  Saving model ...\nLoss decreased (0.000766 --> 0.000766).  Saving model ...\nLoss decreased (0.000766 --> 0.000766).  Saving model ...\nLoss decreased (0.000766 --> 0.000766).  Saving model ...\nEpoch: 4600, loss: 0.00107\nLoss decreased (0.000766 --> 0.000766).  Saving model ...\nLoss decreased (0.000766 --> 0.000765).  Saving model ...\nLoss decreased (0.000765 --> 0.000765).  Saving model ...\nLoss decreased (0.000765 --> 0.000765).  Saving model ...\nLoss decreased (0.000765 --> 0.000764).  Saving model ...\nEpoch: 4700, loss: 0.00076\nLoss decreased (0.000764 --> 0.000764).  Saving model ...\nLoss decreased (0.000764 --> 0.000764).  Saving model ...\nLoss decreased (0.000764 --> 0.000763).  Saving model ...\nLoss decreased (0.000763 --> 0.000763).  Saving model ...\nLoss decreased (0.000763 --> 0.000763).  Saving model ...\nLoss decreased (0.000763 --> 0.000763).  Saving model ...\nLoss decreased (0.000763 --> 0.000762).  Saving model ...\nLoss decreased (0.000762 --> 0.000762).  Saving model ...\nEpoch: 4800, loss: 0.00076\nLoss decreased (0.000762 --> 0.000761).  Saving model ...\nLoss decreased (0.000761 --> 0.000761).  Saving model ...\nLoss decreased (0.000761 --> 0.000761).  Saving model ...\nEpoch: 4900, loss: 0.00090\nLoss decreased (0.000761 --> 0.000760).  Saving model ...\nLoss decreased (0.000760 --> 0.000760).  Saving model ...\nLoss decreased (0.000760 --> 0.000759).  Saving model ...\nLoss decreased (0.000759 --> 0.000759).  Saving model ...\nLoss decreased (0.000759 --> 0.000759).  Saving model ...\nLoss decreased (0.000759 --> 0.000759).  Saving model ...\nEpoch: 5000, loss: 0.00077\nLoss decreased (0.000759 --> 0.000759).  Saving model ...\nLoss decreased (0.000759 --> 0.000758).  Saving model ...\nLoss decreased (0.000758 --> 0.000757).  Saving model ...\nEpoch: 5100, loss: 0.00081\nLoss decreased (0.000757 --> 0.000757).  Saving model ...\nLoss decreased (0.000757 --> 0.000756).  Saving model ...\nLoss decreased (0.000756 --> 0.000756).  Saving model ...\nLoss decreased (0.000756 --> 0.000756).  Saving model ...\nEpoch: 5200, loss: 0.00076\nLoss decreased (0.000756 --> 0.000755).  Saving model ...\nLoss decreased (0.000755 --> 0.000755).  Saving model ...\nLoss decreased (0.000755 --> 0.000755).  Saving model ...\nLoss decreased (0.000755 --> 0.000754).  Saving model ...\nLoss decreased (0.000754 --> 0.000754).  Saving model ...\nEpoch: 5300, loss: 0.00083\nLoss decreased (0.000754 --> 0.000754).  Saving model ...\nLoss decreased (0.000754 --> 0.000753).  Saving model ...\nEpoch: 5400, loss: 0.00079\nLoss decreased (0.000753 --> 0.000753).  Saving model ...\nLoss decreased (0.000753 --> 0.000753).  Saving model ...\nLoss decreased (0.000753 --> 0.000752).  Saving model ...\nLoss decreased (0.000752 --> 0.000752).  Saving model ...\nLoss decreased (0.000752 --> 0.000752).  Saving model ...\nLoss decreased (0.000752 --> 0.000752).  Saving model ...\nLoss decreased (0.000752 --> 0.000752).  Saving model ...\nLoss decreased (0.000752 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nEpoch: 5500, loss: 0.00082\nLoss decreased (0.000751 --> 0.000751).  Saving model ...\nLoss decreased (0.000751 --> 0.000750).  Saving model ...\nLoss decreased (0.000750 --> 0.000750).  Saving model ...\nLoss decreased (0.000750 --> 0.000750).  Saving model ...\nLoss decreased (0.000750 --> 0.000750).  Saving model ...\nLoss decreased (0.000750 --> 0.000750).  Saving model ...\nLoss decreased (0.000750 --> 0.000750).  Saving model ...\nLoss decreased (0.000750 --> 0.000750).  Saving model ...\nLoss decreased (0.000750 --> 0.000750).  Saving model ...\nLoss decreased (0.000750 --> 0.000750).  Saving model ...\nEpoch: 5600, loss: 0.00079\nLoss decreased (0.000750 --> 0.000749).  Saving model ...\nLoss decreased (0.000749 --> 0.000749).  Saving model ...\nLoss decreased (0.000749 --> 0.000748).  Saving model ...\nLoss decreased (0.000748 --> 0.000748).  Saving model ...\nLoss decreased (0.000748 --> 0.000748).  Saving model ...\nEpoch: 5700, loss: 0.00076\nLoss decreased (0.000748 --> 0.000747).  Saving model ...\nLoss decreased (0.000747 --> 0.000747).  Saving model ...\nLoss decreased (0.000747 --> 0.000747).  Saving model ...\nEpoch: 5800, loss: 0.00075\nLoss decreased (0.000747 --> 0.000747).  Saving model ...\nLoss decreased (0.000747 --> 0.000747).  Saving model ...\nLoss decreased (0.000747 --> 0.000746).  Saving model ...\nLoss decreased (0.000746 --> 0.000746).  Saving model ...\nLoss decreased (0.000746 --> 0.000746).  Saving model ...\nLoss decreased (0.000746 --> 0.000746).  Saving model ...\nLoss decreased (0.000746 --> 0.000746).  Saving model ...\nLoss decreased (0.000746 --> 0.000746).  Saving model ...\nLoss decreased (0.000746 --> 0.000746).  Saving model ...\nLoss decreased (0.000746 --> 0.000746).  Saving model ...\nEpoch: 5900, loss: 0.00075\nLoss decreased (0.000746 --> 0.000746).  Saving model ...\nLoss decreased (0.000746 --> 0.000746).  Saving model ...\nLoss decreased (0.000746 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000745).  Saving model ...\nLoss decreased (0.000745 --> 0.000744).  Saving model ...\nEpoch: 6000, loss: 0.00075\nLoss decreased (0.000744 --> 0.000744).  Saving model ...\nLoss decreased (0.000744 --> 0.000744).  Saving model ...\nLoss decreased (0.000744 --> 0.000743).  Saving model ...\nLoss decreased (0.000743 --> 0.000743).  Saving model ...\nLoss decreased (0.000743 --> 0.000743).  Saving model ...\nEpoch: 6100, loss: 0.00091\nLoss decreased (0.000743 --> 0.000742).  Saving model ...\nLoss decreased (0.000742 --> 0.000742).  Saving model ...\nLoss decreased (0.000742 --> 0.000742).  Saving model ...\nEpoch: 6200, loss: 0.00074\nLoss decreased (0.000742 --> 0.000741).  Saving model ...\nLoss decreased (0.000741 --> 0.000741).  Saving model ...\nLoss decreased (0.000741 --> 0.000741).  Saving model ...\nLoss decreased (0.000741 --> 0.000741).  Saving model ...\nLoss decreased (0.000741 --> 0.000740).  Saving model ...\nEpoch: 6300, loss: 0.00085\nLoss decreased (0.000740 --> 0.000740).  Saving model ...\nLoss decreased (0.000740 --> 0.000739).  Saving model ...\nLoss decreased (0.000739 --> 0.000739).  Saving model ...\nEpoch: 6400, loss: 0.00077\nLoss decreased (0.000739 --> 0.000738).  Saving model ...\nLoss decreased (0.000738 --> 0.000738).  Saving model ...\nLoss decreased (0.000738 --> 0.000738).  Saving model ...\nLoss decreased (0.000738 --> 0.000737).  Saving model ...\nEpoch: 6500, loss: 0.00081\nLoss decreased (0.000737 --> 0.000737).  Saving model ...\nLoss decreased (0.000737 --> 0.000737).  Saving model ...\nLoss decreased (0.000737 --> 0.000737).  Saving model ...\nLoss decreased (0.000737 --> 0.000736).  Saving model ...\nLoss decreased (0.000736 --> 0.000736).  Saving model ...\nLoss decreased (0.000736 --> 0.000736).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 6600, loss: 0.00083\nLoss decreased (0.000736 --> 0.000736).  Saving model ...\nLoss decreased (0.000736 --> 0.000736).  Saving model ...\nLoss decreased (0.000736 --> 0.000735).  Saving model ...\nLoss decreased (0.000735 --> 0.000735).  Saving model ...\nEpoch: 6700, loss: 0.00075\nLoss decreased (0.000735 --> 0.000735).  Saving model ...\nLoss decreased (0.000735 --> 0.000735).  Saving model ...\nLoss decreased (0.000735 --> 0.000734).  Saving model ...\nLoss decreased (0.000734 --> 0.000734).  Saving model ...\nLoss decreased (0.000734 --> 0.000734).  Saving model ...\nEpoch: 6800, loss: 0.00074\nLoss decreased (0.000734 --> 0.000734).  Saving model ...\nLoss decreased (0.000734 --> 0.000733).  Saving model ...\nLoss decreased (0.000733 --> 0.000733).  Saving model ...\nEpoch: 6900, loss: 0.00078\nLoss decreased (0.000733 --> 0.000733).  Saving model ...\nLoss decreased (0.000733 --> 0.000733).  Saving model ...\nLoss decreased (0.000733 --> 0.000732).  Saving model ...\nLoss decreased (0.000732 --> 0.000732).  Saving model ...\nLoss decreased (0.000732 --> 0.000732).  Saving model ...\nLoss decreased (0.000732 --> 0.000732).  Saving model ...\nEpoch: 7000, loss: 0.00075\nLoss decreased (0.000732 --> 0.000731).  Saving model ...\nLoss decreased (0.000731 --> 0.000731).  Saving model ...\nEpoch: 7100, loss: 0.00075\nLoss decreased (0.000731 --> 0.000731).  Saving model ...\nLoss decreased (0.000731 --> 0.000730).  Saving model ...\nLoss decreased (0.000730 --> 0.000730).  Saving model ...\nLoss decreased (0.000730 --> 0.000730).  Saving model ...\nLoss decreased (0.000730 --> 0.000730).  Saving model ...\nLoss decreased (0.000730 --> 0.000730).  Saving model ...\nLoss decreased (0.000730 --> 0.000730).  Saving model ...\nLoss decreased (0.000730 --> 0.000730).  Saving model ...\nLoss decreased (0.000730 --> 0.000730).  Saving model ...\nLoss decreased (0.000730 --> 0.000729).  Saving model ...\nLoss decreased (0.000729 --> 0.000729).  Saving model ...\nEpoch: 7200, loss: 0.00073\nLoss decreased (0.000729 --> 0.000729).  Saving model ...\nLoss decreased (0.000729 --> 0.000729).  Saving model ...\nLoss decreased (0.000729 --> 0.000729).  Saving model ...\nLoss decreased (0.000729 --> 0.000729).  Saving model ...\nLoss decreased (0.000729 --> 0.000729).  Saving model ...\nLoss decreased (0.000729 --> 0.000729).  Saving model ...\nLoss decreased (0.000729 --> 0.000729).  Saving model ...\nLoss decreased (0.000729 --> 0.000729).  Saving model ...\nLoss decreased (0.000729 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nEpoch: 7300, loss: 0.00074\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000728).  Saving model ...\nLoss decreased (0.000728 --> 0.000727).  Saving model ...\nLoss decreased (0.000727 --> 0.000727).  Saving model ...\nLoss decreased (0.000727 --> 0.000727).  Saving model ...\nLoss decreased (0.000727 --> 0.000727).  Saving model ...\nLoss decreased (0.000727 --> 0.000727).  Saving model ...\nLoss decreased (0.000727 --> 0.000727).  Saving model ...\nLoss decreased (0.000727 --> 0.000727).  Saving model ...\nLoss decreased (0.000727 --> 0.000727).  Saving model ...\nEpoch: 7400, loss: 0.00073\nLoss decreased (0.000727 --> 0.000727).  Saving model ...\nLoss decreased (0.000727 --> 0.000727).  Saving model ...\nLoss decreased (0.000727 --> 0.000726).  Saving model ...\nLoss decreased (0.000726 --> 0.000726).  Saving model ...\nLoss decreased (0.000726 --> 0.000726).  Saving model ...\nLoss decreased (0.000726 --> 0.000726).  Saving model ...\nLoss decreased (0.000726 --> 0.000726).  Saving model ...\nEpoch: 7500, loss: 0.00073\nLoss decreased (0.000726 --> 0.000726).  Saving model ...\nLoss decreased (0.000726 --> 0.000725).  Saving model ...\nLoss decreased (0.000725 --> 0.000725).  Saving model ...\nLoss decreased (0.000725 --> 0.000725).  Saving model ...\nLoss decreased (0.000725 --> 0.000725).  Saving model ...\nLoss decreased (0.000725 --> 0.000724).  Saving model ...\nLoss decreased (0.000724 --> 0.000724).  Saving model ...\nLoss decreased (0.000724 --> 0.000724).  Saving model ...\nEpoch: 7600, loss: 0.00073\nLoss decreased (0.000724 --> 0.000724).  Saving model ...\nLoss decreased (0.000724 --> 0.000724).  Saving model ...\nLoss decreased (0.000724 --> 0.000723).  Saving model ...\nEpoch: 7700, loss: 0.00073\nLoss decreased (0.000723 --> 0.000723).  Saving model ...\nLoss decreased (0.000723 --> 0.000723).  Saving model ...\nLoss decreased (0.000723 --> 0.000723).  Saving model ...\nLoss decreased (0.000723 --> 0.000723).  Saving model ...\nLoss decreased (0.000723 --> 0.000722).  Saving model ...\nEpoch: 7800, loss: 0.00077\nLoss decreased (0.000722 --> 0.000722).  Saving model ...\nLoss decreased (0.000722 --> 0.000722).  Saving model ...\nLoss decreased (0.000722 --> 0.000721).  Saving model ...\nLoss decreased (0.000721 --> 0.000721).  Saving model ...\nLoss decreased (0.000721 --> 0.000721).  Saving model ...\nLoss decreased (0.000721 --> 0.000721).  Saving model ...\nLoss decreased (0.000721 --> 0.000721).  Saving model ...\nEpoch: 7900, loss: 0.00072\nLoss decreased (0.000721 --> 0.000720).  Saving model ...\nLoss decreased (0.000720 --> 0.000720).  Saving model ...\nLoss decreased (0.000720 --> 0.000720).  Saving model ...\nLoss decreased (0.000720 --> 0.000720).  Saving model ...\nLoss decreased (0.000720 --> 0.000720).  Saving model ...\nEpoch: 8000, loss: 0.00082\nLoss decreased (0.000720 --> 0.000719).  Saving model ...\nEpoch: 8100, loss: 0.00073\nLoss decreased (0.000719 --> 0.000719).  Saving model ...\nLoss decreased (0.000719 --> 0.000719).  Saving model ...\nLoss decreased (0.000719 --> 0.000718).  Saving model ...\nLoss decreased (0.000718 --> 0.000718).  Saving model ...\nLoss decreased (0.000718 --> 0.000718).  Saving model ...\nLoss decreased (0.000718 --> 0.000718).  Saving model ...\nLoss decreased (0.000718 --> 0.000718).  Saving model ...\nLoss decreased (0.000718 --> 0.000718).  Saving model ...\nLoss decreased (0.000718 --> 0.000718).  Saving model ...\nLoss decreased (0.000718 --> 0.000718).  Saving model ...\nLoss decreased (0.000718 --> 0.000718).  Saving model ...\nLoss decreased (0.000718 --> 0.000718).  Saving model ...\nEpoch: 8200, loss: 0.00074\nLoss decreased (0.000718 --> 0.000718).  Saving model ...\nLoss decreased (0.000718 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nEpoch: 8300, loss: 0.00072\nLoss decreased (0.000717 --> 0.000717).  Saving model ...\nLoss decreased (0.000717 --> 0.000716).  Saving model ...\nLoss decreased (0.000716 --> 0.000716).  Saving model ...\nLoss decreased (0.000716 --> 0.000716).  Saving model ...\nLoss decreased (0.000716 --> 0.000716).  Saving model ...\nEpoch: 8400, loss: 0.00073\nLoss decreased (0.000716 --> 0.000715).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000715 --> 0.000715).  Saving model ...\nEpoch: 8500, loss: 0.00075\nLoss decreased (0.000715 --> 0.000714).  Saving model ...\nLoss decreased (0.000714 --> 0.000714).  Saving model ...\nLoss decreased (0.000714 --> 0.000714).  Saving model ...\nLoss decreased (0.000714 --> 0.000714).  Saving model ...\nEpoch: 8600, loss: 0.00085\nLoss decreased (0.000714 --> 0.000714).  Saving model ...\nLoss decreased (0.000714 --> 0.000714).  Saving model ...\nLoss decreased (0.000714 --> 0.000713).  Saving model ...\nLoss decreased (0.000713 --> 0.000713).  Saving model ...\nLoss decreased (0.000713 --> 0.000713).  Saving model ...\nEpoch: 8700, loss: 0.00072\nLoss decreased (0.000713 --> 0.000712).  Saving model ...\nLoss decreased (0.000712 --> 0.000712).  Saving model ...\nLoss decreased (0.000712 --> 0.000712).  Saving model ...\nLoss decreased (0.000712 --> 0.000712).  Saving model ...\nLoss decreased (0.000712 --> 0.000711).  Saving model ...\nEpoch: 8800, loss: 0.00072\nLoss decreased (0.000711 --> 0.000711).  Saving model ...\nLoss decreased (0.000711 --> 0.000711).  Saving model ...\nLoss decreased (0.000711 --> 0.000711).  Saving model ...\nLoss decreased (0.000711 --> 0.000710).  Saving model ...\nLoss decreased (0.000710 --> 0.000710).  Saving model ...\nLoss decreased (0.000710 --> 0.000710).  Saving model ...\nLoss decreased (0.000710 --> 0.000710).  Saving model ...\nLoss decreased (0.000710 --> 0.000710).  Saving model ...\nLoss decreased (0.000710 --> 0.000710).  Saving model ...\nEpoch: 8900, loss: 0.00093\nLoss decreased (0.000710 --> 0.000710).  Saving model ...\nLoss decreased (0.000710 --> 0.000710).  Saving model ...\nLoss decreased (0.000710 --> 0.000710).  Saving model ...\nLoss decreased (0.000710 --> 0.000710).  Saving model ...\nLoss decreased (0.000710 --> 0.000710).  Saving model ...\nLoss decreased (0.000710 --> 0.000709).  Saving model ...\nLoss decreased (0.000709 --> 0.000709).  Saving model ...\nLoss decreased (0.000709 --> 0.000709).  Saving model ...\nEpoch: 9000, loss: 0.00073\nLoss decreased (0.000709 --> 0.000709).  Saving model ...\nLoss decreased (0.000709 --> 0.000709).  Saving model ...\nLoss decreased (0.000709 --> 0.000709).  Saving model ...\nLoss decreased (0.000709 --> 0.000709).  Saving model ...\nEpoch: 9100, loss: 0.00073\nLoss decreased (0.000709 --> 0.000708).  Saving model ...\nLoss decreased (0.000708 --> 0.000708).  Saving model ...\nEpoch: 9200, loss: 0.00071\nLoss decreased (0.000708 --> 0.000707).  Saving model ...\nLoss decreased (0.000707 --> 0.000707).  Saving model ...\nLoss decreased (0.000707 --> 0.000707).  Saving model ...\nLoss decreased (0.000707 --> 0.000707).  Saving model ...\nLoss decreased (0.000707 --> 0.000707).  Saving model ...\nEpoch: 9300, loss: 0.00081\nLoss decreased (0.000707 --> 0.000706).  Saving model ...\nLoss decreased (0.000706 --> 0.000706).  Saving model ...\nLoss decreased (0.000706 --> 0.000706).  Saving model ...\nLoss decreased (0.000706 --> 0.000706).  Saving model ...\nLoss decreased (0.000706 --> 0.000706).  Saving model ...\nLoss decreased (0.000706 --> 0.000706).  Saving model ...\nLoss decreased (0.000706 --> 0.000706).  Saving model ...\nEpoch: 9400, loss: 0.00071\nLoss decreased (0.000706 --> 0.000706).  Saving model ...\nLoss decreased (0.000706 --> 0.000705).  Saving model ...\nLoss decreased (0.000705 --> 0.000705).  Saving model ...\nLoss decreased (0.000705 --> 0.000705).  Saving model ...\nEpoch: 9500, loss: 0.00071\nLoss decreased (0.000705 --> 0.000705).  Saving model ...\nLoss decreased (0.000705 --> 0.000705).  Saving model ...\nLoss decreased (0.000705 --> 0.000704).  Saving model ...\nLoss decreased (0.000704 --> 0.000704).  Saving model ...\nLoss decreased (0.000704 --> 0.000704).  Saving model ...\nLoss decreased (0.000704 --> 0.000704).  Saving model ...\nLoss decreased (0.000704 --> 0.000704).  Saving model ...\nLoss decreased (0.000704 --> 0.000704).  Saving model ...\nLoss decreased (0.000704 --> 0.000704).  Saving model ...\nLoss decreased (0.000704 --> 0.000704).  Saving model ...\nEpoch: 9600, loss: 0.00070\nLoss decreased (0.000704 --> 0.000704).  Saving model ...\nLoss decreased (0.000704 --> 0.000703).  Saving model ...\nLoss decreased (0.000703 --> 0.000703).  Saving model ...\nLoss decreased (0.000703 --> 0.000703).  Saving model ...\nLoss decreased (0.000703 --> 0.000703).  Saving model ...\nLoss decreased (0.000703 --> 0.000703).  Saving model ...\nLoss decreased (0.000703 --> 0.000703).  Saving model ...\nLoss decreased (0.000703 --> 0.000703).  Saving model ...\nLoss decreased (0.000703 --> 0.000703).  Saving model ...\nLoss decreased (0.000703 --> 0.000703).  Saving model ...\nEpoch: 9700, loss: 0.00070\nLoss decreased (0.000703 --> 0.000703).  Saving model ...\nLoss decreased (0.000703 --> 0.000702).  Saving model ...\nLoss decreased (0.000702 --> 0.000702).  Saving model ...\nEpoch: 9800, loss: 0.00075\nLoss decreased (0.000702 --> 0.000702).  Saving model ...\nLoss decreased (0.000702 --> 0.000702).  Saving model ...\nLoss decreased (0.000702 --> 0.000702).  Saving model ...\nLoss decreased (0.000702 --> 0.000701).  Saving model ...\nLoss decreased (0.000701 --> 0.000701).  Saving model ...\nEpoch: 9900, loss: 0.00075\nLoss decreased (0.000701 --> 0.000701).  Saving model ...\nLoss decreased (0.000701 --> 0.000700).  Saving model ...\nLoss decreased (0.000700 --> 0.000700).  Saving model ...\nEpoch: 10000, loss: 0.00071\nLoss decreased (0.000700 --> 0.000700).  Saving model ...\nLoss decreased (0.000700 --> 0.000700).  Saving model ...\nLoss decreased (0.000700 --> 0.000699).  Saving model ...\nEpoch: 10100, loss: 0.00074\nLoss decreased (0.000699 --> 0.000699).  Saving model ...\nLoss decreased (0.000699 --> 0.000699).  Saving model ...\nLoss decreased (0.000699 --> 0.000698).  Saving model ...\nEpoch: 10200, loss: 0.00070\nLoss decreased (0.000698 --> 0.000698).  Saving model ...\nLoss decreased (0.000698 --> 0.000698).  Saving model ...\nLoss decreased (0.000698 --> 0.000698).  Saving model ...\nLoss decreased (0.000698 --> 0.000697).  Saving model ...\nEpoch: 10300, loss: 0.00079\nLoss decreased (0.000697 --> 0.000697).  Saving model ...\nLoss decreased (0.000697 --> 0.000697).  Saving model ...\nLoss decreased (0.000697 --> 0.000697).  Saving model ...\nLoss decreased (0.000697 --> 0.000697).  Saving model ...\nLoss decreased (0.000697 --> 0.000696).  Saving model ...\nEpoch: 10400, loss: 0.00070\nLoss decreased (0.000696 --> 0.000696).  Saving model ...\nLoss decreased (0.000696 --> 0.000696).  Saving model ...\nLoss decreased (0.000696 --> 0.000696).  Saving model ...\nEpoch: 10500, loss: 0.00070\nLoss decreased (0.000696 --> 0.000696).  Saving model ...\nLoss decreased (0.000696 --> 0.000695).  Saving model ...\nLoss decreased (0.000695 --> 0.000695).  Saving model ...\nEpoch: 10600, loss: 0.00070\nLoss decreased (0.000695 --> 0.000695).  Saving model ...\nLoss decreased (0.000695 --> 0.000694).  Saving model ...\nLoss decreased (0.000694 --> 0.000694).  Saving model ...\nEpoch: 10700, loss: 0.00069\nLoss decreased (0.000694 --> 0.000694).  Saving model ...\nLoss decreased (0.000694 --> 0.000694).  Saving model ...\nLoss decreased (0.000694 --> 0.000694).  Saving model ...\nLoss decreased (0.000694 --> 0.000694).  Saving model ...\nLoss decreased (0.000694 --> 0.000693).  Saving model ...\nLoss decreased (0.000693 --> 0.000693).  Saving model ...\nEpoch: 10800, loss: 0.00077\nLoss decreased (0.000693 --> 0.000693).  Saving model ...\nLoss decreased (0.000693 --> 0.000693).  Saving model ...\nLoss decreased (0.000693 --> 0.000692).  Saving model ...\nLoss decreased (0.000692 --> 0.000692).  Saving model ...\nEpoch: 10900, loss: 0.00070\nLoss decreased (0.000692 --> 0.000692).  Saving model ...\nLoss decreased (0.000692 --> 0.000692).  Saving model ...\nLoss decreased (0.000692 --> 0.000692).  Saving model ...\nLoss decreased (0.000692 --> 0.000692).  Saving model ...\nLoss decreased (0.000692 --> 0.000691).  Saving model ...\nLoss decreased (0.000691 --> 0.000691).  Saving model ...\nEpoch: 11000, loss: 0.00069\nLoss decreased (0.000691 --> 0.000691).  Saving model ...\nLoss decreased (0.000691 --> 0.000691).  Saving model ...\nLoss decreased (0.000691 --> 0.000691).  Saving model ...\nEpoch: 11100, loss: 0.00070\nLoss decreased (0.000691 --> 0.000691).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000691 --> 0.000690).  Saving model ...\nLoss decreased (0.000690 --> 0.000690).  Saving model ...\nEpoch: 11200, loss: 0.00079\nLoss decreased (0.000690 --> 0.000690).  Saving model ...\nLoss decreased (0.000690 --> 0.000690).  Saving model ...\nLoss decreased (0.000690 --> 0.000689).  Saving model ...\nLoss decreased (0.000689 --> 0.000689).  Saving model ...\nLoss decreased (0.000689 --> 0.000689).  Saving model ...\nLoss decreased (0.000689 --> 0.000689).  Saving model ...\nLoss decreased (0.000689 --> 0.000689).  Saving model ...\nLoss decreased (0.000689 --> 0.000689).  Saving model ...\nEpoch: 11300, loss: 0.00069\nLoss decreased (0.000689 --> 0.000688).  Saving model ...\nLoss decreased (0.000688 --> 0.000688).  Saving model ...\nEpoch: 11400, loss: 0.00069\nLoss decreased (0.000688 --> 0.000688).  Saving model ...\nLoss decreased (0.000688 --> 0.000688).  Saving model ...\nEpoch: 11500, loss: 0.00072\nLoss decreased (0.000688 --> 0.000687).  Saving model ...\nLoss decreased (0.000687 --> 0.000687).  Saving model ...\nLoss decreased (0.000687 --> 0.000687).  Saving model ...\nLoss decreased (0.000687 --> 0.000686).  Saving model ...\nEpoch: 11600, loss: 0.00070\nLoss decreased (0.000686 --> 0.000686).  Saving model ...\nLoss decreased (0.000686 --> 0.000686).  Saving model ...\nEpoch: 11700, loss: 0.00070\nLoss decreased (0.000686 --> 0.000685).  Saving model ...\nLoss decreased (0.000685 --> 0.000685).  Saving model ...\nLoss decreased (0.000685 --> 0.000685).  Saving model ...\nEpoch: 11800, loss: 0.00071\nLoss decreased (0.000685 --> 0.000684).  Saving model ...\nLoss decreased (0.000684 --> 0.000684).  Saving model ...\nLoss decreased (0.000684 --> 0.000684).  Saving model ...\nEpoch: 11900, loss: 0.00069\nLoss decreased (0.000684 --> 0.000684).  Saving model ...\nLoss decreased (0.000684 --> 0.000684).  Saving model ...\nEpoch: 12000, loss: 0.00068\nLoss decreased (0.000684 --> 0.000683).  Saving model ...\nLoss decreased (0.000683 --> 0.000683).  Saving model ...\nLoss decreased (0.000683 --> 0.000683).  Saving model ...\nLoss decreased (0.000683 --> 0.000683).  Saving model ...\nLoss decreased (0.000683 --> 0.000682).  Saving model ...\nLoss decreased (0.000682 --> 0.000682).  Saving model ...\nLoss decreased (0.000682 --> 0.000682).  Saving model ...\nLoss decreased (0.000682 --> 0.000682).  Saving model ...\nEpoch: 12100, loss: 0.00069\nLoss decreased (0.000682 --> 0.000682).  Saving model ...\nLoss decreased (0.000682 --> 0.000682).  Saving model ...\nLoss decreased (0.000682 --> 0.000681).  Saving model ...\nLoss decreased (0.000681 --> 0.000681).  Saving model ...\nLoss decreased (0.000681 --> 0.000681).  Saving model ...\nLoss decreased (0.000681 --> 0.000681).  Saving model ...\nEpoch: 12200, loss: 0.00070\nLoss decreased (0.000681 --> 0.000681).  Saving model ...\nLoss decreased (0.000681 --> 0.000681).  Saving model ...\nLoss decreased (0.000681 --> 0.000680).  Saving model ...\nLoss decreased (0.000680 --> 0.000680).  Saving model ...\nEpoch: 12300, loss: 0.00068\nLoss decreased (0.000680 --> 0.000680).  Saving model ...\nLoss decreased (0.000680 --> 0.000680).  Saving model ...\nEpoch: 12400, loss: 0.00069\nLoss decreased (0.000680 --> 0.000680).  Saving model ...\nLoss decreased (0.000680 --> 0.000679).  Saving model ...\nLoss decreased (0.000679 --> 0.000679).  Saving model ...\nLoss decreased (0.000679 --> 0.000679).  Saving model ...\nLoss decreased (0.000679 --> 0.000679).  Saving model ...\nEpoch: 12500, loss: 0.00068\nLoss decreased (0.000679 --> 0.000678).  Saving model ...\nLoss decreased (0.000678 --> 0.000678).  Saving model ...\nLoss decreased (0.000678 --> 0.000678).  Saving model ...\nEpoch: 12600, loss: 0.00072\nLoss decreased (0.000678 --> 0.000678).  Saving model ...\nLoss decreased (0.000678 --> 0.000677).  Saving model ...\nLoss decreased (0.000677 --> 0.000677).  Saving model ...\nLoss decreased (0.000677 --> 0.000677).  Saving model ...\nLoss decreased (0.000677 --> 0.000677).  Saving model ...\nLoss decreased (0.000677 --> 0.000677).  Saving model ...\nLoss decreased (0.000677 --> 0.000677).  Saving model ...\nLoss decreased (0.000677 --> 0.000677).  Saving model ...\nLoss decreased (0.000677 --> 0.000677).  Saving model ...\nEpoch: 12700, loss: 0.00069\nLoss decreased (0.000677 --> 0.000676).  Saving model ...\nLoss decreased (0.000676 --> 0.000676).  Saving model ...\nEpoch: 12800, loss: 0.00072\nLoss decreased (0.000676 --> 0.000676).  Saving model ...\nLoss decreased (0.000676 --> 0.000676).  Saving model ...\nEpoch: 12900, loss: 0.00068\nLoss decreased (0.000676 --> 0.000675).  Saving model ...\nLoss decreased (0.000675 --> 0.000675).  Saving model ...\nLoss decreased (0.000675 --> 0.000675).  Saving model ...\nLoss decreased (0.000675 --> 0.000675).  Saving model ...\nLoss decreased (0.000675 --> 0.000675).  Saving model ...\nLoss decreased (0.000675 --> 0.000675).  Saving model ...\nEpoch: 13000, loss: 0.00102\nLoss decreased (0.000675 --> 0.000674).  Saving model ...\nLoss decreased (0.000674 --> 0.000674).  Saving model ...\nLoss decreased (0.000674 --> 0.000674).  Saving model ...\nLoss decreased (0.000674 --> 0.000674).  Saving model ...\nLoss decreased (0.000674 --> 0.000674).  Saving model ...\nEpoch: 13100, loss: 0.00069\nLoss decreased (0.000674 --> 0.000673).  Saving model ...\nLoss decreased (0.000673 --> 0.000673).  Saving model ...\nLoss decreased (0.000673 --> 0.000673).  Saving model ...\nEpoch: 13200, loss: 0.00081\nLoss decreased (0.000673 --> 0.000673).  Saving model ...\nLoss decreased (0.000673 --> 0.000672).  Saving model ...\nLoss decreased (0.000672 --> 0.000672).  Saving model ...\nLoss decreased (0.000672 --> 0.000672).  Saving model ...\nEpoch: 13300, loss: 0.00068\nLoss decreased (0.000672 --> 0.000672).  Saving model ...\nLoss decreased (0.000672 --> 0.000672).  Saving model ...\nLoss decreased (0.000672 --> 0.000672).  Saving model ...\nLoss decreased (0.000672 --> 0.000671).  Saving model ...\nLoss decreased (0.000671 --> 0.000671).  Saving model ...\nEpoch: 13400, loss: 0.00067\nLoss decreased (0.000671 --> 0.000671).  Saving model ...\nLoss decreased (0.000671 --> 0.000671).  Saving model ...\nLoss decreased (0.000671 --> 0.000671).  Saving model ...\nLoss decreased (0.000671 --> 0.000671).  Saving model ...\nLoss decreased (0.000671 --> 0.000671).  Saving model ...\nLoss decreased (0.000671 --> 0.000671).  Saving model ...\nLoss decreased (0.000671 --> 0.000670).  Saving model ...\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nEpoch: 13500, loss: 0.00068\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nEpoch: 13600, loss: 0.00069\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nLoss decreased (0.000670 --> 0.000670).  Saving model ...\nLoss decreased (0.000670 --> 0.000669).  Saving model ...\nLoss decreased (0.000669 --> 0.000669).  Saving model ...\nLoss decreased (0.000669 --> 0.000669).  Saving model ...\nLoss decreased (0.000669 --> 0.000669).  Saving model ...\nEpoch: 13700, loss: 0.00067\nLoss decreased (0.000669 --> 0.000669).  Saving model ...\nLoss decreased (0.000669 --> 0.000668).  Saving model ...\nLoss decreased (0.000668 --> 0.000668).  Saving model ...\nEpoch: 13800, loss: 0.00071\nEpoch: 13900, loss: 0.00067\nLoss decreased (0.000668 --> 0.000668).  Saving model ...\nLoss decreased (0.000668 --> 0.000667).  Saving model ...\nLoss decreased (0.000667 --> 0.000667).  Saving model ...\nLoss decreased (0.000667 --> 0.000667).  Saving model ...\nEpoch: 14000, loss: 0.00068\nLoss decreased (0.000667 --> 0.000666).  Saving model ...\nLoss decreased (0.000666 --> 0.000666).  Saving model ...\nEpoch: 14100, loss: 0.00070\nLoss decreased (0.000666 --> 0.000665).  Saving model ...\nLoss decreased (0.000665 --> 0.000665).  Saving model ...\nEpoch: 14200, loss: 0.00070\nLoss decreased (0.000665 --> 0.000665).  Saving model ...\nEpoch: 14300, loss: 0.00067\nLoss decreased (0.000665 --> 0.000664).  Saving model ...\nLoss decreased (0.000664 --> 0.000664).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 14400, loss: 0.00074\nLoss decreased (0.000664 --> 0.000664).  Saving model ...\nLoss decreased (0.000664 --> 0.000664).  Saving model ...\nLoss decreased (0.000664 --> 0.000663).  Saving model ...\nLoss decreased (0.000663 --> 0.000663).  Saving model ...\nLoss decreased (0.000663 --> 0.000663).  Saving model ...\nLoss decreased (0.000663 --> 0.000663).  Saving model ...\nLoss decreased (0.000663 --> 0.000663).  Saving model ...\nEpoch: 14500, loss: 0.00066\nLoss decreased (0.000663 --> 0.000663).  Saving model ...\nLoss decreased (0.000663 --> 0.000662).  Saving model ...\nLoss decreased (0.000662 --> 0.000662).  Saving model ...\nEpoch: 14600, loss: 0.00067\nLoss decreased (0.000662 --> 0.000662).  Saving model ...\nLoss decreased (0.000662 --> 0.000662).  Saving model ...\nLoss decreased (0.000662 --> 0.000661).  Saving model ...\nLoss decreased (0.000661 --> 0.000661).  Saving model ...\nEpoch: 14700, loss: 0.00070\nLoss decreased (0.000661 --> 0.000661).  Saving model ...\nLoss decreased (0.000661 --> 0.000661).  Saving model ...\nLoss decreased (0.000661 --> 0.000660).  Saving model ...\nEpoch: 14800, loss: 0.00066\nLoss decreased (0.000660 --> 0.000660).  Saving model ...\nLoss decreased (0.000660 --> 0.000660).  Saving model ...\nEpoch: 14900, loss: 0.00067\nLoss decreased (0.000660 --> 0.000660).  Saving model ...\nLoss decreased (0.000660 --> 0.000660).  Saving model ...\nLoss decreased (0.000660 --> 0.000659).  Saving model ...\nEpoch: 15000, loss: 0.00068\nLoss decreased (0.000659 --> 0.000659).  Saving model ...\nLoss decreased (0.000659 --> 0.000659).  Saving model ...\nLoss decreased (0.000659 --> 0.000659).  Saving model ...\nLoss decreased (0.000659 --> 0.000658).  Saving model ...\nLoss decreased (0.000658 --> 0.000658).  Saving model ...\nLoss decreased (0.000658 --> 0.000658).  Saving model ...\nLoss decreased (0.000658 --> 0.000658).  Saving model ...\nEpoch: 15100, loss: 0.00066\nLoss decreased (0.000658 --> 0.000658).  Saving model ...\nLoss decreased (0.000658 --> 0.000658).  Saving model ...\nEpoch: 15200, loss: 0.00066\nLoss decreased (0.000658 --> 0.000658).  Saving model ...\nLoss decreased (0.000658 --> 0.000657).  Saving model ...\nEpoch: 15300, loss: 0.00069\nLoss decreased (0.000657 --> 0.000657).  Saving model ...\nLoss decreased (0.000657 --> 0.000657).  Saving model ...\nLoss decreased (0.000657 --> 0.000657).  Saving model ...\nLoss decreased (0.000657 --> 0.000656).  Saving model ...\nEpoch: 15400, loss: 0.00066\nLoss decreased (0.000656 --> 0.000656).  Saving model ...\nLoss decreased (0.000656 --> 0.000656).  Saving model ...\nLoss decreased (0.000656 --> 0.000656).  Saving model ...\nLoss decreased (0.000656 --> 0.000656).  Saving model ...\nLoss decreased (0.000656 --> 0.000656).  Saving model ...\nEpoch: 15500, loss: 0.00066\nLoss decreased (0.000656 --> 0.000655).  Saving model ...\nLoss decreased (0.000655 --> 0.000655).  Saving model ...\nLoss decreased (0.000655 --> 0.000655).  Saving model ...\nLoss decreased (0.000655 --> 0.000655).  Saving model ...\nLoss decreased (0.000655 --> 0.000655).  Saving model ...\nLoss decreased (0.000655 --> 0.000655).  Saving model ...\nLoss decreased (0.000655 --> 0.000655).  Saving model ...\nEpoch: 15600, loss: 0.00069\nLoss decreased (0.000655 --> 0.000655).  Saving model ...\nLoss decreased (0.000655 --> 0.000655).  Saving model ...\nLoss decreased (0.000655 --> 0.000654).  Saving model ...\nLoss decreased (0.000654 --> 0.000654).  Saving model ...\nLoss decreased (0.000654 --> 0.000654).  Saving model ...\nLoss decreased (0.000654 --> 0.000654).  Saving model ...\nLoss decreased (0.000654 --> 0.000654).  Saving model ...\nLoss decreased (0.000654 --> 0.000654).  Saving model ...\nLoss decreased (0.000654 --> 0.000654).  Saving model ...\nEpoch: 15700, loss: 0.00072\nLoss decreased (0.000654 --> 0.000654).  Saving model ...\nLoss decreased (0.000654 --> 0.000653).  Saving model ...\nLoss decreased (0.000653 --> 0.000653).  Saving model ...\nEpoch: 15800, loss: 0.00067\nLoss decreased (0.000653 --> 0.000653).  Saving model ...\nLoss decreased (0.000653 --> 0.000653).  Saving model ...\nLoss decreased (0.000653 --> 0.000653).  Saving model ...\nEpoch: 15900, loss: 0.00065\nLoss decreased (0.000653 --> 0.000653).  Saving model ...\nLoss decreased (0.000653 --> 0.000652).  Saving model ...\nLoss decreased (0.000652 --> 0.000652).  Saving model ...\nLoss decreased (0.000652 --> 0.000652).  Saving model ...\nLoss decreased (0.000652 --> 0.000652).  Saving model ...\nEpoch: 16000, loss: 0.00065\nLoss decreased (0.000652 --> 0.000651).  Saving model ...\nEpoch: 16100, loss: 0.00066\nLoss decreased (0.000651 --> 0.000651).  Saving model ...\nEpoch: 16200, loss: 0.00067\nLoss decreased (0.000651 --> 0.000650).  Saving model ...\nEpoch: 16300, loss: 0.00066\nLoss decreased (0.000650 --> 0.000650).  Saving model ...\nLoss decreased (0.000650 --> 0.000650).  Saving model ...\nLoss decreased (0.000650 --> 0.000650).  Saving model ...\nLoss decreased (0.000650 --> 0.000649).  Saving model ...\nLoss decreased (0.000649 --> 0.000649).  Saving model ...\nEpoch: 16400, loss: 0.00066\nLoss decreased (0.000649 --> 0.000649).  Saving model ...\nLoss decreased (0.000649 --> 0.000649).  Saving model ...\nLoss decreased (0.000649 --> 0.000649).  Saving model ...\nLoss decreased (0.000649 --> 0.000649).  Saving model ...\nLoss decreased (0.000649 --> 0.000649).  Saving model ...\nLoss decreased (0.000649 --> 0.000649).  Saving model ...\nEpoch: 16500, loss: 0.00065\nLoss decreased (0.000649 --> 0.000648).  Saving model ...\nLoss decreased (0.000648 --> 0.000648).  Saving model ...\nEpoch: 16600, loss: 0.00068\nLoss decreased (0.000648 --> 0.000648).  Saving model ...\nLoss decreased (0.000648 --> 0.000648).  Saving model ...\nEpoch: 16700, loss: 0.00066\nLoss decreased (0.000648 --> 0.000647).  Saving model ...\nLoss decreased (0.000647 --> 0.000647).  Saving model ...\nLoss decreased (0.000647 --> 0.000646).  Saving model ...\nEpoch: 16800, loss: 0.00071\nLoss decreased (0.000646 --> 0.000646).  Saving model ...\nLoss decreased (0.000646 --> 0.000646).  Saving model ...\nLoss decreased (0.000646 --> 0.000646).  Saving model ...\nEpoch: 16900, loss: 0.00066\nLoss decreased (0.000646 --> 0.000646).  Saving model ...\nLoss decreased (0.000646 --> 0.000645).  Saving model ...\nLoss decreased (0.000645 --> 0.000645).  Saving model ...\nLoss decreased (0.000645 --> 0.000645).  Saving model ...\nEpoch: 17000, loss: 0.00091\nLoss decreased (0.000645 --> 0.000645).  Saving model ...\nLoss decreased (0.000645 --> 0.000644).  Saving model ...\nEpoch: 17100, loss: 0.00066\nLoss decreased (0.000644 --> 0.000644).  Saving model ...\nLoss decreased (0.000644 --> 0.000644).  Saving model ...\nLoss decreased (0.000644 --> 0.000644).  Saving model ...\nLoss decreased (0.000644 --> 0.000644).  Saving model ...\nEpoch: 17200, loss: 0.00066\nLoss decreased (0.000644 --> 0.000644).  Saving model ...\nLoss decreased (0.000644 --> 0.000644).  Saving model ...\nLoss decreased (0.000644 --> 0.000643).  Saving model ...\nLoss decreased (0.000643 --> 0.000643).  Saving model ...\nLoss decreased (0.000643 --> 0.000643).  Saving model ...\nEpoch: 17300, loss: 0.00066\nLoss decreased (0.000643 --> 0.000643).  Saving model ...\nLoss decreased (0.000643 --> 0.000643).  Saving model ...\nEpoch: 17400, loss: 0.00069\nLoss decreased (0.000643 --> 0.000642).  Saving model ...\nLoss decreased (0.000642 --> 0.000642).  Saving model ...\nLoss decreased (0.000642 --> 0.000642).  Saving model ...\nEpoch: 17500, loss: 0.00069\nLoss decreased (0.000642 --> 0.000642).  Saving model ...\nLoss decreased (0.000642 --> 0.000642).  Saving model ...\nLoss decreased (0.000642 --> 0.000641).  Saving model ...\nEpoch: 17600, loss: 0.00068\nLoss decreased (0.000641 --> 0.000641).  Saving model ...\nLoss decreased (0.000641 --> 0.000641).  Saving model ...\nEpoch: 17700, loss: 0.00065\nLoss decreased (0.000641 --> 0.000640).  Saving model ...\nEpoch: 17800, loss: 0.00066\nLoss decreased (0.000640 --> 0.000640).  Saving model ...\nLoss decreased (0.000640 --> 0.000639).  Saving model ...\nEpoch: 17900, loss: 0.00065\nLoss decreased (0.000639 --> 0.000639).  Saving model ...\nLoss decreased (0.000639 --> 0.000639).  Saving model ...\nLoss decreased (0.000639 --> 0.000639).  Saving model ...\nEpoch: 18000, loss: 0.00074\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000639 --> 0.000639).  Saving model ...\nLoss decreased (0.000639 --> 0.000639).  Saving model ...\nLoss decreased (0.000639 --> 0.000638).  Saving model ...\nLoss decreased (0.000638 --> 0.000638).  Saving model ...\nEpoch: 18100, loss: 0.00065\nLoss decreased (0.000638 --> 0.000638).  Saving model ...\nLoss decreased (0.000638 --> 0.000638).  Saving model ...\nLoss decreased (0.000638 --> 0.000638).  Saving model ...\nLoss decreased (0.000638 --> 0.000638).  Saving model ...\nLoss decreased (0.000638 --> 0.000638).  Saving model ...\nLoss decreased (0.000638 --> 0.000638).  Saving model ...\nLoss decreased (0.000638 --> 0.000638).  Saving model ...\nLoss decreased (0.000638 --> 0.000638).  Saving model ...\nLoss decreased (0.000638 --> 0.000637).  Saving model ...\nLoss decreased (0.000637 --> 0.000637).  Saving model ...\nLoss decreased (0.000637 --> 0.000637).  Saving model ...\nLoss decreased (0.000637 --> 0.000637).  Saving model ...\nEpoch: 18200, loss: 0.00064\nLoss decreased (0.000637 --> 0.000637).  Saving model ...\nLoss decreased (0.000637 --> 0.000637).  Saving model ...\nEpoch: 18300, loss: 0.00066\nLoss decreased (0.000637 --> 0.000637).  Saving model ...\nLoss decreased (0.000637 --> 0.000636).  Saving model ...\nLoss decreased (0.000636 --> 0.000636).  Saving model ...\nLoss decreased (0.000636 --> 0.000636).  Saving model ...\nEpoch: 18400, loss: 0.00067\nLoss decreased (0.000636 --> 0.000636).  Saving model ...\nLoss decreased (0.000636 --> 0.000636).  Saving model ...\nLoss decreased (0.000636 --> 0.000636).  Saving model ...\nLoss decreased (0.000636 --> 0.000636).  Saving model ...\nLoss decreased (0.000636 --> 0.000636).  Saving model ...\nLoss decreased (0.000636 --> 0.000636).  Saving model ...\nLoss decreased (0.000636 --> 0.000636).  Saving model ...\nLoss decreased (0.000636 --> 0.000636).  Saving model ...\nLoss decreased (0.000636 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nEpoch: 18500, loss: 0.00065\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nEpoch: 18600, loss: 0.00064\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000635).  Saving model ...\nLoss decreased (0.000635 --> 0.000634).  Saving model ...\nLoss decreased (0.000634 --> 0.000634).  Saving model ...\nEpoch: 18700, loss: 0.00064\nLoss decreased (0.000634 --> 0.000634).  Saving model ...\nEpoch: 18800, loss: 0.00067\nLoss decreased (0.000634 --> 0.000634).  Saving model ...\nLoss decreased (0.000634 --> 0.000633).  Saving model ...\nEpoch: 18900, loss: 0.00068\nLoss decreased (0.000633 --> 0.000633).  Saving model ...\nLoss decreased (0.000633 --> 0.000632).  Saving model ...\nEpoch: 19000, loss: 0.00069\nLoss decreased (0.000632 --> 0.000632).  Saving model ...\nLoss decreased (0.000632 --> 0.000632).  Saving model ...\nEpoch: 19100, loss: 0.00065\nLoss decreased (0.000632 --> 0.000632).  Saving model ...\nLoss decreased (0.000632 --> 0.000631).  Saving model ...\nLoss decreased (0.000631 --> 0.000631).  Saving model ...\nLoss decreased (0.000631 --> 0.000631).  Saving model ...\nEpoch: 19200, loss: 0.00071\nLoss decreased (0.000631 --> 0.000631).  Saving model ...\nLoss decreased (0.000631 --> 0.000631).  Saving model ...\nEpoch: 19300, loss: 0.00063\nLoss decreased (0.000631 --> 0.000630).  Saving model ...\nLoss decreased (0.000630 --> 0.000630).  Saving model ...\nEpoch: 19400, loss: 0.00067\nLoss decreased (0.000630 --> 0.000630).  Saving model ...\nLoss decreased (0.000630 --> 0.000629).  Saving model ...\nEpoch: 19500, loss: 0.00066\nLoss decreased (0.000629 --> 0.000629).  Saving model ...\nLoss decreased (0.000629 --> 0.000629).  Saving model ...\nLoss decreased (0.000629 --> 0.000629).  Saving model ...\nLoss decreased (0.000629 --> 0.000629).  Saving model ...\nLoss decreased (0.000629 --> 0.000628).  Saving model ...\nLoss decreased (0.000628 --> 0.000628).  Saving model ...\nLoss decreased (0.000628 --> 0.000628).  Saving model ...\nLoss decreased (0.000628 --> 0.000628).  Saving model ...\nEpoch: 19600, loss: 0.00063\nLoss decreased (0.000628 --> 0.000628).  Saving model ...\nLoss decreased (0.000628 --> 0.000628).  Saving model ...\nLoss decreased (0.000628 --> 0.000628).  Saving model ...\nLoss decreased (0.000628 --> 0.000628).  Saving model ...\nLoss decreased (0.000628 --> 0.000628).  Saving model ...\nLoss decreased (0.000628 --> 0.000628).  Saving model ...\nEpoch: 19700, loss: 0.00071\nLoss decreased (0.000628 --> 0.000628).  Saving model ...\nLoss decreased (0.000628 --> 0.000627).  Saving model ...\nLoss decreased (0.000627 --> 0.000627).  Saving model ...\nLoss decreased (0.000627 --> 0.000627).  Saving model ...\nEpoch: 19800, loss: 0.00063\nLoss decreased (0.000627 --> 0.000627).  Saving model ...\nLoss decreased (0.000627 --> 0.000627).  Saving model ...\nEpoch: 19900, loss: 0.00065\nLoss decreased (0.000627 --> 0.000626).  Saving model ...\nLoss decreased (0.000626 --> 0.000626).  Saving model ...\nLoss decreased (0.000626 --> 0.000626).  Saving model ...\nEpoch: 20000, loss: 0.00063\nLoss decreased (0.000626 --> 0.000626).  Saving model ...\nLoss decreased (0.000626 --> 0.000625).  Saving model ...\nLoss decreased (0.000625 --> 0.000625).  Saving model ...\nEpoch: 20100, loss: 0.00068\nLoss decreased (0.000625 --> 0.000625).  Saving model ...\nLoss decreased (0.000625 --> 0.000624).  Saving model ...\nEpoch: 20200, loss: 0.00073\nLoss decreased (0.000624 --> 0.000624).  Saving model ...\nLoss decreased (0.000624 --> 0.000624).  Saving model ...\nEpoch: 20300, loss: 0.00069\nLoss decreased (0.000624 --> 0.000624).  Saving model ...\nLoss decreased (0.000624 --> 0.000624).  Saving model ...\nLoss decreased (0.000624 --> 0.000624).  Saving model ...\nLoss decreased (0.000624 --> 0.000623).  Saving model ...\nEpoch: 20400, loss: 0.00066\nLoss decreased (0.000623 --> 0.000623).  Saving model ...\nLoss decreased (0.000623 --> 0.000623).  Saving model ...\nEpoch: 20500, loss: 0.00063\nLoss decreased (0.000623 --> 0.000622).  Saving model ...\nLoss decreased (0.000622 --> 0.000622).  Saving model ...\nLoss decreased (0.000622 --> 0.000622).  Saving model ...\nLoss decreased (0.000622 --> 0.000622).  Saving model ...\nLoss decreased (0.000622 --> 0.000622).  Saving model ...\nEpoch: 20600, loss: 0.00063\nLoss decreased (0.000622 --> 0.000622).  Saving model ...\nLoss decreased (0.000622 --> 0.000621).  Saving model ...\nEpoch: 20700, loss: 0.00064\nLoss decreased (0.000621 --> 0.000621).  Saving model ...\nLoss decreased (0.000621 --> 0.000621).  Saving model ...\nLoss decreased (0.000621 --> 0.000621).  Saving model ...\nLoss decreased (0.000621 --> 0.000620).  Saving model ...\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nEpoch: 20800, loss: 0.00062\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nLoss decreased (0.000620 --> 0.000620).  Saving model ...\nEpoch: 20900, loss: 0.00064\nLoss decreased (0.000620 --> 0.000619).  Saving model ...\nLoss decreased (0.000619 --> 0.000619).  Saving model ...\nEpoch: 21000, loss: 0.00065\nLoss decreased (0.000619 --> 0.000619).  Saving model ...\nLoss decreased (0.000619 --> 0.000619).  Saving model ...\nLoss decreased (0.000619 --> 0.000619).  Saving model ...\nLoss decreased (0.000619 --> 0.000619).  Saving model ...\nLoss decreased (0.000619 --> 0.000618).  Saving model ...\nEpoch: 21100, loss: 0.00066\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000618 --> 0.000618).  Saving model ...\nLoss decreased (0.000618 --> 0.000618).  Saving model ...\nLoss decreased (0.000618 --> 0.000618).  Saving model ...\nEpoch: 21200, loss: 0.00063\nLoss decreased (0.000618 --> 0.000617).  Saving model ...\nEpoch: 21300, loss: 0.00064\nLoss decreased (0.000617 --> 0.000617).  Saving model ...\nLoss decreased (0.000617 --> 0.000617).  Saving model ...\nLoss decreased (0.000617 --> 0.000616).  Saving model ...\nEpoch: 21400, loss: 0.00063\nLoss decreased (0.000616 --> 0.000616).  Saving model ...\nLoss decreased (0.000616 --> 0.000616).  Saving model ...\nLoss decreased (0.000616 --> 0.000616).  Saving model ...\nEpoch: 21500, loss: 0.00063\nLoss decreased (0.000616 --> 0.000616).  Saving model ...\nLoss decreased (0.000616 --> 0.000615).  Saving model ...\nLoss decreased (0.000615 --> 0.000615).  Saving model ...\nLoss decreased (0.000615 --> 0.000615).  Saving model ...\nEpoch: 21600, loss: 0.00064\nLoss decreased (0.000615 --> 0.000615).  Saving model ...\nLoss decreased (0.000615 --> 0.000615).  Saving model ...\nLoss decreased (0.000615 --> 0.000614).  Saving model ...\nLoss decreased (0.000614 --> 0.000614).  Saving model ...\nEpoch: 21700, loss: 0.00072\nLoss decreased (0.000614 --> 0.000614).  Saving model ...\nLoss decreased (0.000614 --> 0.000614).  Saving model ...\nLoss decreased (0.000614 --> 0.000613).  Saving model ...\nEpoch: 21800, loss: 0.00072\nLoss decreased (0.000613 --> 0.000613).  Saving model ...\nLoss decreased (0.000613 --> 0.000613).  Saving model ...\nLoss decreased (0.000613 --> 0.000613).  Saving model ...\nLoss decreased (0.000613 --> 0.000613).  Saving model ...\nLoss decreased (0.000613 --> 0.000613).  Saving model ...\nLoss decreased (0.000613 --> 0.000613).  Saving model ...\nEpoch: 21900, loss: 0.00062\nEpoch: 22000, loss: 0.00062\nLoss decreased (0.000613 --> 0.000613).  Saving model ...\nLoss decreased (0.000613 --> 0.000612).  Saving model ...\nLoss decreased (0.000612 --> 0.000612).  Saving model ...\nLoss decreased (0.000612 --> 0.000612).  Saving model ...\nLoss decreased (0.000612 --> 0.000612).  Saving model ...\nEpoch: 22100, loss: 0.00065\nLoss decreased (0.000612 --> 0.000612).  Saving model ...\nLoss decreased (0.000612 --> 0.000612).  Saving model ...\nLoss decreased (0.000612 --> 0.000611).  Saving model ...\nLoss decreased (0.000611 --> 0.000611).  Saving model ...\nLoss decreased (0.000611 --> 0.000611).  Saving model ...\nEpoch: 22200, loss: 0.00061\nLoss decreased (0.000611 --> 0.000611).  Saving model ...\nLoss decreased (0.000611 --> 0.000610).  Saving model ...\nLoss decreased (0.000610 --> 0.000610).  Saving model ...\nEpoch: 22300, loss: 0.00061\nLoss decreased (0.000610 --> 0.000610).  Saving model ...\nLoss decreased (0.000610 --> 0.000610).  Saving model ...\nLoss decreased (0.000610 --> 0.000610).  Saving model ...\nLoss decreased (0.000610 --> 0.000610).  Saving model ...\nLoss decreased (0.000610 --> 0.000610).  Saving model ...\nEpoch: 22400, loss: 0.00062\nLoss decreased (0.000610 --> 0.000610).  Saving model ...\nLoss decreased (0.000610 --> 0.000609).  Saving model ...\nLoss decreased (0.000609 --> 0.000609).  Saving model ...\nLoss decreased (0.000609 --> 0.000609).  Saving model ...\nEpoch: 22500, loss: 0.00062\nLoss decreased (0.000609 --> 0.000609).  Saving model ...\nLoss decreased (0.000609 --> 0.000609).  Saving model ...\nLoss decreased (0.000609 --> 0.000609).  Saving model ...\nEpoch: 22600, loss: 0.00062\nLoss decreased (0.000609 --> 0.000608).  Saving model ...\nLoss decreased (0.000608 --> 0.000608).  Saving model ...\nLoss decreased (0.000608 --> 0.000608).  Saving model ...\nEpoch: 22700, loss: 0.00071\nLoss decreased (0.000608 --> 0.000608).  Saving model ...\nLoss decreased (0.000608 --> 0.000608).  Saving model ...\nLoss decreased (0.000608 --> 0.000607).  Saving model ...\nLoss decreased (0.000607 --> 0.000607).  Saving model ...\nLoss decreased (0.000607 --> 0.000607).  Saving model ...\nLoss decreased (0.000607 --> 0.000607).  Saving model ...\nLoss decreased (0.000607 --> 0.000607).  Saving model ...\nEpoch: 22800, loss: 0.00061\nLoss decreased (0.000607 --> 0.000607).  Saving model ...\nEpoch: 22900, loss: 0.00068\nLoss decreased (0.000607 --> 0.000607).  Saving model ...\nLoss decreased (0.000607 --> 0.000607).  Saving model ...\nLoss decreased (0.000607 --> 0.000606).  Saving model ...\nLoss decreased (0.000606 --> 0.000606).  Saving model ...\nEpoch: 23000, loss: 0.00061\nLoss decreased (0.000606 --> 0.000606).  Saving model ...\nLoss decreased (0.000606 --> 0.000606).  Saving model ...\nLoss decreased (0.000606 --> 0.000606).  Saving model ...\nLoss decreased (0.000606 --> 0.000606).  Saving model ...\nLoss decreased (0.000606 --> 0.000605).  Saving model ...\nEpoch: 23100, loss: 0.00061\nLoss decreased (0.000605 --> 0.000605).  Saving model ...\nLoss decreased (0.000605 --> 0.000605).  Saving model ...\nEpoch: 23200, loss: 0.00065\nLoss decreased (0.000605 --> 0.000605).  Saving model ...\nLoss decreased (0.000605 --> 0.000605).  Saving model ...\nLoss decreased (0.000605 --> 0.000605).  Saving model ...\nLoss decreased (0.000605 --> 0.000604).  Saving model ...\nLoss decreased (0.000604 --> 0.000604).  Saving model ...\nEpoch: 23300, loss: 0.00061\nLoss decreased (0.000604 --> 0.000604).  Saving model ...\nEpoch: 23400, loss: 0.00061\nLoss decreased (0.000604 --> 0.000603).  Saving model ...\nLoss decreased (0.000603 --> 0.000603).  Saving model ...\nLoss decreased (0.000603 --> 0.000603).  Saving model ...\nLoss decreased (0.000603 --> 0.000603).  Saving model ...\nLoss decreased (0.000603 --> 0.000603).  Saving model ...\nEpoch: 23500, loss: 0.00061\nLoss decreased (0.000603 --> 0.000602).  Saving model ...\nLoss decreased (0.000602 --> 0.000602).  Saving model ...\nEpoch: 23600, loss: 0.00065\nLoss decreased (0.000602 --> 0.000602).  Saving model ...\nLoss decreased (0.000602 --> 0.000602).  Saving model ...\nLoss decreased (0.000602 --> 0.000602).  Saving model ...\nEpoch: 23700, loss: 0.00063\nLoss decreased (0.000602 --> 0.000601).  Saving model ...\nLoss decreased (0.000601 --> 0.000601).  Saving model ...\nEpoch: 23800, loss: 0.00065\nLoss decreased (0.000601 --> 0.000601).  Saving model ...\nLoss decreased (0.000601 --> 0.000601).  Saving model ...\nLoss decreased (0.000601 --> 0.000600).  Saving model ...\nEpoch: 23900, loss: 0.00070\nLoss decreased (0.000600 --> 0.000600).  Saving model ...\nLoss decreased (0.000600 --> 0.000600).  Saving model ...\nLoss decreased (0.000600 --> 0.000600).  Saving model ...\nEpoch: 24000, loss: 0.00061\nLoss decreased (0.000600 --> 0.000600).  Saving model ...\nLoss decreased (0.000600 --> 0.000600).  Saving model ...\nLoss decreased (0.000600 --> 0.000599).  Saving model ...\nLoss decreased (0.000599 --> 0.000599).  Saving model ...\nLoss decreased (0.000599 --> 0.000599).  Saving model ...\nEpoch: 24100, loss: 0.00060\nLoss decreased (0.000599 --> 0.000599).  Saving model ...\nLoss decreased (0.000599 --> 0.000599).  Saving model ...\nEpoch: 24200, loss: 0.00062\nLoss decreased (0.000599 --> 0.000599).  Saving model ...\nLoss decreased (0.000599 --> 0.000598).  Saving model ...\nLoss decreased (0.000598 --> 0.000598).  Saving model ...\nLoss decreased (0.000598 --> 0.000598).  Saving model ...\nLoss decreased (0.000598 --> 0.000598).  Saving model ...\nLoss decreased (0.000598 --> 0.000598).  Saving model ...\nLoss decreased (0.000598 --> 0.000598).  Saving model ...\nEpoch: 24300, loss: 0.00071\nLoss decreased (0.000598 --> 0.000598).  Saving model ...\nLoss decreased (0.000598 --> 0.000597).  Saving model ...\nEpoch: 24400, loss: 0.00066\nLoss decreased (0.000597 --> 0.000597).  Saving model ...\nLoss decreased (0.000597 --> 0.000597).  Saving model ...\nLoss decreased (0.000597 --> 0.000597).  Saving model ...\nEpoch: 24500, loss: 0.00060\nLoss decreased (0.000597 --> 0.000596).  Saving model ...\nLoss decreased (0.000596 --> 0.000596).  Saving model ...\nEpoch: 24600, loss: 0.00060\nLoss decreased (0.000596 --> 0.000596).  Saving model ...\nLoss decreased (0.000596 --> 0.000596).  Saving model ...\nLoss decreased (0.000596 --> 0.000596).  Saving model ...\nEpoch: 24700, loss: 0.00064\nLoss decreased (0.000596 --> 0.000596).  Saving model ...\nLoss decreased (0.000596 --> 0.000595).  Saving model ...\nLoss decreased (0.000595 --> 0.000595).  Saving model ...\nEpoch: 24800, loss: 0.00061\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000595 --> 0.000595).  Saving model ...\nLoss decreased (0.000595 --> 0.000595).  Saving model ...\nLoss decreased (0.000595 --> 0.000594).  Saving model ...\nEpoch: 24900, loss: 0.00065\nLoss decreased (0.000594 --> 0.000594).  Saving model ...\nLoss decreased (0.000594 --> 0.000594).  Saving model ...\nLoss decreased (0.000594 --> 0.000594).  Saving model ...\nEpoch: 25000, loss: 0.00063\nLoss decreased (0.000594 --> 0.000594).  Saving model ...\nLoss decreased (0.000594 --> 0.000594).  Saving model ...\nLoss decreased (0.000594 --> 0.000593).  Saving model ...\nEpoch: 25100, loss: 0.00059\nLoss decreased (0.000593 --> 0.000593).  Saving model ...\nLoss decreased (0.000593 --> 0.000593).  Saving model ...\nLoss decreased (0.000593 --> 0.000592).  Saving model ...\nEpoch: 25200, loss: 0.00060\nLoss decreased (0.000592 --> 0.000592).  Saving model ...\nEpoch: 25300, loss: 0.00062\nLoss decreased (0.000592 --> 0.000592).  Saving model ...\nLoss decreased (0.000592 --> 0.000592).  Saving model ...\nLoss decreased (0.000592 --> 0.000592).  Saving model ...\nLoss decreased (0.000592 --> 0.000591).  Saving model ...\nLoss decreased (0.000591 --> 0.000591).  Saving model ...\nEpoch: 25400, loss: 0.00061\nLoss decreased (0.000591 --> 0.000591).  Saving model ...\nLoss decreased (0.000591 --> 0.000591).  Saving model ...\nLoss decreased (0.000591 --> 0.000591).  Saving model ...\nLoss decreased (0.000591 --> 0.000591).  Saving model ...\nEpoch: 25500, loss: 0.00060\nLoss decreased (0.000591 --> 0.000591).  Saving model ...\nLoss decreased (0.000591 --> 0.000591).  Saving model ...\nLoss decreased (0.000591 --> 0.000590).  Saving model ...\nEpoch: 25600, loss: 0.00060\nLoss decreased (0.000590 --> 0.000590).  Saving model ...\nEpoch: 25700, loss: 0.00060\nLoss decreased (0.000590 --> 0.000590).  Saving model ...\nLoss decreased (0.000590 --> 0.000590).  Saving model ...\nEpoch: 25800, loss: 0.00060\nLoss decreased (0.000590 --> 0.000589).  Saving model ...\nLoss decreased (0.000589 --> 0.000589).  Saving model ...\nLoss decreased (0.000589 --> 0.000589).  Saving model ...\nLoss decreased (0.000589 --> 0.000589).  Saving model ...\nEpoch: 25900, loss: 0.00060\nLoss decreased (0.000589 --> 0.000589).  Saving model ...\nLoss decreased (0.000589 --> 0.000588).  Saving model ...\nLoss decreased (0.000588 --> 0.000588).  Saving model ...\nEpoch: 26000, loss: 0.00059\nLoss decreased (0.000588 --> 0.000588).  Saving model ...\nLoss decreased (0.000588 --> 0.000588).  Saving model ...\nLoss decreased (0.000588 --> 0.000588).  Saving model ...\nEpoch: 26100, loss: 0.00059\nLoss decreased (0.000588 --> 0.000588).  Saving model ...\nLoss decreased (0.000588 --> 0.000587).  Saving model ...\nLoss decreased (0.000587 --> 0.000587).  Saving model ...\nLoss decreased (0.000587 --> 0.000587).  Saving model ...\nEpoch: 26200, loss: 0.00062\nLoss decreased (0.000587 --> 0.000587).  Saving model ...\nLoss decreased (0.000587 --> 0.000587).  Saving model ...\nLoss decreased (0.000587 --> 0.000587).  Saving model ...\nEpoch: 26300, loss: 0.00059\nLoss decreased (0.000587 --> 0.000587).  Saving model ...\nLoss decreased (0.000587 --> 0.000587).  Saving model ...\nLoss decreased (0.000587 --> 0.000586).  Saving model ...\nLoss decreased (0.000586 --> 0.000586).  Saving model ...\nEpoch: 26400, loss: 0.00059\nLoss decreased (0.000586 --> 0.000586).  Saving model ...\nLoss decreased (0.000586 --> 0.000585).  Saving model ...\nLoss decreased (0.000585 --> 0.000585).  Saving model ...\nEpoch: 26500, loss: 0.00059\nLoss decreased (0.000585 --> 0.000585).  Saving model ...\nLoss decreased (0.000585 --> 0.000585).  Saving model ...\nEpoch: 26600, loss: 0.00064\nLoss decreased (0.000585 --> 0.000585).  Saving model ...\nLoss decreased (0.000585 --> 0.000585).  Saving model ...\nLoss decreased (0.000585 --> 0.000585).  Saving model ...\nLoss decreased (0.000585 --> 0.000585).  Saving model ...\nLoss decreased (0.000585 --> 0.000584).  Saving model ...\nEpoch: 26700, loss: 0.00061\nLoss decreased (0.000584 --> 0.000584).  Saving model ...\nLoss decreased (0.000584 --> 0.000584).  Saving model ...\nEpoch: 26800, loss: 0.00063\nLoss decreased (0.000584 --> 0.000583).  Saving model ...\nLoss decreased (0.000583 --> 0.000583).  Saving model ...\nLoss decreased (0.000583 --> 0.000583).  Saving model ...\nEpoch: 26900, loss: 0.00059\nLoss decreased (0.000583 --> 0.000583).  Saving model ...\nLoss decreased (0.000583 --> 0.000583).  Saving model ...\nLoss decreased (0.000583 --> 0.000583).  Saving model ...\nEpoch: 27000, loss: 0.00063\nLoss decreased (0.000583 --> 0.000582).  Saving model ...\nLoss decreased (0.000582 --> 0.000582).  Saving model ...\nLoss decreased (0.000582 --> 0.000582).  Saving model ...\nEpoch: 27100, loss: 0.00059\nLoss decreased (0.000582 --> 0.000582).  Saving model ...\nLoss decreased (0.000582 --> 0.000581).  Saving model ...\nEpoch: 27200, loss: 0.00059\nLoss decreased (0.000581 --> 0.000581).  Saving model ...\nLoss decreased (0.000581 --> 0.000581).  Saving model ...\nLoss decreased (0.000581 --> 0.000581).  Saving model ...\nEpoch: 27300, loss: 0.00061\nLoss decreased (0.000581 --> 0.000581).  Saving model ...\nLoss decreased (0.000581 --> 0.000580).  Saving model ...\nLoss decreased (0.000580 --> 0.000580).  Saving model ...\nEpoch: 27400, loss: 0.00061\nLoss decreased (0.000580 --> 0.000580).  Saving model ...\nLoss decreased (0.000580 --> 0.000580).  Saving model ...\nLoss decreased (0.000580 --> 0.000580).  Saving model ...\nEpoch: 27500, loss: 0.00062\nLoss decreased (0.000580 --> 0.000580).  Saving model ...\nLoss decreased (0.000580 --> 0.000579).  Saving model ...\nLoss decreased (0.000579 --> 0.000579).  Saving model ...\nEpoch: 27600, loss: 0.00058\nLoss decreased (0.000579 --> 0.000579).  Saving model ...\nLoss decreased (0.000579 --> 0.000578).  Saving model ...\nEpoch: 27700, loss: 0.00061\nLoss decreased (0.000578 --> 0.000578).  Saving model ...\nLoss decreased (0.000578 --> 0.000578).  Saving model ...\nLoss decreased (0.000578 --> 0.000578).  Saving model ...\nEpoch: 27800, loss: 0.00060\nLoss decreased (0.000578 --> 0.000578).  Saving model ...\nLoss decreased (0.000578 --> 0.000577).  Saving model ...\nEpoch: 27900, loss: 0.00063\nLoss decreased (0.000577 --> 0.000577).  Saving model ...\nLoss decreased (0.000577 --> 0.000577).  Saving model ...\nLoss decreased (0.000577 --> 0.000577).  Saving model ...\nEpoch: 28000, loss: 0.00070\nLoss decreased (0.000577 --> 0.000577).  Saving model ...\nLoss decreased (0.000577 --> 0.000576).  Saving model ...\nLoss decreased (0.000576 --> 0.000576).  Saving model ...\nLoss decreased (0.000576 --> 0.000576).  Saving model ...\nEpoch: 28100, loss: 0.00058\nLoss decreased (0.000576 --> 0.000576).  Saving model ...\nLoss decreased (0.000576 --> 0.000576).  Saving model ...\nLoss decreased (0.000576 --> 0.000576).  Saving model ...\nEpoch: 28200, loss: 0.00059\nLoss decreased (0.000576 --> 0.000575).  Saving model ...\nLoss decreased (0.000575 --> 0.000575).  Saving model ...\nEpoch: 28300, loss: 0.00058\nLoss decreased (0.000575 --> 0.000575).  Saving model ...\nEpoch: 28400, loss: 0.00058\nLoss decreased (0.000575 --> 0.000575).  Saving model ...\nLoss decreased (0.000575 --> 0.000574).  Saving model ...\nLoss decreased (0.000574 --> 0.000574).  Saving model ...\nEpoch: 28500, loss: 0.00060\nLoss decreased (0.000574 --> 0.000574).  Saving model ...\nLoss decreased (0.000574 --> 0.000574).  Saving model ...\nEpoch: 28600, loss: 0.00062\nLoss decreased (0.000574 --> 0.000573).  Saving model ...\nLoss decreased (0.000573 --> 0.000573).  Saving model ...\nLoss decreased (0.000573 --> 0.000573).  Saving model ...\nEpoch: 28700, loss: 0.00062\nLoss decreased (0.000573 --> 0.000573).  Saving model ...\nLoss decreased (0.000573 --> 0.000573).  Saving model ...\nLoss decreased (0.000573 --> 0.000573).  Saving model ...\nLoss decreased (0.000573 --> 0.000573).  Saving model ...\nEpoch: 28800, loss: 0.00058\nLoss decreased (0.000573 --> 0.000572).  Saving model ...\nLoss decreased (0.000572 --> 0.000572).  Saving model ...\nLoss decreased (0.000572 --> 0.000572).  Saving model ...\nEpoch: 28900, loss: 0.00069\nLoss decreased (0.000572 --> 0.000572).  Saving model ...\nLoss decreased (0.000572 --> 0.000572).  Saving model ...\nLoss decreased (0.000572 --> 0.000571).  Saving model ...\nEpoch: 29000, loss: 0.00058\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000571 --> 0.000571).  Saving model ...\nLoss decreased (0.000571 --> 0.000571).  Saving model ...\nEpoch: 29100, loss: 0.00060\nLoss decreased (0.000571 --> 0.000571).  Saving model ...\nLoss decreased (0.000571 --> 0.000571).  Saving model ...\nLoss decreased (0.000571 --> 0.000570).  Saving model ...\nEpoch: 29200, loss: 0.00059\nLoss decreased (0.000570 --> 0.000570).  Saving model ...\nLoss decreased (0.000570 --> 0.000570).  Saving model ...\nLoss decreased (0.000570 --> 0.000570).  Saving model ...\nLoss decreased (0.000570 --> 0.000570).  Saving model ...\nLoss decreased (0.000570 --> 0.000570).  Saving model ...\nLoss decreased (0.000570 --> 0.000570).  Saving model ...\nLoss decreased (0.000570 --> 0.000570).  Saving model ...\nLoss decreased (0.000570 --> 0.000570).  Saving model ...\nEpoch: 29300, loss: 0.00057\nLoss decreased (0.000570 --> 0.000570).  Saving model ...\nLoss decreased (0.000570 --> 0.000570).  Saving model ...\nLoss decreased (0.000570 --> 0.000569).  Saving model ...\nEpoch: 29400, loss: 0.00058\nLoss decreased (0.000569 --> 0.000569).  Saving model ...\nLoss decreased (0.000569 --> 0.000569).  Saving model ...\nLoss decreased (0.000569 --> 0.000569).  Saving model ...\nEpoch: 29500, loss: 0.00060\nLoss decreased (0.000569 --> 0.000569).  Saving model ...\nLoss decreased (0.000569 --> 0.000569).  Saving model ...\nLoss decreased (0.000569 --> 0.000568).  Saving model ...\nLoss decreased (0.000568 --> 0.000568).  Saving model ...\nEpoch: 29600, loss: 0.00057\nLoss decreased (0.000568 --> 0.000568).  Saving model ...\nEpoch: 29700, loss: 0.00060\nLoss decreased (0.000568 --> 0.000567).  Saving model ...\nLoss decreased (0.000567 --> 0.000567).  Saving model ...\nLoss decreased (0.000567 --> 0.000567).  Saving model ...\nEpoch: 29800, loss: 0.00059\nLoss decreased (0.000567 --> 0.000567).  Saving model ...\nLoss decreased (0.000567 --> 0.000567).  Saving model ...\nLoss decreased (0.000567 --> 0.000567).  Saving model ...\nLoss decreased (0.000567 --> 0.000567).  Saving model ...\nLoss decreased (0.000567 --> 0.000567).  Saving model ...\nEpoch: 29900, loss: 0.00058\nLoss decreased (0.000567 --> 0.000566).  Saving model ...\nEpoch: 30000, loss: 0.00060\nLoss decreased (0.000566 --> 0.000566).  Saving model ...\nLoss decreased (0.000566 --> 0.000566).  Saving model ...\nLoss decreased (0.000566 --> 0.000566).  Saving model ...\nLoss decreased (0.000566 --> 0.000566).  Saving model ...\nLoss decreased (0.000566 --> 0.000565).  Saving model ...\nEpoch: 30100, loss: 0.00057\nLoss decreased (0.000565 --> 0.000565).  Saving model ...\nLoss decreased (0.000565 --> 0.000565).  Saving model ...\nLoss decreased (0.000565 --> 0.000565).  Saving model ...\nEpoch: 30200, loss: 0.00058\nLoss decreased (0.000565 --> 0.000565).  Saving model ...\nLoss decreased (0.000565 --> 0.000565).  Saving model ...\nLoss decreased (0.000565 --> 0.000564).  Saving model ...\nLoss decreased (0.000564 --> 0.000564).  Saving model ...\nEpoch: 30300, loss: 0.00064\nLoss decreased (0.000564 --> 0.000564).  Saving model ...\nEpoch: 30400, loss: 0.00058\nLoss decreased (0.000564 --> 0.000564).  Saving model ...\nLoss decreased (0.000564 --> 0.000564).  Saving model ...\nEpoch: 30500, loss: 0.00057\nLoss decreased (0.000564 --> 0.000563).  Saving model ...\nLoss decreased (0.000563 --> 0.000563).  Saving model ...\nEpoch: 30600, loss: 0.00061\nLoss decreased (0.000563 --> 0.000563).  Saving model ...\nLoss decreased (0.000563 --> 0.000563).  Saving model ...\nLoss decreased (0.000563 --> 0.000562).  Saving model ...\nLoss decreased (0.000562 --> 0.000562).  Saving model ...\nEpoch: 30700, loss: 0.00062\nLoss decreased (0.000562 --> 0.000562).  Saving model ...\nLoss decreased (0.000562 --> 0.000562).  Saving model ...\nLoss decreased (0.000562 --> 0.000562).  Saving model ...\nEpoch: 30800, loss: 0.00061\nLoss decreased (0.000562 --> 0.000561).  Saving model ...\nEpoch: 30900, loss: 0.00057\nLoss decreased (0.000561 --> 0.000561).  Saving model ...\nLoss decreased (0.000561 --> 0.000561).  Saving model ...\nLoss decreased (0.000561 --> 0.000561).  Saving model ...\nLoss decreased (0.000561 --> 0.000561).  Saving model ...\nEpoch: 31000, loss: 0.00058\nLoss decreased (0.000561 --> 0.000561).  Saving model ...\nLoss decreased (0.000561 --> 0.000560).  Saving model ...\nLoss decreased (0.000560 --> 0.000560).  Saving model ...\nLoss decreased (0.000560 --> 0.000560).  Saving model ...\nEpoch: 31100, loss: 0.00056\nLoss decreased (0.000560 --> 0.000560).  Saving model ...\nLoss decreased (0.000560 --> 0.000560).  Saving model ...\nLoss decreased (0.000560 --> 0.000559).  Saving model ...\nLoss decreased (0.000559 --> 0.000559).  Saving model ...\nLoss decreased (0.000559 --> 0.000559).  Saving model ...\nLoss decreased (0.000559 --> 0.000559).  Saving model ...\nLoss decreased (0.000559 --> 0.000559).  Saving model ...\nLoss decreased (0.000559 --> 0.000559).  Saving model ...\nLoss decreased (0.000559 --> 0.000559).  Saving model ...\nLoss decreased (0.000559 --> 0.000559).  Saving model ...\nEpoch: 31200, loss: 0.00056\nEpoch: 31300, loss: 0.00056\nLoss decreased (0.000559 --> 0.000559).  Saving model ...\nLoss decreased (0.000559 --> 0.000559).  Saving model ...\nLoss decreased (0.000559 --> 0.000558).  Saving model ...\nEpoch: 31400, loss: 0.00064\nLoss decreased (0.000558 --> 0.000558).  Saving model ...\nLoss decreased (0.000558 --> 0.000558).  Saving model ...\nLoss decreased (0.000558 --> 0.000558).  Saving model ...\nEpoch: 31500, loss: 0.00056\nLoss decreased (0.000558 --> 0.000558).  Saving model ...\nLoss decreased (0.000558 --> 0.000557).  Saving model ...\nLoss decreased (0.000557 --> 0.000557).  Saving model ...\nEpoch: 31600, loss: 0.00067\nLoss decreased (0.000557 --> 0.000557).  Saving model ...\nLoss decreased (0.000557 --> 0.000557).  Saving model ...\nLoss decreased (0.000557 --> 0.000557).  Saving model ...\nEpoch: 31700, loss: 0.00059\nLoss decreased (0.000557 --> 0.000557).  Saving model ...\nLoss decreased (0.000557 --> 0.000557).  Saving model ...\nLoss decreased (0.000557 --> 0.000556).  Saving model ...\nLoss decreased (0.000556 --> 0.000556).  Saving model ...\nLoss decreased (0.000556 --> 0.000556).  Saving model ...\nLoss decreased (0.000556 --> 0.000556).  Saving model ...\nEpoch: 31800, loss: 0.00058\nLoss decreased (0.000556 --> 0.000556).  Saving model ...\nLoss decreased (0.000556 --> 0.000556).  Saving model ...\nLoss decreased (0.000556 --> 0.000556).  Saving model ...\nEpoch: 31900, loss: 0.00058\nLoss decreased (0.000556 --> 0.000556).  Saving model ...\nLoss decreased (0.000556 --> 0.000556).  Saving model ...\nLoss decreased (0.000556 --> 0.000556).  Saving model ...\nLoss decreased (0.000556 --> 0.000555).  Saving model ...\nEpoch: 32000, loss: 0.00057\nLoss decreased (0.000555 --> 0.000555).  Saving model ...\nLoss decreased (0.000555 --> 0.000555).  Saving model ...\nLoss decreased (0.000555 --> 0.000555).  Saving model ...\nLoss decreased (0.000555 --> 0.000555).  Saving model ...\nEpoch: 32100, loss: 0.00062\nLoss decreased (0.000555 --> 0.000555).  Saving model ...\nLoss decreased (0.000555 --> 0.000555).  Saving model ...\nLoss decreased (0.000555 --> 0.000554).  Saving model ...\nEpoch: 32200, loss: 0.00058\nLoss decreased (0.000554 --> 0.000554).  Saving model ...\nLoss decreased (0.000554 --> 0.000554).  Saving model ...\nLoss decreased (0.000554 --> 0.000554).  Saving model ...\nEpoch: 32300, loss: 0.00059\nLoss decreased (0.000554 --> 0.000554).  Saving model ...\nLoss decreased (0.000554 --> 0.000554).  Saving model ...\nLoss decreased (0.000554 --> 0.000553).  Saving model ...\nLoss decreased (0.000553 --> 0.000553).  Saving model ...\nLoss decreased (0.000553 --> 0.000553).  Saving model ...\nEpoch: 32400, loss: 0.00065\nLoss decreased (0.000553 --> 0.000553).  Saving model ...\nLoss decreased (0.000553 --> 0.000553).  Saving model ...\nLoss decreased (0.000553 --> 0.000553).  Saving model ...\nEpoch: 32500, loss: 0.00055\nLoss decreased (0.000553 --> 0.000553).  Saving model ...\nLoss decreased (0.000553 --> 0.000552).  Saving model ...\nEpoch: 32600, loss: 0.00056\nLoss decreased (0.000552 --> 0.000552).  Saving model ...\nLoss decreased (0.000552 --> 0.000552).  Saving model ...\nLoss decreased (0.000552 --> 0.000552).  Saving model ...\nLoss decreased (0.000552 --> 0.000552).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000552 --> 0.000552).  Saving model ...\nEpoch: 32700, loss: 0.00057\nLoss decreased (0.000552 --> 0.000552).  Saving model ...\nLoss decreased (0.000552 --> 0.000552).  Saving model ...\nLoss decreased (0.000552 --> 0.000551).  Saving model ...\nEpoch: 32800, loss: 0.00062\nLoss decreased (0.000551 --> 0.000551).  Saving model ...\nLoss decreased (0.000551 --> 0.000551).  Saving model ...\nLoss decreased (0.000551 --> 0.000551).  Saving model ...\nLoss decreased (0.000551 --> 0.000551).  Saving model ...\nEpoch: 32900, loss: 0.00059\nLoss decreased (0.000551 --> 0.000551).  Saving model ...\nEpoch: 33000, loss: 0.00060\nLoss decreased (0.000551 --> 0.000550).  Saving model ...\nLoss decreased (0.000550 --> 0.000550).  Saving model ...\nLoss decreased (0.000550 --> 0.000550).  Saving model ...\nLoss decreased (0.000550 --> 0.000550).  Saving model ...\nEpoch: 33100, loss: 0.00059\nLoss decreased (0.000550 --> 0.000550).  Saving model ...\nLoss decreased (0.000550 --> 0.000549).  Saving model ...\nLoss decreased (0.000549 --> 0.000549).  Saving model ...\nEpoch: 33200, loss: 0.00068\nLoss decreased (0.000549 --> 0.000549).  Saving model ...\nLoss decreased (0.000549 --> 0.000549).  Saving model ...\nEpoch: 33300, loss: 0.00055\nLoss decreased (0.000549 --> 0.000549).  Saving model ...\nLoss decreased (0.000549 --> 0.000549).  Saving model ...\nEpoch: 33400, loss: 0.00055\nLoss decreased (0.000549 --> 0.000548).  Saving model ...\nLoss decreased (0.000548 --> 0.000548).  Saving model ...\nEpoch: 33500, loss: 0.00056\nLoss decreased (0.000548 --> 0.000548).  Saving model ...\nLoss decreased (0.000548 --> 0.000548).  Saving model ...\nLoss decreased (0.000548 --> 0.000548).  Saving model ...\nLoss decreased (0.000548 --> 0.000548).  Saving model ...\nLoss decreased (0.000548 --> 0.000547).  Saving model ...\nEpoch: 33600, loss: 0.00055\nEpoch: 33700, loss: 0.00055\nLoss decreased (0.000547 --> 0.000547).  Saving model ...\nLoss decreased (0.000547 --> 0.000547).  Saving model ...\nLoss decreased (0.000547 --> 0.000547).  Saving model ...\nLoss decreased (0.000547 --> 0.000547).  Saving model ...\nLoss decreased (0.000547 --> 0.000547).  Saving model ...\nLoss decreased (0.000547 --> 0.000546).  Saving model ...\nEpoch: 33800, loss: 0.00055\nEpoch: 33900, loss: 0.00057\nLoss decreased (0.000546 --> 0.000546).  Saving model ...\nLoss decreased (0.000546 --> 0.000546).  Saving model ...\nLoss decreased (0.000546 --> 0.000546).  Saving model ...\nLoss decreased (0.000546 --> 0.000546).  Saving model ...\nLoss decreased (0.000546 --> 0.000546).  Saving model ...\nLoss decreased (0.000546 --> 0.000546).  Saving model ...\nEpoch: 34000, loss: 0.00055\nLoss decreased (0.000546 --> 0.000546).  Saving model ...\nLoss decreased (0.000546 --> 0.000546).  Saving model ...\nLoss decreased (0.000546 --> 0.000545).  Saving model ...\nEpoch: 34100, loss: 0.00062\nLoss decreased (0.000545 --> 0.000545).  Saving model ...\nLoss decreased (0.000545 --> 0.000545).  Saving model ...\nLoss decreased (0.000545 --> 0.000545).  Saving model ...\nEpoch: 34200, loss: 0.00059\nLoss decreased (0.000545 --> 0.000545).  Saving model ...\nEpoch: 34300, loss: 0.00060\nLoss decreased (0.000545 --> 0.000544).  Saving model ...\nLoss decreased (0.000544 --> 0.000544).  Saving model ...\nLoss decreased (0.000544 --> 0.000544).  Saving model ...\nLoss decreased (0.000544 --> 0.000544).  Saving model ...\nLoss decreased (0.000544 --> 0.000544).  Saving model ...\nLoss decreased (0.000544 --> 0.000544).  Saving model ...\nEpoch: 34400, loss: 0.00056\nLoss decreased (0.000544 --> 0.000544).  Saving model ...\nLoss decreased (0.000544 --> 0.000544).  Saving model ...\nLoss decreased (0.000544 --> 0.000544).  Saving model ...\nLoss decreased (0.000544 --> 0.000544).  Saving model ...\nLoss decreased (0.000544 --> 0.000544).  Saving model ...\nEpoch: 34500, loss: 0.00058\nLoss decreased (0.000544 --> 0.000543).  Saving model ...\nEpoch: 34600, loss: 0.00056\nLoss decreased (0.000543 --> 0.000543).  Saving model ...\nLoss decreased (0.000543 --> 0.000543).  Saving model ...\nLoss decreased (0.000543 --> 0.000543).  Saving model ...\nLoss decreased (0.000543 --> 0.000543).  Saving model ...\nLoss decreased (0.000543 --> 0.000543).  Saving model ...\nEpoch: 34700, loss: 0.00055\nLoss decreased (0.000543 --> 0.000543).  Saving model ...\nLoss decreased (0.000543 --> 0.000543).  Saving model ...\nLoss decreased (0.000543 --> 0.000542).  Saving model ...\nLoss decreased (0.000542 --> 0.000542).  Saving model ...\nEpoch: 34800, loss: 0.00056\nLoss decreased (0.000542 --> 0.000542).  Saving model ...\nLoss decreased (0.000542 --> 0.000542).  Saving model ...\nEpoch: 34900, loss: 0.00055\nLoss decreased (0.000542 --> 0.000542).  Saving model ...\nLoss decreased (0.000542 --> 0.000541).  Saving model ...\nLoss decreased (0.000541 --> 0.000541).  Saving model ...\nEpoch: 35000, loss: 0.00055\nLoss decreased (0.000541 --> 0.000541).  Saving model ...\nLoss decreased (0.000541 --> 0.000541).  Saving model ...\nLoss decreased (0.000541 --> 0.000541).  Saving model ...\nEpoch: 35100, loss: 0.00058\nLoss decreased (0.000541 --> 0.000541).  Saving model ...\nLoss decreased (0.000541 --> 0.000540).  Saving model ...\nLoss decreased (0.000540 --> 0.000540).  Saving model ...\nEpoch: 35200, loss: 0.00062\nLoss decreased (0.000540 --> 0.000540).  Saving model ...\nEpoch: 35300, loss: 0.00056\nLoss decreased (0.000540 --> 0.000540).  Saving model ...\nEpoch: 35400, loss: 0.00055\nLoss decreased (0.000540 --> 0.000539).  Saving model ...\nLoss decreased (0.000539 --> 0.000539).  Saving model ...\nLoss decreased (0.000539 --> 0.000539).  Saving model ...\nEpoch: 35500, loss: 0.00054\nLoss decreased (0.000539 --> 0.000539).  Saving model ...\nLoss decreased (0.000539 --> 0.000539).  Saving model ...\nEpoch: 35600, loss: 0.00054\nLoss decreased (0.000539 --> 0.000539).  Saving model ...\nLoss decreased (0.000539 --> 0.000538).  Saving model ...\nLoss decreased (0.000538 --> 0.000538).  Saving model ...\nEpoch: 35700, loss: 0.00060\nLoss decreased (0.000538 --> 0.000538).  Saving model ...\nLoss decreased (0.000538 --> 0.000538).  Saving model ...\nLoss decreased (0.000538 --> 0.000538).  Saving model ...\nEpoch: 35800, loss: 0.00057\nLoss decreased (0.000538 --> 0.000538).  Saving model ...\nLoss decreased (0.000538 --> 0.000538).  Saving model ...\nLoss decreased (0.000538 --> 0.000538).  Saving model ...\nLoss decreased (0.000538 --> 0.000537).  Saving model ...\nEpoch: 35900, loss: 0.00056\nLoss decreased (0.000537 --> 0.000537).  Saving model ...\nLoss decreased (0.000537 --> 0.000537).  Saving model ...\nEpoch: 36000, loss: 0.00058\nLoss decreased (0.000537 --> 0.000537).  Saving model ...\nLoss decreased (0.000537 --> 0.000537).  Saving model ...\nLoss decreased (0.000537 --> 0.000537).  Saving model ...\nLoss decreased (0.000537 --> 0.000536).  Saving model ...\nEpoch: 36100, loss: 0.00059\nLoss decreased (0.000536 --> 0.000536).  Saving model ...\nLoss decreased (0.000536 --> 0.000536).  Saving model ...\nLoss decreased (0.000536 --> 0.000536).  Saving model ...\nEpoch: 36200, loss: 0.00054\nLoss decreased (0.000536 --> 0.000536).  Saving model ...\nEpoch: 36300, loss: 0.00062\nLoss decreased (0.000536 --> 0.000535).  Saving model ...\nLoss decreased (0.000535 --> 0.000535).  Saving model ...\nEpoch: 36400, loss: 0.00058\nLoss decreased (0.000535 --> 0.000535).  Saving model ...\nLoss decreased (0.000535 --> 0.000535).  Saving model ...\nEpoch: 36500, loss: 0.00054\nLoss decreased (0.000535 --> 0.000535).  Saving model ...\nLoss decreased (0.000535 --> 0.000534).  Saving model ...\nLoss decreased (0.000534 --> 0.000534).  Saving model ...\nEpoch: 36600, loss: 0.00056\nLoss decreased (0.000534 --> 0.000534).  Saving model ...\nLoss decreased (0.000534 --> 0.000534).  Saving model ...\nLoss decreased (0.000534 --> 0.000534).  Saving model ...\nLoss decreased (0.000534 --> 0.000534).  Saving model ...\nEpoch: 36700, loss: 0.00054\nLoss decreased (0.000534 --> 0.000534).  Saving model ...\nLoss decreased (0.000534 --> 0.000534).  Saving model ...\nEpoch: 36800, loss: 0.00070\nLoss decreased (0.000534 --> 0.000533).  Saving model ...\nEpoch: 36900, loss: 0.00053\nLoss decreased (0.000533 --> 0.000533).  Saving model ...\nLoss decreased (0.000533 --> 0.000533).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 37000, loss: 0.00056\nLoss decreased (0.000533 --> 0.000532).  Saving model ...\nLoss decreased (0.000532 --> 0.000532).  Saving model ...\nLoss decreased (0.000532 --> 0.000532).  Saving model ...\nLoss decreased (0.000532 --> 0.000532).  Saving model ...\nEpoch: 37100, loss: 0.00055\nLoss decreased (0.000532 --> 0.000532).  Saving model ...\nLoss decreased (0.000532 --> 0.000532).  Saving model ...\nLoss decreased (0.000532 --> 0.000532).  Saving model ...\nEpoch: 37200, loss: 0.00056\nLoss decreased (0.000532 --> 0.000532).  Saving model ...\nLoss decreased (0.000532 --> 0.000531).  Saving model ...\nLoss decreased (0.000531 --> 0.000531).  Saving model ...\nEpoch: 37300, loss: 0.00055\nEpoch: 37400, loss: 0.00056\nLoss decreased (0.000531 --> 0.000531).  Saving model ...\nLoss decreased (0.000531 --> 0.000531).  Saving model ...\nLoss decreased (0.000531 --> 0.000530).  Saving model ...\nLoss decreased (0.000530 --> 0.000530).  Saving model ...\nEpoch: 37500, loss: 0.00053\nLoss decreased (0.000530 --> 0.000530).  Saving model ...\nLoss decreased (0.000530 --> 0.000530).  Saving model ...\nEpoch: 37600, loss: 0.00054\nLoss decreased (0.000530 --> 0.000530).  Saving model ...\nLoss decreased (0.000530 --> 0.000530).  Saving model ...\nLoss decreased (0.000530 --> 0.000529).  Saving model ...\nEpoch: 37700, loss: 0.00062\nEpoch: 37800, loss: 0.00060\nLoss decreased (0.000529 --> 0.000529).  Saving model ...\nLoss decreased (0.000529 --> 0.000529).  Saving model ...\nLoss decreased (0.000529 --> 0.000529).  Saving model ...\nEpoch: 37900, loss: 0.00055\nLoss decreased (0.000529 --> 0.000529).  Saving model ...\nLoss decreased (0.000529 --> 0.000529).  Saving model ...\nEpoch: 38000, loss: 0.00054\nLoss decreased (0.000529 --> 0.000529).  Saving model ...\nLoss decreased (0.000529 --> 0.000528).  Saving model ...\nLoss decreased (0.000528 --> 0.000528).  Saving model ...\nEpoch: 38100, loss: 0.00058\nLoss decreased (0.000528 --> 0.000528).  Saving model ...\nEpoch: 38200, loss: 0.00057\nLoss decreased (0.000528 --> 0.000528).  Saving model ...\nLoss decreased (0.000528 --> 0.000527).  Saving model ...\nLoss decreased (0.000527 --> 0.000527).  Saving model ...\nEpoch: 38300, loss: 0.00053\nLoss decreased (0.000527 --> 0.000527).  Saving model ...\nLoss decreased (0.000527 --> 0.000527).  Saving model ...\nEpoch: 38400, loss: 0.00055\nLoss decreased (0.000527 --> 0.000527).  Saving model ...\nLoss decreased (0.000527 --> 0.000526).  Saving model ...\nLoss decreased (0.000526 --> 0.000526).  Saving model ...\nLoss decreased (0.000526 --> 0.000526).  Saving model ...\nLoss decreased (0.000526 --> 0.000526).  Saving model ...\nEpoch: 38500, loss: 0.00055\nLoss decreased (0.000526 --> 0.000526).  Saving model ...\nLoss decreased (0.000526 --> 0.000526).  Saving model ...\nEpoch: 38600, loss: 0.00055\nLoss decreased (0.000526 --> 0.000526).  Saving model ...\nLoss decreased (0.000526 --> 0.000526).  Saving model ...\nEpoch: 38700, loss: 0.00061\nLoss decreased (0.000526 --> 0.000525).  Saving model ...\nLoss decreased (0.000525 --> 0.000525).  Saving model ...\nEpoch: 38800, loss: 0.00056\nLoss decreased (0.000525 --> 0.000525).  Saving model ...\nEpoch: 38900, loss: 0.00057\nLoss decreased (0.000525 --> 0.000525).  Saving model ...\nLoss decreased (0.000525 --> 0.000525).  Saving model ...\nLoss decreased (0.000525 --> 0.000524).  Saving model ...\nLoss decreased (0.000524 --> 0.000524).  Saving model ...\nEpoch: 39000, loss: 0.00055\nLoss decreased (0.000524 --> 0.000524).  Saving model ...\nLoss decreased (0.000524 --> 0.000524).  Saving model ...\nEpoch: 39100, loss: 0.00053\nLoss decreased (0.000524 --> 0.000524).  Saving model ...\nLoss decreased (0.000524 --> 0.000524).  Saving model ...\nEpoch: 39200, loss: 0.00057\nLoss decreased (0.000524 --> 0.000523).  Saving model ...\nEpoch: 39300, loss: 0.00054\nLoss decreased (0.000523 --> 0.000523).  Saving model ...\nLoss decreased (0.000523 --> 0.000523).  Saving model ...\nEpoch: 39400, loss: 0.00057\nLoss decreased (0.000523 --> 0.000522).  Saving model ...\nEpoch: 39500, loss: 0.00055\nLoss decreased (0.000522 --> 0.000522).  Saving model ...\nLoss decreased (0.000522 --> 0.000522).  Saving model ...\nEpoch: 39600, loss: 0.00055\nLoss decreased (0.000522 --> 0.000522).  Saving model ...\nLoss decreased (0.000522 --> 0.000522).  Saving model ...\nEpoch: 39700, loss: 0.00054\nLoss decreased (0.000522 --> 0.000521).  Saving model ...\nLoss decreased (0.000521 --> 0.000521).  Saving model ...\nLoss decreased (0.000521 --> 0.000521).  Saving model ...\nEpoch: 39800, loss: 0.00053\nLoss decreased (0.000521 --> 0.000521).  Saving model ...\nLoss decreased (0.000521 --> 0.000521).  Saving model ...\nLoss decreased (0.000521 --> 0.000521).  Saving model ...\nEpoch: 39900, loss: 0.00054\nLoss decreased (0.000521 --> 0.000521).  Saving model ...\nEpoch: 40000, loss: 0.00052\nLoss decreased (0.000521 --> 0.000521).  Saving model ...\nLoss decreased (0.000521 --> 0.000520).  Saving model ...\nEpoch: 40100, loss: 0.00053\nLoss decreased (0.000520 --> 0.000520).  Saving model ...\nLoss decreased (0.000520 --> 0.000520).  Saving model ...\nLoss decreased (0.000520 --> 0.000520).  Saving model ...\nEpoch: 40200, loss: 0.00057\nLoss decreased (0.000520 --> 0.000520).  Saving model ...\nLoss decreased (0.000520 --> 0.000520).  Saving model ...\nEpoch: 40300, loss: 0.00056\nLoss decreased (0.000520 --> 0.000519).  Saving model ...\nLoss decreased (0.000519 --> 0.000519).  Saving model ...\nLoss decreased (0.000519 --> 0.000519).  Saving model ...\nEpoch: 40400, loss: 0.00055\nLoss decreased (0.000519 --> 0.000519).  Saving model ...\nEpoch: 40500, loss: 0.00052\nLoss decreased (0.000519 --> 0.000518).  Saving model ...\nLoss decreased (0.000518 --> 0.000518).  Saving model ...\nLoss decreased (0.000518 --> 0.000518).  Saving model ...\nEpoch: 40600, loss: 0.00054\nLoss decreased (0.000518 --> 0.000518).  Saving model ...\nLoss decreased (0.000518 --> 0.000518).  Saving model ...\nLoss decreased (0.000518 --> 0.000518).  Saving model ...\nLoss decreased (0.000518 --> 0.000517).  Saving model ...\nEpoch: 40700, loss: 0.00052\nLoss decreased (0.000517 --> 0.000517).  Saving model ...\nLoss decreased (0.000517 --> 0.000517).  Saving model ...\nLoss decreased (0.000517 --> 0.000517).  Saving model ...\nEpoch: 40800, loss: 0.00057\nLoss decreased (0.000517 --> 0.000517).  Saving model ...\nLoss decreased (0.000517 --> 0.000517).  Saving model ...\nLoss decreased (0.000517 --> 0.000517).  Saving model ...\nEpoch: 40900, loss: 0.00052\nLoss decreased (0.000517 --> 0.000517).  Saving model ...\nLoss decreased (0.000517 --> 0.000517).  Saving model ...\nLoss decreased (0.000517 --> 0.000516).  Saving model ...\nLoss decreased (0.000516 --> 0.000516).  Saving model ...\nLoss decreased (0.000516 --> 0.000516).  Saving model ...\nEpoch: 41000, loss: 0.00052\nLoss decreased (0.000516 --> 0.000516).  Saving model ...\nEpoch: 41100, loss: 0.00052\nLoss decreased (0.000516 --> 0.000516).  Saving model ...\nLoss decreased (0.000516 --> 0.000516).  Saving model ...\nEpoch: 41200, loss: 0.00055\nLoss decreased (0.000516 --> 0.000516).  Saving model ...\nLoss decreased (0.000516 --> 0.000516).  Saving model ...\nLoss decreased (0.000516 --> 0.000515).  Saving model ...\nEpoch: 41300, loss: 0.00055\nLoss decreased (0.000515 --> 0.000515).  Saving model ...\nEpoch: 41400, loss: 0.00053\nLoss decreased (0.000515 --> 0.000515).  Saving model ...\nLoss decreased (0.000515 --> 0.000515).  Saving model ...\nLoss decreased (0.000515 --> 0.000514).  Saving model ...\nEpoch: 41500, loss: 0.00056\nLoss decreased (0.000514 --> 0.000514).  Saving model ...\nLoss decreased (0.000514 --> 0.000514).  Saving model ...\nEpoch: 41600, loss: 0.00052\nLoss decreased (0.000514 --> 0.000514).  Saving model ...\nEpoch: 41700, loss: 0.00057\nLoss decreased (0.000514 --> 0.000514).  Saving model ...\nLoss decreased (0.000514 --> 0.000514).  Saving model ...\nLoss decreased (0.000514 --> 0.000513).  Saving model ...\nEpoch: 41800, loss: 0.00052\nLoss decreased (0.000513 --> 0.000513).  Saving model ...\nLoss decreased (0.000513 --> 0.000513).  Saving model ...\nLoss decreased (0.000513 --> 0.000513).  Saving model ...\nEpoch: 41900, loss: 0.00057\nLoss decreased (0.000513 --> 0.000513).  Saving model ...\nLoss decreased (0.000513 --> 0.000513).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000513 --> 0.000513).  Saving model ...\nLoss decreased (0.000513 --> 0.000512).  Saving model ...\nLoss decreased (0.000512 --> 0.000512).  Saving model ...\nEpoch: 42000, loss: 0.00055\nEpoch: 42100, loss: 0.00051\nLoss decreased (0.000512 --> 0.000512).  Saving model ...\nLoss decreased (0.000512 --> 0.000512).  Saving model ...\nLoss decreased (0.000512 --> 0.000512).  Saving model ...\nEpoch: 42200, loss: 0.00054\nLoss decreased (0.000512 --> 0.000511).  Saving model ...\nEpoch: 42300, loss: 0.00052\nLoss decreased (0.000511 --> 0.000511).  Saving model ...\nLoss decreased (0.000511 --> 0.000511).  Saving model ...\nEpoch: 42400, loss: 0.00055\nLoss decreased (0.000511 --> 0.000511).  Saving model ...\nEpoch: 42500, loss: 0.00053\nLoss decreased (0.000511 --> 0.000511).  Saving model ...\nLoss decreased (0.000511 --> 0.000511).  Saving model ...\nLoss decreased (0.000511 --> 0.000510).  Saving model ...\nLoss decreased (0.000510 --> 0.000510).  Saving model ...\nEpoch: 42600, loss: 0.00054\nLoss decreased (0.000510 --> 0.000510).  Saving model ...\nLoss decreased (0.000510 --> 0.000510).  Saving model ...\nEpoch: 42700, loss: 0.00051\nLoss decreased (0.000510 --> 0.000510).  Saving model ...\nEpoch: 42800, loss: 0.00051\nLoss decreased (0.000510 --> 0.000509).  Saving model ...\nEpoch: 42900, loss: 0.00051\nLoss decreased (0.000509 --> 0.000509).  Saving model ...\nLoss decreased (0.000509 --> 0.000509).  Saving model ...\nLoss decreased (0.000509 --> 0.000509).  Saving model ...\nEpoch: 43000, loss: 0.00055\nLoss decreased (0.000509 --> 0.000509).  Saving model ...\nEpoch: 43100, loss: 0.00053\nLoss decreased (0.000509 --> 0.000508).  Saving model ...\nLoss decreased (0.000508 --> 0.000508).  Saving model ...\nLoss decreased (0.000508 --> 0.000508).  Saving model ...\nEpoch: 43200, loss: 0.00056\nLoss decreased (0.000508 --> 0.000508).  Saving model ...\nLoss decreased (0.000508 --> 0.000508).  Saving model ...\nEpoch: 43300, loss: 0.00051\nEpoch: 43400, loss: 0.00051\nLoss decreased (0.000508 --> 0.000508).  Saving model ...\nLoss decreased (0.000508 --> 0.000507).  Saving model ...\nLoss decreased (0.000507 --> 0.000507).  Saving model ...\nEpoch: 43500, loss: 0.00051\nLoss decreased (0.000507 --> 0.000507).  Saving model ...\nLoss decreased (0.000507 --> 0.000507).  Saving model ...\nLoss decreased (0.000507 --> 0.000507).  Saving model ...\nEpoch: 43600, loss: 0.00056\nLoss decreased (0.000507 --> 0.000507).  Saving model ...\nLoss decreased (0.000507 --> 0.000506).  Saving model ...\nEpoch: 43700, loss: 0.00056\nLoss decreased (0.000506 --> 0.000506).  Saving model ...\nLoss decreased (0.000506 --> 0.000506).  Saving model ...\nLoss decreased (0.000506 --> 0.000506).  Saving model ...\nEpoch: 43800, loss: 0.00056\nLoss decreased (0.000506 --> 0.000506).  Saving model ...\nEpoch: 43900, loss: 0.00051\nLoss decreased (0.000506 --> 0.000506).  Saving model ...\nLoss decreased (0.000506 --> 0.000506).  Saving model ...\nLoss decreased (0.000506 --> 0.000505).  Saving model ...\nLoss decreased (0.000505 --> 0.000505).  Saving model ...\nEpoch: 44000, loss: 0.00051\nLoss decreased (0.000505 --> 0.000505).  Saving model ...\nLoss decreased (0.000505 --> 0.000505).  Saving model ...\nEpoch: 44100, loss: 0.00053\nLoss decreased (0.000505 --> 0.000505).  Saving model ...\nLoss decreased (0.000505 --> 0.000505).  Saving model ...\nLoss decreased (0.000505 --> 0.000505).  Saving model ...\nEpoch: 44200, loss: 0.00059\nLoss decreased (0.000505 --> 0.000505).  Saving model ...\nLoss decreased (0.000505 --> 0.000504).  Saving model ...\nEpoch: 44300, loss: 0.00057\nLoss decreased (0.000504 --> 0.000504).  Saving model ...\nLoss decreased (0.000504 --> 0.000504).  Saving model ...\nEpoch: 44400, loss: 0.00051\nLoss decreased (0.000504 --> 0.000504).  Saving model ...\nLoss decreased (0.000504 --> 0.000504).  Saving model ...\nLoss decreased (0.000504 --> 0.000504).  Saving model ...\nEpoch: 44500, loss: 0.00053\nLoss decreased (0.000504 --> 0.000503).  Saving model ...\nLoss decreased (0.000503 --> 0.000503).  Saving model ...\nEpoch: 44600, loss: 0.00054\nLoss decreased (0.000503 --> 0.000503).  Saving model ...\nLoss decreased (0.000503 --> 0.000503).  Saving model ...\nEpoch: 44700, loss: 0.00060\nLoss decreased (0.000503 --> 0.000502).  Saving model ...\nEpoch: 44800, loss: 0.00052\nLoss decreased (0.000502 --> 0.000502).  Saving model ...\nEpoch: 44900, loss: 0.00051\nLoss decreased (0.000502 --> 0.000502).  Saving model ...\nLoss decreased (0.000502 --> 0.000502).  Saving model ...\nLoss decreased (0.000502 --> 0.000502).  Saving model ...\nLoss decreased (0.000502 --> 0.000502).  Saving model ...\nEpoch: 45000, loss: 0.00051\nLoss decreased (0.000502 --> 0.000502).  Saving model ...\nLoss decreased (0.000502 --> 0.000501).  Saving model ...\nEpoch: 45100, loss: 0.00054\nLoss decreased (0.000501 --> 0.000501).  Saving model ...\nLoss decreased (0.000501 --> 0.000501).  Saving model ...\nLoss decreased (0.000501 --> 0.000501).  Saving model ...\nEpoch: 45200, loss: 0.00055\nLoss decreased (0.000501 --> 0.000501).  Saving model ...\nEpoch: 45300, loss: 0.00054\nLoss decreased (0.000501 --> 0.000500).  Saving model ...\nLoss decreased (0.000500 --> 0.000500).  Saving model ...\nEpoch: 45400, loss: 0.00053\nLoss decreased (0.000500 --> 0.000500).  Saving model ...\nEpoch: 45500, loss: 0.00050\nLoss decreased (0.000500 --> 0.000500).  Saving model ...\nLoss decreased (0.000500 --> 0.000500).  Saving model ...\nLoss decreased (0.000500 --> 0.000500).  Saving model ...\nEpoch: 45600, loss: 0.00051\nLoss decreased (0.000500 --> 0.000500).  Saving model ...\nLoss decreased (0.000500 --> 0.000499).  Saving model ...\nEpoch: 45700, loss: 0.00051\nLoss decreased (0.000499 --> 0.000499).  Saving model ...\nLoss decreased (0.000499 --> 0.000499).  Saving model ...\nLoss decreased (0.000499 --> 0.000499).  Saving model ...\nEpoch: 45800, loss: 0.00050\nEpoch: 45900, loss: 0.00051\nLoss decreased (0.000499 --> 0.000499).  Saving model ...\nLoss decreased (0.000499 --> 0.000498).  Saving model ...\nLoss decreased (0.000498 --> 0.000498).  Saving model ...\nLoss decreased (0.000498 --> 0.000498).  Saving model ...\nEpoch: 46000, loss: 0.00050\nLoss decreased (0.000498 --> 0.000498).  Saving model ...\nEpoch: 46100, loss: 0.00054\nLoss decreased (0.000498 --> 0.000498).  Saving model ...\nLoss decreased (0.000498 --> 0.000497).  Saving model ...\nEpoch: 46200, loss: 0.00056\nLoss decreased (0.000497 --> 0.000497).  Saving model ...\nEpoch: 46300, loss: 0.00053\nLoss decreased (0.000497 --> 0.000497).  Saving model ...\nLoss decreased (0.000497 --> 0.000497).  Saving model ...\nEpoch: 46400, loss: 0.00052\nLoss decreased (0.000497 --> 0.000497).  Saving model ...\nLoss decreased (0.000497 --> 0.000496).  Saving model ...\nEpoch: 46500, loss: 0.00050\nLoss decreased (0.000496 --> 0.000496).  Saving model ...\nLoss decreased (0.000496 --> 0.000496).  Saving model ...\nEpoch: 46600, loss: 0.00051\nLoss decreased (0.000496 --> 0.000496).  Saving model ...\nLoss decreased (0.000496 --> 0.000496).  Saving model ...\nLoss decreased (0.000496 --> 0.000496).  Saving model ...\nEpoch: 46700, loss: 0.00055\nLoss decreased (0.000496 --> 0.000496).  Saving model ...\nLoss decreased (0.000496 --> 0.000496).  Saving model ...\nLoss decreased (0.000496 --> 0.000496).  Saving model ...\nLoss decreased (0.000496 --> 0.000496).  Saving model ...\nLoss decreased (0.000496 --> 0.000496).  Saving model ...\nEpoch: 46800, loss: 0.00056\nLoss decreased (0.000496 --> 0.000495).  Saving model ...\nLoss decreased (0.000495 --> 0.000495).  Saving model ...\nLoss decreased (0.000495 --> 0.000495).  Saving model ...\nEpoch: 46900, loss: 0.00056\nLoss decreased (0.000495 --> 0.000495).  Saving model ...\nLoss decreased (0.000495 --> 0.000495).  Saving model ...\nEpoch: 47000, loss: 0.00050\nLoss decreased (0.000495 --> 0.000495).  Saving model ...\nLoss decreased (0.000495 --> 0.000495).  Saving model ...\nLoss decreased (0.000495 --> 0.000495).  Saving model ...\nLoss decreased (0.000495 --> 0.000494).  Saving model ...\nEpoch: 47100, loss: 0.00055\nLoss decreased (0.000494 --> 0.000494).  Saving model ...\nLoss decreased (0.000494 --> 0.000494).  Saving model ...\nEpoch: 47200, loss: 0.00053\nLoss decreased (0.000494 --> 0.000494).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 47300, loss: 0.00049\nLoss decreased (0.000494 --> 0.000494).  Saving model ...\nLoss decreased (0.000494 --> 0.000494).  Saving model ...\nLoss decreased (0.000494 --> 0.000493).  Saving model ...\nEpoch: 47400, loss: 0.00059\nLoss decreased (0.000493 --> 0.000493).  Saving model ...\nLoss decreased (0.000493 --> 0.000493).  Saving model ...\nLoss decreased (0.000493 --> 0.000493).  Saving model ...\nEpoch: 47500, loss: 0.00064\nLoss decreased (0.000493 --> 0.000493).  Saving model ...\nLoss decreased (0.000493 --> 0.000493).  Saving model ...\nEpoch: 47600, loss: 0.00050\nLoss decreased (0.000493 --> 0.000493).  Saving model ...\nLoss decreased (0.000493 --> 0.000492).  Saving model ...\nEpoch: 47700, loss: 0.00050\nLoss decreased (0.000492 --> 0.000492).  Saving model ...\nLoss decreased (0.000492 --> 0.000492).  Saving model ...\nLoss decreased (0.000492 --> 0.000492).  Saving model ...\nEpoch: 47800, loss: 0.00050\nLoss decreased (0.000492 --> 0.000492).  Saving model ...\nLoss decreased (0.000492 --> 0.000492).  Saving model ...\nLoss decreased (0.000492 --> 0.000492).  Saving model ...\nEpoch: 47900, loss: 0.00049\nLoss decreased (0.000492 --> 0.000492).  Saving model ...\nLoss decreased (0.000492 --> 0.000491).  Saving model ...\nEpoch: 48000, loss: 0.00053\nLoss decreased (0.000491 --> 0.000491).  Saving model ...\nLoss decreased (0.000491 --> 0.000491).  Saving model ...\nLoss decreased (0.000491 --> 0.000491).  Saving model ...\nLoss decreased (0.000491 --> 0.000491).  Saving model ...\nEpoch: 48100, loss: 0.00053\nLoss decreased (0.000491 --> 0.000491).  Saving model ...\nEpoch: 48200, loss: 0.00051\nLoss decreased (0.000491 --> 0.000490).  Saving model ...\nLoss decreased (0.000490 --> 0.000490).  Saving model ...\nEpoch: 48300, loss: 0.00049\nLoss decreased (0.000490 --> 0.000490).  Saving model ...\nEpoch: 48400, loss: 0.00049\nEpoch: 48500, loss: 0.00051\nLoss decreased (0.000490 --> 0.000489).  Saving model ...\nLoss decreased (0.000489 --> 0.000489).  Saving model ...\nEpoch: 48600, loss: 0.00052\nLoss decreased (0.000489 --> 0.000489).  Saving model ...\nLoss decreased (0.000489 --> 0.000489).  Saving model ...\nEpoch: 48700, loss: 0.00055\nLoss decreased (0.000489 --> 0.000489).  Saving model ...\nLoss decreased (0.000489 --> 0.000489).  Saving model ...\nLoss decreased (0.000489 --> 0.000489).  Saving model ...\nLoss decreased (0.000489 --> 0.000488).  Saving model ...\nLoss decreased (0.000488 --> 0.000488).  Saving model ...\nEpoch: 48800, loss: 0.00059\nLoss decreased (0.000488 --> 0.000488).  Saving model ...\nEpoch: 48900, loss: 0.00053\nLoss decreased (0.000488 --> 0.000488).  Saving model ...\nLoss decreased (0.000488 --> 0.000488).  Saving model ...\nEpoch: 49000, loss: 0.00053\nLoss decreased (0.000488 --> 0.000488).  Saving model ...\nLoss decreased (0.000488 --> 0.000487).  Saving model ...\nEpoch: 49100, loss: 0.00051\nLoss decreased (0.000487 --> 0.000487).  Saving model ...\nLoss decreased (0.000487 --> 0.000487).  Saving model ...\nEpoch: 49200, loss: 0.00050\nLoss decreased (0.000487 --> 0.000487).  Saving model ...\nLoss decreased (0.000487 --> 0.000487).  Saving model ...\nEpoch: 49300, loss: 0.00050\nLoss decreased (0.000487 --> 0.000486).  Saving model ...\nLoss decreased (0.000486 --> 0.000486).  Saving model ...\nEpoch: 49400, loss: 0.00053\nLoss decreased (0.000486 --> 0.000486).  Saving model ...\nLoss decreased (0.000486 --> 0.000486).  Saving model ...\nEpoch: 49500, loss: 0.00053\nLoss decreased (0.000486 --> 0.000486).  Saving model ...\nLoss decreased (0.000486 --> 0.000485).  Saving model ...\nEpoch: 49600, loss: 0.00049\nLoss decreased (0.000485 --> 0.000485).  Saving model ...\nLoss decreased (0.000485 --> 0.000485).  Saving model ...\nEpoch: 49700, loss: 0.00049\nLoss decreased (0.000485 --> 0.000485).  Saving model ...\nLoss decreased (0.000485 --> 0.000485).  Saving model ...\nEpoch: 49800, loss: 0.00050\nLoss decreased (0.000485 --> 0.000485).  Saving model ...\nLoss decreased (0.000485 --> 0.000485).  Saving model ...\nEpoch: 49900, loss: 0.00055\nLoss decreased (0.000485 --> 0.000484).  Saving model ...\nEpoch: 50000, loss: 0.00058\nLoss decreased (0.000484 --> 0.000484).  Saving model ...\nLoss decreased (0.000484 --> 0.000484).  Saving model ...\nLoss decreased (0.000484 --> 0.000484).  Saving model ...\nLoss decreased (0.000484 --> 0.000484).  Saving model ...\nEpoch: 50100, loss: 0.00055\nLoss decreased (0.000484 --> 0.000483).  Saving model ...\nLoss decreased (0.000483 --> 0.000483).  Saving model ...\nEpoch: 50200, loss: 0.00054\nLoss decreased (0.000483 --> 0.000483).  Saving model ...\nEpoch: 50300, loss: 0.00052\nLoss decreased (0.000483 --> 0.000483).  Saving model ...\nLoss decreased (0.000483 --> 0.000483).  Saving model ...\nEpoch: 50400, loss: 0.00052\nLoss decreased (0.000483 --> 0.000483).  Saving model ...\nLoss decreased (0.000483 --> 0.000482).  Saving model ...\nEpoch: 50500, loss: 0.00052\nLoss decreased (0.000482 --> 0.000482).  Saving model ...\nLoss decreased (0.000482 --> 0.000482).  Saving model ...\nEpoch: 50600, loss: 0.00048\nLoss decreased (0.000482 --> 0.000482).  Saving model ...\nLoss decreased (0.000482 --> 0.000482).  Saving model ...\nLoss decreased (0.000482 --> 0.000481).  Saving model ...\nEpoch: 50700, loss: 0.00060\nLoss decreased (0.000481 --> 0.000481).  Saving model ...\nEpoch: 50800, loss: 0.00053\nLoss decreased (0.000481 --> 0.000481).  Saving model ...\nLoss decreased (0.000481 --> 0.000481).  Saving model ...\nLoss decreased (0.000481 --> 0.000481).  Saving model ...\nEpoch: 50900, loss: 0.00050\nLoss decreased (0.000481 --> 0.000481).  Saving model ...\nEpoch: 51000, loss: 0.00052\nLoss decreased (0.000481 --> 0.000480).  Saving model ...\nEpoch: 51100, loss: 0.00049\nLoss decreased (0.000480 --> 0.000480).  Saving model ...\nLoss decreased (0.000480 --> 0.000480).  Saving model ...\nLoss decreased (0.000480 --> 0.000480).  Saving model ...\nEpoch: 51200, loss: 0.00058\nLoss decreased (0.000480 --> 0.000479).  Saving model ...\nLoss decreased (0.000479 --> 0.000479).  Saving model ...\nEpoch: 51300, loss: 0.00057\nLoss decreased (0.000479 --> 0.000479).  Saving model ...\nLoss decreased (0.000479 --> 0.000479).  Saving model ...\nEpoch: 51400, loss: 0.00048\nLoss decreased (0.000479 --> 0.000479).  Saving model ...\nLoss decreased (0.000479 --> 0.000479).  Saving model ...\nLoss decreased (0.000479 --> 0.000479).  Saving model ...\nLoss decreased (0.000479 --> 0.000479).  Saving model ...\nLoss decreased (0.000479 --> 0.000479).  Saving model ...\nLoss decreased (0.000479 --> 0.000479).  Saving model ...\nEpoch: 51500, loss: 0.00048\nLoss decreased (0.000479 --> 0.000479).  Saving model ...\nLoss decreased (0.000479 --> 0.000478).  Saving model ...\nLoss decreased (0.000478 --> 0.000478).  Saving model ...\nLoss decreased (0.000478 --> 0.000478).  Saving model ...\nLoss decreased (0.000478 --> 0.000478).  Saving model ...\nLoss decreased (0.000478 --> 0.000478).  Saving model ...\nEpoch: 51600, loss: 0.00050\nEpoch: 51700, loss: 0.00050\nLoss decreased (0.000478 --> 0.000478).  Saving model ...\nLoss decreased (0.000478 --> 0.000478).  Saving model ...\nLoss decreased (0.000478 --> 0.000478).  Saving model ...\nLoss decreased (0.000478 --> 0.000477).  Saving model ...\nEpoch: 51800, loss: 0.00048\nLoss decreased (0.000477 --> 0.000477).  Saving model ...\nEpoch: 51900, loss: 0.00051\nLoss decreased (0.000477 --> 0.000477).  Saving model ...\nEpoch: 52000, loss: 0.00048\nLoss decreased (0.000477 --> 0.000477).  Saving model ...\nEpoch: 52100, loss: 0.00048\nLoss decreased (0.000477 --> 0.000476).  Saving model ...\nLoss decreased (0.000476 --> 0.000476).  Saving model ...\nLoss decreased (0.000476 --> 0.000476).  Saving model ...\nEpoch: 52200, loss: 0.00048\nLoss decreased (0.000476 --> 0.000476).  Saving model ...\nLoss decreased (0.000476 --> 0.000476).  Saving model ...\nLoss decreased (0.000476 --> 0.000476).  Saving model ...\nLoss decreased (0.000476 --> 0.000476).  Saving model ...\nLoss decreased (0.000476 --> 0.000476).  Saving model ...\nLoss decreased (0.000476 --> 0.000476).  Saving model ...\nEpoch: 52300, loss: 0.00051\nLoss decreased (0.000476 --> 0.000476).  Saving model ...\nEpoch: 52400, loss: 0.00048\nEpoch: 52500, loss: 0.00048\nLoss decreased (0.000476 --> 0.000475).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000475 --> 0.000475).  Saving model ...\nLoss decreased (0.000475 --> 0.000475).  Saving model ...\nLoss decreased (0.000475 --> 0.000475).  Saving model ...\nLoss decreased (0.000475 --> 0.000474).  Saving model ...\nEpoch: 52600, loss: 0.00059\nLoss decreased (0.000474 --> 0.000474).  Saving model ...\nEpoch: 52700, loss: 0.00055\nLoss decreased (0.000474 --> 0.000474).  Saving model ...\nLoss decreased (0.000474 --> 0.000474).  Saving model ...\nEpoch: 52800, loss: 0.00056\nLoss decreased (0.000474 --> 0.000474).  Saving model ...\nLoss decreased (0.000474 --> 0.000474).  Saving model ...\nLoss decreased (0.000474 --> 0.000474).  Saving model ...\nLoss decreased (0.000474 --> 0.000474).  Saving model ...\nEpoch: 52900, loss: 0.00050\nLoss decreased (0.000474 --> 0.000474).  Saving model ...\nLoss decreased (0.000474 --> 0.000473).  Saving model ...\nEpoch: 53000, loss: 0.00050\nLoss decreased (0.000473 --> 0.000473).  Saving model ...\nEpoch: 53100, loss: 0.00052\nLoss decreased (0.000473 --> 0.000473).  Saving model ...\nLoss decreased (0.000473 --> 0.000473).  Saving model ...\nLoss decreased (0.000473 --> 0.000473).  Saving model ...\nEpoch: 53200, loss: 0.00047\nLoss decreased (0.000473 --> 0.000472).  Saving model ...\nLoss decreased (0.000472 --> 0.000472).  Saving model ...\nLoss decreased (0.000472 --> 0.000472).  Saving model ...\nLoss decreased (0.000472 --> 0.000472).  Saving model ...\nEpoch: 53300, loss: 0.00050\nEpoch: 53400, loss: 0.00047\nLoss decreased (0.000472 --> 0.000472).  Saving model ...\nLoss decreased (0.000472 --> 0.000472).  Saving model ...\nLoss decreased (0.000472 --> 0.000472).  Saving model ...\nLoss decreased (0.000472 --> 0.000472).  Saving model ...\nEpoch: 53500, loss: 0.00052\nLoss decreased (0.000472 --> 0.000472).  Saving model ...\nLoss decreased (0.000472 --> 0.000471).  Saving model ...\nLoss decreased (0.000471 --> 0.000471).  Saving model ...\nLoss decreased (0.000471 --> 0.000471).  Saving model ...\nEpoch: 53600, loss: 0.00047\nEpoch: 53700, loss: 0.00053\nLoss decreased (0.000471 --> 0.000471).  Saving model ...\nLoss decreased (0.000471 --> 0.000471).  Saving model ...\nLoss decreased (0.000471 --> 0.000470).  Saving model ...\nEpoch: 53800, loss: 0.00050\nLoss decreased (0.000470 --> 0.000470).  Saving model ...\nEpoch: 53900, loss: 0.00053\nLoss decreased (0.000470 --> 0.000470).  Saving model ...\nLoss decreased (0.000470 --> 0.000470).  Saving model ...\nEpoch: 54000, loss: 0.00055\nLoss decreased (0.000470 --> 0.000470).  Saving model ...\nEpoch: 54100, loss: 0.00051\nLoss decreased (0.000470 --> 0.000470).  Saving model ...\nLoss decreased (0.000470 --> 0.000469).  Saving model ...\nLoss decreased (0.000469 --> 0.000469).  Saving model ...\nEpoch: 54200, loss: 0.00056\nLoss decreased (0.000469 --> 0.000469).  Saving model ...\nLoss decreased (0.000469 --> 0.000469).  Saving model ...\nLoss decreased (0.000469 --> 0.000469).  Saving model ...\nLoss decreased (0.000469 --> 0.000469).  Saving model ...\nEpoch: 54300, loss: 0.00062\nEpoch: 54400, loss: 0.00049\nLoss decreased (0.000469 --> 0.000468).  Saving model ...\nLoss decreased (0.000468 --> 0.000468).  Saving model ...\nLoss decreased (0.000468 --> 0.000468).  Saving model ...\nEpoch: 54500, loss: 0.00051\nEpoch: 54600, loss: 0.00047\nLoss decreased (0.000468 --> 0.000468).  Saving model ...\nLoss decreased (0.000468 --> 0.000468).  Saving model ...\nEpoch: 54700, loss: 0.00050\nLoss decreased (0.000468 --> 0.000467).  Saving model ...\nEpoch: 54800, loss: 0.00053\nLoss decreased (0.000467 --> 0.000467).  Saving model ...\nLoss decreased (0.000467 --> 0.000467).  Saving model ...\nEpoch: 54900, loss: 0.00050\nLoss decreased (0.000467 --> 0.000467).  Saving model ...\nEpoch: 55000, loss: 0.00050\nLoss decreased (0.000467 --> 0.000467).  Saving model ...\nLoss decreased (0.000467 --> 0.000466).  Saving model ...\nLoss decreased (0.000466 --> 0.000466).  Saving model ...\nLoss decreased (0.000466 --> 0.000466).  Saving model ...\nEpoch: 55100, loss: 0.00047\nEpoch: 55200, loss: 0.00053\nLoss decreased (0.000466 --> 0.000466).  Saving model ...\nLoss decreased (0.000466 --> 0.000466).  Saving model ...\nEpoch: 55300, loss: 0.00049\nLoss decreased (0.000466 --> 0.000466).  Saving model ...\nEpoch: 55400, loss: 0.00059\nLoss decreased (0.000466 --> 0.000465).  Saving model ...\nEpoch: 55500, loss: 0.00051\nLoss decreased (0.000465 --> 0.000465).  Saving model ...\nLoss decreased (0.000465 --> 0.000465).  Saving model ...\nLoss decreased (0.000465 --> 0.000465).  Saving model ...\nLoss decreased (0.000465 --> 0.000465).  Saving model ...\nLoss decreased (0.000465 --> 0.000465).  Saving model ...\nEpoch: 55600, loss: 0.00047\nLoss decreased (0.000465 --> 0.000465).  Saving model ...\nEpoch: 55700, loss: 0.00051\nLoss decreased (0.000465 --> 0.000464).  Saving model ...\nLoss decreased (0.000464 --> 0.000464).  Saving model ...\nEpoch: 55800, loss: 0.00047\nLoss decreased (0.000464 --> 0.000464).  Saving model ...\nLoss decreased (0.000464 --> 0.000464).  Saving model ...\nLoss decreased (0.000464 --> 0.000464).  Saving model ...\nEpoch: 55900, loss: 0.00046\nLoss decreased (0.000464 --> 0.000464).  Saving model ...\nLoss decreased (0.000464 --> 0.000464).  Saving model ...\nEpoch: 56000, loss: 0.00050\nEpoch: 56100, loss: 0.00048\nLoss decreased (0.000464 --> 0.000463).  Saving model ...\nLoss decreased (0.000463 --> 0.000463).  Saving model ...\nEpoch: 56200, loss: 0.00049\nLoss decreased (0.000463 --> 0.000463).  Saving model ...\nLoss decreased (0.000463 --> 0.000463).  Saving model ...\nLoss decreased (0.000463 --> 0.000462).  Saving model ...\nEpoch: 56300, loss: 0.00053\nEpoch: 56400, loss: 0.00047\nLoss decreased (0.000462 --> 0.000462).  Saving model ...\nLoss decreased (0.000462 --> 0.000462).  Saving model ...\nLoss decreased (0.000462 --> 0.000462).  Saving model ...\nEpoch: 56500, loss: 0.00046\nLoss decreased (0.000462 --> 0.000462).  Saving model ...\nEpoch: 56600, loss: 0.00050\nLoss decreased (0.000462 --> 0.000461).  Saving model ...\nLoss decreased (0.000461 --> 0.000461).  Saving model ...\nLoss decreased (0.000461 --> 0.000461).  Saving model ...\nLoss decreased (0.000461 --> 0.000461).  Saving model ...\nEpoch: 56700, loss: 0.00050\nEpoch: 56800, loss: 0.00048\nLoss decreased (0.000461 --> 0.000461).  Saving model ...\nEpoch: 56900, loss: 0.00050\nLoss decreased (0.000461 --> 0.000461).  Saving model ...\nLoss decreased (0.000461 --> 0.000461).  Saving model ...\nEpoch: 57000, loss: 0.00052\nLoss decreased (0.000461 --> 0.000460).  Saving model ...\nEpoch: 57100, loss: 0.00046\nLoss decreased (0.000460 --> 0.000460).  Saving model ...\nLoss decreased (0.000460 --> 0.000460).  Saving model ...\nEpoch: 57200, loss: 0.00047\nLoss decreased (0.000460 --> 0.000460).  Saving model ...\nLoss decreased (0.000460 --> 0.000459).  Saving model ...\nLoss decreased (0.000459 --> 0.000459).  Saving model ...\nEpoch: 57300, loss: 0.00053\nLoss decreased (0.000459 --> 0.000459).  Saving model ...\nLoss decreased (0.000459 --> 0.000459).  Saving model ...\nEpoch: 57400, loss: 0.00046\nEpoch: 57500, loss: 0.00048\nLoss decreased (0.000459 --> 0.000459).  Saving model ...\nLoss decreased (0.000459 --> 0.000458).  Saving model ...\nLoss decreased (0.000458 --> 0.000458).  Saving model ...\nEpoch: 57600, loss: 0.00047\nEpoch: 57700, loss: 0.00046\nLoss decreased (0.000458 --> 0.000458).  Saving model ...\nEpoch: 57800, loss: 0.00048\nLoss decreased (0.000458 --> 0.000458).  Saving model ...\nEpoch: 57900, loss: 0.00048\nLoss decreased (0.000458 --> 0.000457).  Saving model ...\nEpoch: 58000, loss: 0.00046\nLoss decreased (0.000457 --> 0.000457).  Saving model ...\nLoss decreased (0.000457 --> 0.000457).  Saving model ...\nEpoch: 58100, loss: 0.00053\nEpoch: 58200, loss: 0.00048\nLoss decreased (0.000457 --> 0.000457).  Saving model ...\nLoss decreased (0.000457 --> 0.000457).  Saving model ...\nEpoch: 58300, loss: 0.00046\nLoss decreased (0.000457 --> 0.000456).  Saving model ...\nLoss decreased (0.000456 --> 0.000456).  Saving model ...\nEpoch: 58400, loss: 0.00046\nLoss decreased (0.000456 --> 0.000456).  Saving model ...\nLoss decreased (0.000456 --> 0.000456).  Saving model ...\nLoss decreased (0.000456 --> 0.000456).  Saving model ...\nEpoch: 58500, loss: 0.00046\nLoss decreased (0.000456 --> 0.000456).  Saving model ...\nLoss decreased (0.000456 --> 0.000455).  Saving model ...\n","name":"stdout"},{"output_type":"stream","text":"Epoch: 58600, loss: 0.00051\nLoss decreased (0.000455 --> 0.000455).  Saving model ...\nLoss decreased (0.000455 --> 0.000455).  Saving model ...\nEpoch: 58700, loss: 0.00047\nEpoch: 58800, loss: 0.00047\nLoss decreased (0.000455 --> 0.000455).  Saving model ...\nLoss decreased (0.000455 --> 0.000455).  Saving model ...\nLoss decreased (0.000455 --> 0.000454).  Saving model ...\nLoss decreased (0.000454 --> 0.000454).  Saving model ...\nEpoch: 58900, loss: 0.00046\nEpoch: 59000, loss: 0.00046\nEpoch: 59100, loss: 0.00050\nLoss decreased (0.000454 --> 0.000454).  Saving model ...\nEpoch: 59200, loss: 0.00048\nLoss decreased (0.000454 --> 0.000454).  Saving model ...\nLoss decreased (0.000454 --> 0.000454).  Saving model ...\nEpoch: 59300, loss: 0.00050\nLoss decreased (0.000454 --> 0.000453).  Saving model ...\nLoss decreased (0.000453 --> 0.000453).  Saving model ...\nEpoch: 59400, loss: 0.00045\nLoss decreased (0.000453 --> 0.000453).  Saving model ...\nLoss decreased (0.000453 --> 0.000453).  Saving model ...\nEpoch: 59500, loss: 0.00045\nLoss decreased (0.000453 --> 0.000453).  Saving model ...\nLoss decreased (0.000453 --> 0.000453).  Saving model ...\nEpoch: 59600, loss: 0.00051\nEpoch: 59700, loss: 0.00046\nLoss decreased (0.000453 --> 0.000452).  Saving model ...\nLoss decreased (0.000452 --> 0.000452).  Saving model ...\nLoss decreased (0.000452 --> 0.000452).  Saving model ...\nLoss decreased (0.000452 --> 0.000452).  Saving model ...\nEpoch: 59800, loss: 0.00051\nLoss decreased (0.000452 --> 0.000452).  Saving model ...\nLoss decreased (0.000452 --> 0.000452).  Saving model ...\nLoss decreased (0.000452 --> 0.000452).  Saving model ...\nEpoch: 59900, loss: 0.00046\nLoss decreased (0.000452 --> 0.000451).  Saving model ...\nEpoch: 60000, loss: 0.00048\nLoss decreased (0.000451 --> 0.000451).  Saving model ...\nEpoch: 60100, loss: 0.00048\nLoss decreased (0.000451 --> 0.000451).  Saving model ...\nLoss decreased (0.000451 --> 0.000451).  Saving model ...\nLoss decreased (0.000451 --> 0.000451).  Saving model ...\nEpoch: 60200, loss: 0.00045\nLoss decreased (0.000451 --> 0.000450).  Saving model ...\nLoss decreased (0.000450 --> 0.000450).  Saving model ...\nEpoch: 60300, loss: 0.00054\nEpoch: 60400, loss: 0.00051\nLoss decreased (0.000450 --> 0.000450).  Saving model ...\nLoss decreased (0.000450 --> 0.000450).  Saving model ...\nEpoch: 60500, loss: 0.00050\nLoss decreased (0.000450 --> 0.000450).  Saving model ...\nEpoch: 60600, loss: 0.00045\nLoss decreased (0.000450 --> 0.000449).  Saving model ...\nEpoch: 60700, loss: 0.00051\nEpoch: 60800, loss: 0.00048\nLoss decreased (0.000449 --> 0.000449).  Saving model ...\nLoss decreased (0.000449 --> 0.000449).  Saving model ...\nLoss decreased (0.000449 --> 0.000449).  Saving model ...\nEpoch: 60900, loss: 0.00049\nEpoch: 61000, loss: 0.00045\nLoss decreased (0.000449 --> 0.000448).  Saving model ...\nLoss decreased (0.000448 --> 0.000448).  Saving model ...\nEpoch: 61100, loss: 0.00051\nLoss decreased (0.000448 --> 0.000448).  Saving model ...\nEpoch: 61200, loss: 0.00054\nLoss decreased (0.000448 --> 0.000448).  Saving model ...\nLoss decreased (0.000448 --> 0.000448).  Saving model ...\nEpoch: 61300, loss: 0.00047\nLoss decreased (0.000448 --> 0.000448).  Saving model ...\nLoss decreased (0.000448 --> 0.000448).  Saving model ...\nEpoch: 61400, loss: 0.00052\nLoss decreased (0.000448 --> 0.000448).  Saving model ...\nEpoch: 61500, loss: 0.00045\nLoss decreased (0.000448 --> 0.000447).  Saving model ...\nEpoch: 61600, loss: 0.00048\nLoss decreased (0.000447 --> 0.000447).  Saving model ...\nLoss decreased (0.000447 --> 0.000447).  Saving model ...\nLoss decreased (0.000447 --> 0.000447).  Saving model ...\nLoss decreased (0.000447 --> 0.000447).  Saving model ...\nEpoch: 61700, loss: 0.00050\nLoss decreased (0.000447 --> 0.000446).  Saving model ...\nLoss decreased (0.000446 --> 0.000446).  Saving model ...\nEpoch: 61800, loss: 0.00056\nEpoch: 61900, loss: 0.00051\nLoss decreased (0.000446 --> 0.000446).  Saving model ...\nEpoch: 62000, loss: 0.00049\nLoss decreased (0.000446 --> 0.000446).  Saving model ...\nEpoch: 62100, loss: 0.00050\nEpoch: 62200, loss: 0.00047\nEpoch: 62300, loss: 0.00048\nLoss decreased (0.000446 --> 0.000445).  Saving model ...\nEpoch: 62400, loss: 0.00049\nLoss decreased (0.000445 --> 0.000445).  Saving model ...\nLoss decreased (0.000445 --> 0.000445).  Saving model ...\nLoss decreased (0.000445 --> 0.000445).  Saving model ...\nLoss decreased (0.000445 --> 0.000445).  Saving model ...\nEpoch: 62500, loss: 0.00045\nEpoch: 62600, loss: 0.00048\nLoss decreased (0.000445 --> 0.000444).  Saving model ...\nEpoch: 62700, loss: 0.00045\nLoss decreased (0.000444 --> 0.000444).  Saving model ...\nLoss decreased (0.000444 --> 0.000444).  Saving model ...\nLoss decreased (0.000444 --> 0.000444).  Saving model ...\nEpoch: 62800, loss: 0.00048\nLoss decreased (0.000444 --> 0.000444).  Saving model ...\nLoss decreased (0.000444 --> 0.000444).  Saving model ...\nLoss decreased (0.000444 --> 0.000443).  Saving model ...\nEpoch: 62900, loss: 0.00046\nLoss decreased (0.000443 --> 0.000443).  Saving model ...\nEpoch: 63000, loss: 0.00050\nLoss decreased (0.000443 --> 0.000443).  Saving model ...\nLoss decreased (0.000443 --> 0.000443).  Saving model ...\nLoss decreased (0.000443 --> 0.000443).  Saving model ...\nEpoch: 63100, loss: 0.00044\nEpoch: 63200, loss: 0.00048\nLoss decreased (0.000443 --> 0.000442).  Saving model ...\nLoss decreased (0.000442 --> 0.000442).  Saving model ...\nEpoch: 63300, loss: 0.00046\nLoss decreased (0.000442 --> 0.000442).  Saving model ...\nEpoch: 63400, loss: 0.00044\nLoss decreased (0.000442 --> 0.000442).  Saving model ...\nEpoch: 63500, loss: 0.00049\nLoss decreased (0.000442 --> 0.000442).  Saving model ...\nLoss decreased (0.000442 --> 0.000442).  Saving model ...\nEpoch: 63600, loss: 0.00051\nLoss decreased (0.000442 --> 0.000442).  Saving model ...\nLoss decreased (0.000442 --> 0.000442).  Saving model ...\nLoss decreased (0.000442 --> 0.000441).  Saving model ...\nEpoch: 63700, loss: 0.00044\nLoss decreased (0.000441 --> 0.000441).  Saving model ...\nLoss decreased (0.000441 --> 0.000441).  Saving model ...\nLoss decreased (0.000441 --> 0.000441).  Saving model ...\nEpoch: 63800, loss: 0.00050\nEpoch: 63900, loss: 0.00052\nLoss decreased (0.000441 --> 0.000441).  Saving model ...\nEpoch: 64000, loss: 0.00047\nEpoch: 64100, loss: 0.00048\nLoss decreased (0.000441 --> 0.000440).  Saving model ...\nEpoch: 64200, loss: 0.00046\nLoss decreased (0.000440 --> 0.000440).  Saving model ...\nLoss decreased (0.000440 --> 0.000440).  Saving model ...\nEpoch: 64300, loss: 0.00051\nLoss decreased (0.000440 --> 0.000440).  Saving model ...\nLoss decreased (0.000440 --> 0.000440).  Saving model ...\nLoss decreased (0.000440 --> 0.000440).  Saving model ...\nEpoch: 64400, loss: 0.00044\nLoss decreased (0.000440 --> 0.000440).  Saving model ...\nLoss decreased (0.000440 --> 0.000439).  Saving model ...\nEpoch: 64500, loss: 0.00044\nLoss decreased (0.000439 --> 0.000439).  Saving model ...\nEpoch: 64600, loss: 0.00049\nLoss decreased (0.000439 --> 0.000439).  Saving model ...\nLoss decreased (0.000439 --> 0.000439).  Saving model ...\nLoss decreased (0.000439 --> 0.000439).  Saving model ...\nEpoch: 64700, loss: 0.00049\nLoss decreased (0.000439 --> 0.000439).  Saving model ...\nEpoch: 64800, loss: 0.00045\nEpoch: 64900, loss: 0.00046\nLoss decreased (0.000439 --> 0.000439).  Saving model ...\nLoss decreased (0.000439 --> 0.000438).  Saving model ...\nEpoch: 65000, loss: 0.00050\nLoss decreased (0.000438 --> 0.000438).  Saving model ...\nLoss decreased (0.000438 --> 0.000438).  Saving model ...\nEpoch: 65100, loss: 0.00044\nEpoch: 65200, loss: 0.00046\nLoss decreased (0.000438 --> 0.000438).  Saving model ...\nLoss decreased (0.000438 --> 0.000438).  Saving model ...\nEpoch: 65300, loss: 0.00051\nLoss decreased (0.000438 --> 0.000438).  Saving model ...\nEpoch: 65400, loss: 0.00050\nLoss decreased (0.000438 --> 0.000437).  Saving model ...\nLoss decreased (0.000437 --> 0.000437).  Saving model ...\nLoss decreased (0.000437 --> 0.000437).  Saving model ...\nEpoch: 65500, loss: 0.00045\nLoss decreased (0.000437 --> 0.000437).  Saving model ...\nEpoch: 65600, loss: 0.00045\nEpoch: 65700, loss: 0.00045\nLoss decreased (0.000437 --> 0.000437).  Saving model ...\nLoss decreased (0.000437 --> 0.000436).  Saving model ...\nEpoch: 65800, loss: 0.00050\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000436 --> 0.000436).  Saving model ...\nLoss decreased (0.000436 --> 0.000436).  Saving model ...\nEpoch: 65900, loss: 0.00050\nEpoch: 66000, loss: 0.00046\nEpoch: 66100, loss: 0.00046\nLoss decreased (0.000436 --> 0.000436).  Saving model ...\nLoss decreased (0.000436 --> 0.000435).  Saving model ...\nLoss decreased (0.000435 --> 0.000435).  Saving model ...\nEpoch: 66200, loss: 0.00050\nLoss decreased (0.000435 --> 0.000435).  Saving model ...\nLoss decreased (0.000435 --> 0.000435).  Saving model ...\nEpoch: 66300, loss: 0.00044\nLoss decreased (0.000435 --> 0.000435).  Saving model ...\nEpoch: 66400, loss: 0.00046\nLoss decreased (0.000435 --> 0.000435).  Saving model ...\nLoss decreased (0.000435 --> 0.000435).  Saving model ...\nEpoch: 66500, loss: 0.00043\nLoss decreased (0.000435 --> 0.000435).  Saving model ...\nLoss decreased (0.000435 --> 0.000434).  Saving model ...\nEpoch: 66600, loss: 0.00048\nLoss decreased (0.000434 --> 0.000434).  Saving model ...\nEpoch: 66700, loss: 0.00044\nEpoch: 66800, loss: 0.00051\nLoss decreased (0.000434 --> 0.000434).  Saving model ...\nEpoch: 66900, loss: 0.00046\nLoss decreased (0.000434 --> 0.000434).  Saving model ...\nEpoch: 67000, loss: 0.00049\nEpoch: 67100, loss: 0.00045\nLoss decreased (0.000434 --> 0.000433).  Saving model ...\nEpoch: 67200, loss: 0.00044\nLoss decreased (0.000433 --> 0.000433).  Saving model ...\nEpoch: 67300, loss: 0.00044\nLoss decreased (0.000433 --> 0.000433).  Saving model ...\nLoss decreased (0.000433 --> 0.000433).  Saving model ...\nEpoch: 67400, loss: 0.00043\nEpoch: 67500, loss: 0.00050\nLoss decreased (0.000433 --> 0.000433).  Saving model ...\nLoss decreased (0.000433 --> 0.000432).  Saving model ...\nEpoch: 67600, loss: 0.00050\nLoss decreased (0.000432 --> 0.000432).  Saving model ...\nLoss decreased (0.000432 --> 0.000432).  Saving model ...\nEpoch: 67700, loss: 0.00046\nLoss decreased (0.000432 --> 0.000432).  Saving model ...\nEpoch: 67800, loss: 0.00045\nLoss decreased (0.000432 --> 0.000432).  Saving model ...\nLoss decreased (0.000432 --> 0.000432).  Saving model ...\nEpoch: 67900, loss: 0.00058\nLoss decreased (0.000432 --> 0.000431).  Saving model ...\nLoss decreased (0.000431 --> 0.000431).  Saving model ...\nLoss decreased (0.000431 --> 0.000431).  Saving model ...\nEpoch: 68000, loss: 0.00044\nEpoch: 68100, loss: 0.00047\nEpoch: 68200, loss: 0.00048\nLoss decreased (0.000431 --> 0.000431).  Saving model ...\nLoss decreased (0.000431 --> 0.000431).  Saving model ...\nEpoch: 68300, loss: 0.00049\nEpoch: 68400, loss: 0.00044\nLoss decreased (0.000431 --> 0.000431).  Saving model ...\nLoss decreased (0.000431 --> 0.000430).  Saving model ...\nEpoch: 68500, loss: 0.00044\nLoss decreased (0.000430 --> 0.000430).  Saving model ...\nEpoch: 68600, loss: 0.00048\nLoss decreased (0.000430 --> 0.000430).  Saving model ...\nLoss decreased (0.000430 --> 0.000430).  Saving model ...\nLoss decreased (0.000430 --> 0.000429).  Saving model ...\nLoss decreased (0.000429 --> 0.000429).  Saving model ...\nEpoch: 68700, loss: 0.00055\nEpoch: 68800, loss: 0.00043\nLoss decreased (0.000429 --> 0.000429).  Saving model ...\nEpoch: 68900, loss: 0.00046\nLoss decreased (0.000429 --> 0.000429).  Saving model ...\nEpoch: 69000, loss: 0.00053\nLoss decreased (0.000429 --> 0.000429).  Saving model ...\nEpoch: 69100, loss: 0.00043\nLoss decreased (0.000429 --> 0.000429).  Saving model ...\nEpoch: 69200, loss: 0.00046\nLoss decreased (0.000429 --> 0.000429).  Saving model ...\nLoss decreased (0.000429 --> 0.000428).  Saving model ...\nEpoch: 69300, loss: 0.00047\nEpoch: 69400, loss: 0.00047\nLoss decreased (0.000428 --> 0.000428).  Saving model ...\nEpoch: 69500, loss: 0.00043\nLoss decreased (0.000428 --> 0.000428).  Saving model ...\nEpoch: 69600, loss: 0.00050\nLoss decreased (0.000428 --> 0.000428).  Saving model ...\nEpoch: 69700, loss: 0.00047\nEpoch: 69800, loss: 0.00043\nLoss decreased (0.000428 --> 0.000428).  Saving model ...\nLoss decreased (0.000428 --> 0.000427).  Saving model ...\nEpoch: 69900, loss: 0.00047\nLoss decreased (0.000427 --> 0.000427).  Saving model ...\nLoss decreased (0.000427 --> 0.000427).  Saving model ...\nEpoch: 70000, loss: 0.00045\nLoss decreased (0.000427 --> 0.000427).  Saving model ...\nEpoch: 70100, loss: 0.00045\nEpoch: 70200, loss: 0.00045\nLoss decreased (0.000427 --> 0.000427).  Saving model ...\nLoss decreased (0.000427 --> 0.000427).  Saving model ...\nLoss decreased (0.000427 --> 0.000426).  Saving model ...\nLoss decreased (0.000426 --> 0.000426).  Saving model ...\nEpoch: 70300, loss: 0.00043\nEpoch: 70400, loss: 0.00057\nEpoch: 70500, loss: 0.00044\nLoss decreased (0.000426 --> 0.000426).  Saving model ...\nLoss decreased (0.000426 --> 0.000426).  Saving model ...\nLoss decreased (0.000426 --> 0.000426).  Saving model ...\nLoss decreased (0.000426 --> 0.000426).  Saving model ...\nEpoch: 70600, loss: 0.00050\nLoss decreased (0.000426 --> 0.000425).  Saving model ...\nEpoch: 70700, loss: 0.00043\nEpoch: 70800, loss: 0.00043\nLoss decreased (0.000425 --> 0.000425).  Saving model ...\nEpoch: 70900, loss: 0.00050\nLoss decreased (0.000425 --> 0.000425).  Saving model ...\nEpoch: 71000, loss: 0.00047\nLoss decreased (0.000425 --> 0.000425).  Saving model ...\nLoss decreased (0.000425 --> 0.000425).  Saving model ...\nEpoch: 71100, loss: 0.00049\nLoss decreased (0.000425 --> 0.000425).  Saving model ...\nLoss decreased (0.000425 --> 0.000424).  Saving model ...\nEpoch: 71200, loss: 0.00047\nLoss decreased (0.000424 --> 0.000424).  Saving model ...\nLoss decreased (0.000424 --> 0.000424).  Saving model ...\nEpoch: 71300, loss: 0.00044\nLoss decreased (0.000424 --> 0.000424).  Saving model ...\nLoss decreased (0.000424 --> 0.000424).  Saving model ...\nLoss decreased (0.000424 --> 0.000424).  Saving model ...\nEpoch: 71400, loss: 0.00046\nEpoch: 71500, loss: 0.00057\nEpoch: 71600, loss: 0.00054\nLoss decreased (0.000424 --> 0.000424).  Saving model ...\nLoss decreased (0.000424 --> 0.000423).  Saving model ...\nLoss decreased (0.000423 --> 0.000423).  Saving model ...\nEpoch: 71700, loss: 0.00045\nLoss decreased (0.000423 --> 0.000423).  Saving model ...\nEpoch: 71800, loss: 0.00048\nEpoch: 71900, loss: 0.00044\nLoss decreased (0.000423 --> 0.000423).  Saving model ...\nEpoch: 72000, loss: 0.00049\nLoss decreased (0.000423 --> 0.000422).  Saving model ...\nLoss decreased (0.000422 --> 0.000422).  Saving model ...\nEpoch: 72100, loss: 0.00044\nEpoch: 72200, loss: 0.00047\nEpoch: 72300, loss: 0.00042\nLoss decreased (0.000422 --> 0.000422).  Saving model ...\nLoss decreased (0.000422 --> 0.000422).  Saving model ...\nEpoch: 72400, loss: 0.00042\nEpoch: 72500, loss: 0.00045\nEpoch: 72600, loss: 0.00043\nLoss decreased (0.000422 --> 0.000422).  Saving model ...\nEpoch: 72700, loss: 0.00043\nLoss decreased (0.000422 --> 0.000421).  Saving model ...\nLoss decreased (0.000421 --> 0.000421).  Saving model ...\nLoss decreased (0.000421 --> 0.000421).  Saving model ...\nLoss decreased (0.000421 --> 0.000421).  Saving model ...\nLoss decreased (0.000421 --> 0.000421).  Saving model ...\nLoss decreased (0.000421 --> 0.000421).  Saving model ...\nLoss decreased (0.000421 --> 0.000421).  Saving model ...\nLoss decreased (0.000421 --> 0.000420).  Saving model ...\nEpoch: 72800, loss: 0.00052\nEpoch: 72900, loss: 0.00046\nEpoch: 73000, loss: 0.00048\nEpoch: 73100, loss: 0.00054\nLoss decreased (0.000420 --> 0.000420).  Saving model ...\nEpoch: 73200, loss: 0.00045\nLoss decreased (0.000420 --> 0.000420).  Saving model ...\nEpoch: 73300, loss: 0.00054\nLoss decreased (0.000420 --> 0.000420).  Saving model ...\nEpoch: 73400, loss: 0.00047\nLoss decreased (0.000420 --> 0.000420).  Saving model ...\nEpoch: 73500, loss: 0.00042\nEpoch: 73600, loss: 0.00043\nLoss decreased (0.000420 --> 0.000420).  Saving model ...\nEpoch: 73700, loss: 0.00042\nLoss decreased (0.000420 --> 0.000420).  Saving model ...\nLoss decreased (0.000420 --> 0.000419).  Saving model ...\nEpoch: 73800, loss: 0.00054\nLoss decreased (0.000419 --> 0.000419).  Saving model ...\nLoss decreased (0.000419 --> 0.000419).  Saving model ...\nLoss decreased (0.000419 --> 0.000419).  Saving model ...\nLoss decreased (0.000419 --> 0.000418).  Saving model ...\nLoss decreased (0.000418 --> 0.000418).  Saving model ...\nEpoch: 73900, loss: 0.00042\nEpoch: 74000, loss: 0.00043\n","name":"stdout"},{"output_type":"stream","text":"Loss decreased (0.000418 --> 0.000418).  Saving model ...\nLoss decreased (0.000418 --> 0.000418).  Saving model ...\nLoss decreased (0.000418 --> 0.000418).  Saving model ...\nEpoch: 74100, loss: 0.00049\nEpoch: 74200, loss: 0.00044\nLoss decreased (0.000418 --> 0.000418).  Saving model ...\nLoss decreased (0.000418 --> 0.000417).  Saving model ...\nEpoch: 74300, loss: 0.00047\nEpoch: 74400, loss: 0.00045\nEpoch: 74500, loss: 0.00042\nLoss decreased (0.000417 --> 0.000417).  Saving model ...\nEpoch: 74600, loss: 0.00042\nLoss decreased (0.000417 --> 0.000417).  Saving model ...\nEpoch: 74700, loss: 0.00043\nEpoch: 74800, loss: 0.00047\nLoss decreased (0.000417 --> 0.000417).  Saving model ...\nEpoch: 74900, loss: 0.00045\nLoss decreased (0.000417 --> 0.000416).  Saving model ...\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Loading model with minimum loss "},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    train_loss= checkpoint['train_loss']\n    for parameter in model.parameters():\n        parameter.requires_grad = False\n    \n    model.eval()\n    \n    return model, train_loss","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_best, train_loss = load_checkpoint('checkpoint.pth')\nmodel_best= model_best\ntrain_loss= train_loss\nprint(model_best)","execution_count":24,"outputs":[{"output_type":"stream","text":"LSTM1(\n  (lstm): LSTM(90, 8, num_layers=3, batch_first=True)\n  (fc_1): Linear(in_features=8, out_features=128, bias=True)\n  (fc): Linear(in_features=128, out_features=1, bias=True)\n  (relu): ReLU()\n)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Testing the model... "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating a datastructure with 60 time stamps and 1 output(for entire data, previously done just on training set)\n\nx= []\ny= []\n\nfor i in range(90, len(data_scaled)):\n    x.append(data_scaled[i-90:i, :])\n    y.append(data_scaled[i, 0])\n    \n# Converting them to numpy arrays\nx, y= np.array(x), np.array(y)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tensors = Variable(torch.Tensor(x)).cuda()\n# X_test_tensors = Variable(torch.Tensor(X_test)).cuda()\n\ny_tensors = Variable(torch.Tensor(y)).cuda()\n# y_test_tensors = Variable(torch.Tensor(y_test)) .cuda()\n\n\nX_tensors_final = torch.reshape(X_tensors,   (X_tensors.shape[0], 1, X_tensors.shape[1]))","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# On complete dataset\n\ntrain_predict = model_best(X_tensors_final.cuda())#forward pass\n\ndata_predict = train_predict.data.cpu()\ndata_predict = data_predict.numpy() #numpy conversion\n\ndataY_plot = y_tensors.data.cpu()\ndataY_plot = dataY_plot.numpy()\n\ndata_predict = mm.inverse_transform(data_predict) #reverse transformation\ndataY_plot = mm.inverse_transform(dataY_plot.reshape(-1, 1))\nplt.figure(figsize=(10,6)) #plotting\n\nplt.plot(dataY_plot, label='Actuall Data') #actual plot\nplt.plot(data_predict, label='Predicted Data') #predicted plot\nplt.title('Time-Series Prediction')\nplt.legend()\nplt.show() ","execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only on Test set\n# We droped the last 30 values for testing above(during data preprocessing)\n\nX_test= X_train_tensors_final[-30:, :].cuda()\ny_test= y_train_tensors[-30:, :].cuda()\n\n\ntest_predict= model_best(X_test) # Forward pass\n\npredicted= test_predict.data.cpu()\npredicted= predicted.numpy()\n\nreal_test= y_test.data.cpu()\nreal_test= real_test.numpy()\n\n\npredicted= mm.inverse_transform(predicted)\nreal_test= mm.inverse_transform(real_test)\n\nplt.figure(figsize=(10,6)) #plotting\nplt.plot(real_test, label='Actuall Data') #actual plot\nplt.plot(predicted, label='Predicted Data') #predicted plot\nplt.title('Time-Series Prediction on Test Data')\nplt.legend()\nplt.show() ","execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 720x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## R2 score calculation over Test set..."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import r2_score\ncoefficient_of_dermination = r2_score(real_test, predicted)\nprint(coefficient_of_dermination)","execution_count":29,"outputs":[{"output_type":"stream","text":"0.9496874998231968\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Saving the model... "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Saving the LSTM model\nimport joblib\njoblib.dump(model_best, '30DayGrowth%Approch2.pkl')","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"['30DayGrowth%Approch2.pkl']"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}